{"pages":[{"title":"我是谁","text":"就不直接放照片了，对我长什么样感兴趣的人可以通过联系方式勾搭我。我怕我放了我的照片被别人的爬虫爬走之后，成为海量数据中的一部分，然后会对计算出的模型产生不利的影响。 基本信息邹迪凯，1997年5月14日生，江苏常州人。 业余爱好（这里的“业”目前指学业，未来指事业） 二次元：偶尔看日本动漫，Fate，SAO，巨人，或追番或补番。 音乐：随便听听，轻音乐或者bgm，需要强调的是：只听不唱。 美剧：神盾局，纸牌屋，西部世界，不过只有当合我胃口的时候，我才会去认真地看。 教育经历 2003-2009 于常州市第二实验小学 2009-2012 于常州市第二十四中学 2012-2015 于常州市第一中学 2015-2019 于东南大学 联系方式","link":"/about.html"},{"title":"","text":"","link":"/tags/index.html"}],"posts":[{"title":"2017-2018 SRTP项目心得体会","text":"我觉得这个SRTP项目给我带来的最大的收获时：不要好高骛远。 本来此项目是一个外包项目，某个老师（以下统称客户）找到我们，提出了要一个如此如此的管理系统的需求。我们起初觉得这个项目很简单，功能不是很复杂，对除了功能需求之外的其他需求都没有太高的要求。除去刚性需求外，项目所必需的技术栈也都在我们的能力范围之内，没有技术盲区，也没有大量的新技术需要学习。虽然我们当时仅仅是大二学生，还缺少一些项目开发经验，但好在小组内每个人的个人编码能力都还不错，对于这么一个任务，我们是有自信能够在短时间内完成并交付的。 随着项目的展开，我们发现事情并不是想象中的那么简单，一个简简单单的功能、一个普通的对话框弹出动作、列表的刷新、数据的导入导出，每一个点都会产生意想不到的情况，需要长时间的反复的调试。分工开发的模块合成之后，又会出现新的问题，这时候就需要分别负责模块的两个人坐在一起看着代码一行一行单步运行，盯着监视窗口里的变量的值。经过了几次集中时间的排查，基本把遇到的所有缺陷全部修复。直到我写这篇心得体会的昨晚，当翻出系统的可执行文件，尝试了一下基本功能的运行，依然能够发现遗漏的问题，然后此时以及是软件正式交付客户使用半年后了。 即便是功能需求全部满足，遇到的缺陷近乎全部解决，但这项目依然很难被称作是一个圆满完成的项目。大量冗余的代码，各种不合理的类设计，性能还有极大的提升空间，编码风格尚需进一步规范。这些问题，在我们完成系统开发回过头来补全开发文档的时候，一一暴露了出来。软件开发是一门很深的学问，而我涉足尚浅。 在去年SRTP项目申报阶段，我注意到很多学生自主申报的项目都是和机器学习、人工智能、图像识别等等最近极为活跃的计算机领域概念相关的。相比之下我们做的这一个简简单单的单机桌面应用程序就有些小巫见大巫了。但是在这么简单的一个开发项目中我们尚且遇到了这么多困难，花费了这么多时间和精力，同样是本科大二的学生，去研究那些未知的领域，然后试图做出一个有一定功能的系统出来，是不是会更加困难呢。当然，各人能力有高低，知识积累方向有差异。 当然，在本项目的开发过程中也是有很多可取之处的，每周日上午我们都会到一个固定的地点进行项目的开发、版本的迭代、缺陷的调试，这样使得我们的产品能够在保证质量的前提下,顺利地在客户要求的时间前交付。 此外，还有一些技术方面的感想，本项目我们使用的时候C++语言的可视化框架Qt，一来是因为短学期实训刚好就是采用Qt的方案，对这个框架的技术路线比较熟悉，二是我们当时都只系统学习过c++语言，若是要采用别的技术可能需要再花费额外的时间进行技术调研和学习。但是实际上不论在哪个平台，用c++写桌面应用程序都已经有些过时了。计算机行业的技术更新迭代很快，需要我们不停地学习、不停地跟进。","link":"/2017/11/20/2017-2018-srtp/"},{"title":"2019年4月19日周记","text":"起因为了能够记录自己平时的学习和生活，并进行一些有益的反思，我开始尝试写周记，同时也真心希望自己能够坚持下去。 说明周记正文内容所涵盖的开始时间为本周周一，结束时间为本周周日。 2019年4月15日-2019年4月21日本周是开始考研复习的第二周，总得来说，复习任务安排不够饱和，负载不够均衡，没有严格按照时间安排。经过反思之后发现有以下几个具体的问题： 没有正式地背单词。 仅仅使用扇贝单词app在空闲时间背单词。这里的空闲时间大部分都安排在图书馆和食堂之间的通勤过程中。虽然利用了这一块时间，但一边走路一边背单词的效率并不是很高。对单词的例句没有作适当的记忆。 对高数的复习没有做到层次化、渐进式。 目前这一阶段的安排是先看汤家凤视频并做笔记，然后看李永乐全书上的内容并完成全书上的例题和习题。但实际操作中发现视频中的授课内容和全书中的内容的难度差异巨大，导致无法充分发挥全书上题目的练习作用。 多次违反了时间安排。 没有严格按照时间安排进行学习。导致复习工作不够饱和，有些“三天打鱼两天晒网”的意思。睡觉不够早，起床也不够早。","link":"/2019/04/19/2019%E5%B9%B44%E6%9C%8819%E6%97%A5%E5%91%A8%E8%AE%B0/"},{"title":"2019年4月29日周记","text":"2019年4月22日-2019年4月28日本周是开始考研复习的第三周，实际上只复习了一天。在周一经过小伙伴提醒五一之后要交论文初稿，因此周一之后的所有时间都用来写论文了。但是论文进度比想象的慢，写作效率不高，可以形容为”以写论文的名义划水“。 毕业论文中已经完成的部分质量尚可，但是进度缓慢，效率低下。本质的原因还是自制力差。 在B站上花的时间太多，主要观看的视频包括但不限于新番吐槽、星际2比赛，把b站动态当成朋友圈刷。自我感觉已经形成了Bilibili瘾。 实际上，如果能够把正事做完的话，是可以适当地在b站休闲一下的。但现实的情况是，因为在b站上”浪费时间“导致正事没有按计划完成。 马上五一假期要到了，预计4月30日完成论文初稿。","link":"/2019/04/29/2019%E5%B9%B44%E6%9C%8829%E6%97%A5%E5%91%A8%E8%AE%B0/"},{"title":"2019年4月消费情况总结及5月预算","text":"起因为了能够及时对”不堪回首“的消费记录进行分析和总结，并对下一个月的消费做出有效可行的预算，在此为消费单独开辟一个栏目。 说明覆盖时间为当月1日到当月最后一日。 4月消费总结4月是完整实习的最后一个月，也是拿完整工资的最后一个月，收入还比较充裕。 本月支出1900，其中在餐饮上花费了接近1000，应该是外卖+水果消费了大部分额外的开支。假设每天花10元买水果，那么一个月就要多花近300元，这样虽然保证了充足的维生素摄入，但是还是有点吃不起的感觉。因为在宿舍带了好几天，那几天平均一天一顿外卖，因此在昂贵的外卖也拉高了原本在低水平食堂就能够满足的开销。 订阅（即购买App和App内会员）上所花的钱也过多，其中quizlet（一个被单词的App）买了却没有用。无效的订阅太多。 云服务的花费主体应该只有阿里云的ECS（HK），ECS的钱主要分为基础配置费和流量费。基础配置费26元（下一个月开始是13元，找到了拼车的小伙伴），流量费1元/G，下一个月应该将流量控制在20G以下。 5月消费预算针对4月的几个消费大头，以及消费效率较低的类别，给出下一个月的预算如下： 类别 预算 饮食 27 * 25（早中晚饭）+ 15 * 10（水果）+4 * 15（零食）= 885 云服务 13 + 20 = 33 订阅 16（滴答清单）+ 10（icloud）+ 12（潮汐）= 32 总计 950 （我才发现有鱼记账没有预算的功能，这点没有moneywiz做得好）","link":"/2019/04/29/2019%E5%B9%B44%E6%9C%88%E6%B6%88%E8%B4%B9%E6%83%85%E5%86%B5%E6%80%BB%E7%BB%93%E5%8F%8A5%E6%9C%88%E9%A2%84%E7%AE%97/"},{"title":"2019年5月消费情况总结及6月预算","text":"5月消费总结我在五月的消费非常地不按照常理出牌，收入方面： 在KXD的四月的实习工资到账，900。 毕业生退学费，1900. 把iPad Pro 10.5在闲鱼上卖掉了，3000. 乍一看感觉收入还挺多的，但收入多导致自己没有给自己的支持限制施加足够的压力和约束力，导致消费也不少。几个大头消费有： 给MacBook Pro补了一个Apple Care Protection Plan，1800左右。这个消费如果分摊到未来两年的话还是很划算的，相当于给电脑买了一个保险。 入坑当红手游明日方舟，氪了不少钱，大概1200，这个氪钱就完全是“瞎花钱”了哈哈哈。 五月消费数据如下图。 此外，我爸给我转了1000元委托我管理BTC交易，虽然名义上是他的钱，但我感觉他一时半会儿不会找我要回去（他要怎么开口？），想在5.14生日那天问我妈要两千结果没有开口。 6月消费预算针对5月的几个消费大头，以及消费效率较低的类别，给出下一个月的预算如下： 类别 预算 饮食 27 x 30（早中晚饭）+ 15 x 10（水果）+ 4 x 30（零食）= 1080 云服务 26 + 20 = 46 订阅 16（滴答清单）+ 10（icloud）+ 12（潮汐）= 32 总计 1148","link":"/2019/06/02/2019%E5%B9%B45%E6%9C%88%E6%B6%88%E8%B4%B9%E6%83%85%E5%86%B5%E6%80%BB%E7%BB%93%E5%8F%8A6%E6%9C%88%E9%A2%84%E7%AE%97/"},{"title":"2019年6月消费情况总结及7月预算","text":"6月消费总结6月是我呆在东大的最后一个月，在这一个月中，有正常的消费（饮食、起居），也有满足特殊要求的（毕业相关事件）。 餐饮占消费支出大头。 买了个键盘，花了不少钱。 其他方面的消费分布较为均匀。 本来我以为消费支出还挺多的，实际上没有5月份多，各方面的支出水平都在预料的范围内。 7月消费预算7月份在家，餐饮方面的支出可以在很大程度上缩减。另其他方面支出除周期性支出外都要缩减（专心复习）。 类别 预算 饮食 4 x 50（零食）= 200 云服务 26 订阅 16（滴答清单）+ 10（icloud）+ 12（潮汐）+10（bear）= 48 总计 274 这个月的消费完全可以通过转嫁到家人身上的方式来减少数额，虽然这样想好想不太好，但这里只从我自己的资产和消费角度考虑。通过他人代自己消费也应算作一种方式。","link":"/2019/07/01/2019%E5%B9%B46%E6%9C%88%E6%B6%88%E8%B4%B9%E6%83%85%E5%86%B5%E6%80%BB%E7%BB%93%E5%8F%8A7%E6%9C%88%E9%A2%84%E7%AE%97/"},{"title":"2019年7月6日周记","text":"2019年6月31日-2019年7月6日离校了，宿舍里的东西全都搬到了家里，也都收拾好了。6月26日开始的为期三天的招生工作也宣告结束，不知道未来还有没有机会和计软的招生老师们一起去一中玩（zhuang）耍（bi）。 本以为到家了之后能够很快进入复习状态的，却没有料到人的惰性比我想象中的更加难以克服。常常是早上睡到89点钟起床，吃完午饭一不小心就在床上躺一整个下午，为了凑个整把晚上的时间也浪费掉了。晚上睡觉也睡得很晚，基本上天天都要熬到后半夜。 我理想的复习状态类似于人们常说的“吃了枪药，打了鸡血”那样的精神状态，但现在的状态却可以称作为“吸了大麻”。究其原因，还是因为觉悟不够，没有认清情况。我认为我目前的复习进度不算快也不算慢，属于比较稳健但又不是很积极的那一类。目前还在全力复习数学，刚刚完成高数知识点和习题的第一轮复习，除了数学之外，每天背单词，每天练字。我打算在数学第一轮结束时开始其他科目的同步复习，我认为这样安排对于我而言应该是合适的。进度是自己的，没有必要和别人去比，现在的问题是调整心态，端正态度，多给自己打一点鸡血，同时主动尝试自我调节的方式。 此时窗外忽然风雨大作，似乎我网上买的50一双的鞋套明天终于能有用武之地了。","link":"/2019/07/06/2019%E5%B9%B47%E6%9C%886%E6%97%A5%E5%91%A8%E8%AE%B0/"},{"title":"Apple产品体验","text":"（挖坑待填）","link":"/2019/04/06/Apple%E4%BA%A7%E5%93%81%E4%BD%93%E9%AA%8C/"},{"title":"有关上线的微信小程序说明","text":"舍友承包了一个娱乐向的小程序，最后把后端Tomcat部署到了这台服务器上的8080端口。如果想要通过网址访问，请使用http协议，因为8080端口没有用apache作反向代理，无法享受https加密传输。 链接： http://111.230.136.225:8080/tuba_war","link":"/2017/12/19/about/"},{"title":"Dell XPS 9360上安装Ubuntu 18.04失败的情况","text":"最近在我的xps 9360上安装Ubuntu18.04的时候出现这样一个问题：在硬盘分区完成之后，本来应该进入安装进度条，结果安装窗口突然变成了黑色，然后窗口自动消失，过了一分钟之后Ubuntu系统提示有一个crash。 后来经过搜索，找到了解决方案： https://bugs.launchpad.net/ubuntu/+source/ubiquity/+bug/1751252","link":"/2018/07/05/dell-xps/"},{"title":"自制Lex程序总结","text":"编译原理实验一提供了三个层面的选择，第一层是最简单的，简单到我都忘记了是什么要求了；第二层是用程序实现基于DFA的词法分析器；第三层有些复杂，模仿Lex的功能，设计一个能够生成词法分析器源代码的程序，即“编译器的编译器”。我不自量力地选了第三层。 其实，第三次说难也不难，说简单的话倒真的到处都是坑。我停停写写，大概写了两三周，更加深刻地体会都自己写业务代码写的飞快，但是一旦碰到有些技术含量的代码就要开始面向Google编程的事实。 现在，程序写的差不多了，以及传到了Github上，还发了两个release版本，先贴上链接。 DokymeLex 我作此文的目的是，对于程序的结构，设计和实现的思路，做一个记录和总结，也包括一些反思。 实现语言我选择了Java，我感觉我的同学基本上都是用java的，极少数会用C++或者C完成本次实验，用Python或者其他脚本语言的估计也不多。选Java不选C++的原因是写java程序的效率相对来说更高一些，对于变量和对象的初始化和传递的把握更加的轻松。同时，处于实验的要求，即便尽量不使用第三方库，java本身提供的标准库比C++丰富的多。比如容器类、字符串处理、IO操作类。针对每个具体的问题，往往能够很轻松地找到多种解决方案，然后进行权衡选择。不像C++，类库不丰富，解决方案较为单一，弹性不足（也有可能是我孤陋寡闻了）。 当然，java作为一种纯面向对象的语言，和C++相比，最核心的代码会更多一些，比如类的声明、成员变量的定义等。尤其是在生成程序源代码的时候，需要加一些额外的代码来保证程序可以通过编译，但是这是可以接受的。以前听有人说，人的思维是面向过程的，面向对象的思维是反人类的，是牺牲程序可读性的，我倒是觉得在不采用常见的设计模式的情况下，面向对象的代码反而更加清晰。 虽然，实验嘛，避嫌，最好少用现成的类库。但是我还是偷偷找了一个用于辅助和图有关计算的库，还用了gradle来管理包。当时找到这个库的主要原因是了解到这个库可以将图可视化显示出来，想着如果能够把DFA中状态转移的过程可视化出来，一边显示，一边做词法分析，就厉害了。但是实际上几百个状态，显示到窗口中，根本没法看。。。但是图的结构还是借助了这个库，因为不想改代码了，而且自己实现图，又要写很长时间的代码，调很长时间的bug。 就算这是实验，也不能啥都自己实现吧，像HashSet、HashMap这些，我要是自己实现的话肯定完成不了。 此外还使用了Apache.commons.cli库来辅助实现命令行参数解析的功能。 程序结构 类的功能和名字差不多，应该挺容易理解的吧。 重点诸如文件解析、命令行参数解析这样的功能我就不详细介绍了。主要讲几个技术含量比较高的，同时也是词法分析中较为核心的部分。 由RE构造NFA123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899//预处理。 Logger.debug(\"Raw input:\" + re.re); re = prePreProcessor(re); Logger.debug(\"Added concat symbol and transform range format to or:\" + re.re); re = infix2suffix(re); Logger.debug(\"Transform infix to suffix:\" + re.re); //Tomphonson算法构造NFA。 Stack stack = new Stack(); char[] characters = re.re.toCharArray(); boolean converting = false; for (char cur : characters) { switch (cur) { case '~': { if (converting) { stack.add(new NoDefiniteAutomation(cur)); converting = false; break; } NoDefiniteAutomation last = stack.pop(); NoDefiniteAutomation lastTwo = stack.pop(); stack.push(lastTwo.concat(last)); break; } case '+': { if (converting) { stack.add(new NoDefiniteAutomation(cur)); converting = false; break; } NoDefiniteAutomation last = stack.pop(); State newStart = new State(); State newEnd = new State(); last.graph.addEdge(new Transition(), newStart, last.start, EdgeType.DIRECTED); last.graph.addEdge(new Transition(), last.end, newEnd, EdgeType.DIRECTED); last.graph.addEdge(new Transition(), newEnd, newStart, EdgeType.DIRECTED); last.start = newStart; last.end = newEnd; stack.push(last); break; } case '*': { if (converting) { stack.add(new NoDefiniteAutomation(cur)); converting = false; break; } NoDefiniteAutomation last = stack.pop(); State newStart = new State(); State newEnd = new State(); last.graph.addEdge(new Transition(), newStart, newEnd, EdgeType.DIRECTED); last.graph.addEdge(new Transition(), newStart, last.start, EdgeType.DIRECTED); last.graph.addEdge(new Transition(), last.end, newEnd, EdgeType.DIRECTED); last.graph.addEdge(new Transition(), last.end, last.start, EdgeType.DIRECTED); last.start = newStart; last.end = newEnd; stack.push(last); break; } case '|': { if (converting) { stack.add(new NoDefiniteAutomation(cur)); converting = false; break; } NoDefiniteAutomation last = stack.pop(); NoDefiniteAutomation lastTwo = stack.pop(); stack.push(last.parellize(lastTwo)); break; } case '\\\\': converting = true; break; default: if (converting && ConvertingMap.keySet().contains(\"\\\\\" + cur)) { stack.add(new NoDefiniteAutomation(ConvertingMap.get(\"\\\\\" + cur))); converting = false; break; } else if (converting) { stack.add(new NoDefiniteAutomation(cur)); converting = false; break; } else { stack.add(new NoDefiniteAutomation(cur)); } break; } } if (stack.size() != 1) { Logger.error(\"The final element left in stack is not only one\"); return null; } NoDefiniteAutomation nfa = stack.pop(); if (re.action != null) { nfa.end.tag = re.action; } nfa.re = re; nfa.end.precedence = re.precedence; return nfa; 代码很长，主要有三个部分： 预处理：添加连接符~，把[a-z]的形式转换为(a|b|c|…|z)的形式。 中缀转后缀。 根据后缀表达式，按照Tomphson算法，生成NFA。 这段代码把1和2的过程浓缩为了两个函数，主要描述了3的实现。一个长长的switch语句，根据下一个符号，选择不同的策略，把栈中的小NFA转换成大的NFA，然后再压入栈中。每个case块都要先判断是否是转义过的，即上一个字符是不是反斜杠。 NFA的串联、并联方法的定义，我也是能放在NFA类中就放在NFA中的。 由多个NFA构造DFA123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960private static DefiniteAutomation build(NoDefiniteAutomation nfa, Set allEndState) { DefiniteAutomation dfa = new DefiniteAutomation(); dfa.allTrans = nfa.getAllTransitionTag(); dfa.nfa = nfa; dfa.table = new ArrayList(); dfa.graph = new DirectedSparseMultigraph(); Set stateSet = new HashSet(); stateSet.add(nfa.start); stateSet = dfa.getEpsilonClosure(stateSet); dfa.table.add(dfa.new TableEntry(stateSet)); for (int i = 0; i < dfa.table.size(); i++) { TableEntry entry = dfa.table.get(i); for (Character transChar : dfa.allTrans) { Transition trans = new Transition(transChar); Set extendedStates = dfa.getStateExtension(entry.nfaStates, trans); Set epslnExtendedStates = dfa.getEpsilonClosure(extendedStates); TableEntry potentialEntry = dfa.new TableEntry(epslnExtendedStates); if (!epslnExtendedStates.isEmpty()) { int existedEntryIndex = dfa.table.indexOf(potentialEntry); if (existedEntryIndex == -1) { dfa.table.add(potentialEntry); entry.transitions.put(trans, dfa.table.size() - 1); Logger.debug(\"Found new dfa state \" + dfa.table.size()); } else { entry.transitions.put(trans, existedEntryIndex); } } } } State.resetId(); List dfaStates = new ArrayList(); for (int i = 0; i < dfa.table.size(); i++) { dfaStates.add(new State()); } Set newEndStates = new HashSet(); for (int i = 0; i < dfa.table.size(); i++) { boolean isEndState = false; TableEntry entry = dfa.table.get(i); for (State nfaEndState : allEndState) { if (entry.nfaStates.contains(nfaEndState) && (dfaStates.get(i).precedence == -1 || dfaStates.get(i).precedence > nfaEndState.precedence)) { dfaStates.get(i).tag = nfaEndState.tag; dfaStates.get(i).precedence = nfaEndState.precedence; isEndState = true; } } if (isEndState) { newEndStates.add(dfaStates.get(i)); } for (Transition trans : entry.transitions.keySet()) { dfa.graph.addEdge(new Transition(trans.tag), dfaStates.get(i), dfaStates.get(entry.transitions.get(trans))); } } dfa.start = dfaStates.get(0); dfa.endStates = newEndStates; return dfa; } 由多个NFA转成DFA，采取的是在表格驱动的基础上，消除~边，并构造子集。 我想要来讨论一下这个函数中所用到的容器类。 驱动表格，使用的是 ArrayList，TableEntry是一个内部类。表格中的项肯定要保证有序，而且变长，如果用java原生数组的话很难预测需要开辟多大的空间。注意：对某个DFA状态，做子集构造、并求得epilison闭包之后，需要判断这个状态是不是已经求得的DFA状态，这里需要一个查找，我直接使用的indexOf方法，并且重写了TableEntry的equals方法。 表格项TableEntry，既要记录其中包含的NFA状态号，又要记录每个项通过每个转换、做子集构造、eplision闭包后的状态。记录NFA状态号，我采用的是HashSet，不需要保序，只要集合，而且判断相等的话应该效率不差。各个符号转义对应的状态，我使用了HashMap，Transition相当于一个字符，Integer标记了转移到的列表项的序号，将来就是DFA的状态号。字符和序号是一一对应的，看似用HashMap没什么问题。但是我觉得这个问题又要归结到为什么字符转换要用Transition，而不是char或者Character。因为我采用了现成库中的图的实现，而它要求每条边的类型的hashcode不能相同，即每条边不能相同，显然我用char或者Character肯定是不行的，所以就只能包一个类了。但是一个列表项里不会出现重复的字符转移，所以理论上这里用char也是可以的。但是用char和用transition差别应该只在存储空间不同，效率上的话，用transition也就是多调用一次hashcode方法而已。而整张表transition最多有26+26+10+一些符号，不超过80个，我觉得问题不大。 运行过程中发现，在某些构造某些DFA状态的时候，花费时间很长。这个时间长可能有两方面原因，一是子集构造的时间较长，epilison边较多，二是判断子集构造完之后的状态是不是已知的状态，查找花费了较长的时间。后者，由于indexof是依次比较，对每个成员调用一次equals，因此把equals的逻辑简化应该可以节约一点时间。我的代码中，TableEntry的equals等价于HashSet的equals，又等价于其中每一个State的equals，State就是一个整数，并且是全局唯一的整数。试问：能不能找到一个HashSet的摘要变量？求和显然不行，直接相连得字符串的话，需要保证两个相同的集合中，把所有元素取出来，得到的元素的顺序是相同的，这样字符串比较才能够确保不漏判。如果顺序不一样，那就要排个序了。以下是HashSet的get和put的实现： 我觉得问题的关键在于indexFor的冲突的特性。 不过我在网上找到的，有人说，程序就不应该依赖HashSet的顺序。emmmmm，也就是说要排个序？但这样的话，每次子集构造出来的HashSet就都要排序了，感觉工作量很大的样子。想到这里，要不干脆全部把HashSet换成有序的LinkedList结构？反正没有很多按索引引用的代码! set还有一种官方实现叫TreeSet，还自带排序功能，今天太晚了，有空我去了解一下。 (忽然有一种开窍的感觉，还真是第一次真正地把数据结构知识应用到项目实践中分析问题。) 难点未完待续","link":"/2017/12/22/dokymelex/"},{"title":"首","text":"还是放弃了自己搭博客，即便是用django这种武装到牙齿的框架（从数据库取数据呈现到网页不超过五行代码），还是避不开前端傻兮兮的布置和设计。听说WordPress挺好用的，就试了试，在腾讯云开了个专门用于wordpress的服务器，配置非常方便，主题也十分丰富，可定制化程度也很高，还有杂七杂八的插件，感觉WordPress也算是不小的项目了。 所以以后就常驻在WordPress上面啦，文章日记或者别的什么乱七八糟的东西就在这上面写了。 令人_愉悦_的是，虽然WordPress是用php写的，但是并不需要我写php，我拒绝php的方针不需要动摇。","link":"/2017/06/25/first/"},{"title":"Git 算是比较系统的学习笔记","text":"Git 基础git是一个分布式的版本管理工具。所谓的分布式就是每个人的电脑上都可以安装完整的git管理系统，进行本地的版本管理，而不需要中央服务器的集中管理。 git不同于其他版本管理系统，它保存的是一个项目中所有文件的集合的快照，如果文件没有被修改那就用一个指针指向之前那个版本。 重点：三种状态 工作区：就是直接在资源管理器中显示的区域。 暂存区：保存了下次将要提交的文件的信息的一个文件。 Git仓库：保存项目元数据和对象数据库的地方。 基本的工作流程是这样子的： 在工作目录中修改文件。 暂存：将文件快照加入暂存区。 提交：将快照存入Git仓库中。 也就是说，一个文件有四种状态：没有修改，已经修改但没有暂存，已经暂存但没有被提交，已经提交。 安装Git此部分省略。。。。。。 初次配置Git的配置文件有两个版本：当前用户版本和系统版本（类似于用户环境变量和系统变量这样吧）。用户配置文件在~/.gitconfig，系统配置文件在Git安装目录下（linux在/etc/gitconfig） 用户信息配置12$ git config --global user.name \"Polydick\"$ git config --global user.email zdk_cz@sina.com 默认文本编辑器配置1$ git config --global core.editor code #这里用visual studio code 查看配置12$ git config --list #查看所有的配置项$ git config #查看配置项 获取Git仓库得到一个仓库有两种方法：1.从远程服务器下载。2.将本地的项目导入git中。 如果要将本地的项目文件夹使用git进行管理： 1$ git init 如果这不是一个空项目，那就需要把当前文件夹中的文件加入到git中。git add追踪文件，git commit提交文件。 $ git add *.c $ git add LICENSE $ git commit -m 'initial project version' 如果是想要从远程服务器上获取到话，git clone会从远程拷贝文件到本地到当前目录中。 记录更新两种状态：已跟踪和未跟踪。已跟踪意味着该文件到修改将会被git记录，将会被纳入项目快照中，未跟踪的话git是不会搭理这个文件到。 一个文件就会在这四个状态中循环往复。 1git status #可以查看当前项目下文件的状态。 README这个文件有一部分变更被暂存了，还有一部分变更没有暂存。如果要暂存新的这一次变更，就要再git add一次。 123456$ git status -s #可以输出更加可读的信息。 M README #被修改，未暂存MM Rakefile #已暂存，又被修改A lib/git.rb #M lib/simplegit.rb #被修改，已暂存?? LICENSE.txt #未追踪 在.gitignore中可以添加要忽略的文件。被忽略的文件对于git来说是不存在的。 git diff可以将暂存提交与否精确到行的级别。此命令比较的是工作目录中当前文件和暂存区域快照之间的差异， 也就是修改之后还没有暂存起来的变化内容。 1git diff --cache #查看将要暂存到内容，即下次将要提交到内容。 1git commit -m \"终于可以提交了好开心，这是一条提交信息。。。\" 12git rm -f somefile #从工作区和暂存区中删除这个文件。不在暂存区中的文件不会被追踪。git rm --cache somefile #从暂存区中删除这个文件。 1git mv old_file_name new_file_name #用来改名的。 查看提交历史1git log #查看提交历史到 12345678910111213141516171819zdk@zdk-X550JX:~/testGit/simplegit-progit$ git logcommit ca82a6dff817ec66f44342007202690a93763949Author: Scott Chacon Date: Mon Mar 17 21:52:11 2008 -0700 changed the verison numbercommit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7Author: Scott Chacon Date: Sat Mar 15 16:40:33 2008 -0700 removed unnecessary test codecommit a11bef06a3f659402fe7563abf99ad00de2209e6Author: Scott Chacon Date: Sat Mar 15 10:31:28 2008 -0700 first commitzdk@zdk-X550JX:~/testGit/simplegit-progit$ 列出的信息包括每次提交的检验和，提交者的邮箱，日期，说明。 撤销操作12git reset HEAD #取消某个文件的暂存。该文件变为修改未暂存的状态。git checkout -- #迅速还原某个文件的修改。 远程仓库12345git remote #列出仓库，clone的仓库至少有origin。git remote add #添加一个远程仓库。git fetch #从远程拉取一个仓库到本地仓库，但不会合并。git push #向远程推送。git remote show #查看远程仓库信息。 重命名和去除，操作还是很常规的。 12git remote rename git remote rm 打标签有两种标签，轻量标签就是一个不会被改变的分支，还有一种是附属标签，就好像一个文件，能够被提交。 12345git tag -a v1.4 -m 'my version 1.4' #添加一个附属标签，名字是v1.4，附属信息为my verison 1.4。git show v1.4 #输出标签信息。git tag v1.4 #给当前的仓库添加一个轻量标签。git tag -a v1.4 a3f4bc #给过去的一次提交添加一个轻量标签。git push origin --tags #把不在远程仓库上的标签都推送到远程仓库上去（push是默认不带标签的）。 git分支简介 上图是某次提交之后git保存的对象结构，其中commit对象保存这次提交的基本信息，tree对象保存目录结构，blob对象保存文件快照。经过多次提交之后，就会有一个commit对象组成的单向链表，如下图所示。 而git所谓的分支就是某个指向commit对象的指针 创建一个testing分支 1git branch testing 一个特殊的指针：Head指针，指向当前所在的分支。在分支就是指针的前提下，切换分支起始就变成了把指针指向新的commit对象这样的简单操作。 1git checkout testing 这时候，当前的分支就变成了testing分支，所有的提交都会被提交到testing分支上，做一些修改，然后commit，head指针就会指向master之后的一个新的commit对象上。 切换分支所导致的切换commit对象会使得工作区内的文件内容发生变化，也就是说如果在命令行里切换了分支，文件资源管理器以及某个编辑器甚至ide中显示的当前目录结构会发生变化。 这时，如果再切换到master分支，然后做一些修改，但是修改和之前testing分支上的修改内容不同。再暂存，提交一次。此时就会出现分叉。 1git log --oneline --decorate --graph --all //查看当前项目的提交记录，以及分支记录，以图形的形式展示。(下图是另一个项目的log，和上图无关)。log输出的图还是挺漂亮的，就是看不太懂。。。 git分支新建与合并因为分支就是指向commit对象的指针，所以分支的新建与合并也可以转换成指针的重新指向以及合并成新的提交。 从上图的场景出发，假设我们要解决#53问题，为了保证解决问题过程中修改的代码不会污染原来的代码，我们新建一个分支iss53。有人说既然每次提交都是有快照的，那我直接在master分支上做增量的修改不行吗，反正到时候也是可以恢复的。 我的理解是分支的意义就是为了能够更加清楚的描述工作流中的代码修改历史，多个分支可以确保多个开发工作流能够同时并行，并且代码互不干扰。就像上述场景中，如果想要同时解决#54问题和#55问题，如果都是在master分支上进行修改，那么代码就会改的很乱，并且在后期的测试和代码审查中也会出现不必要的麻烦。 在iss53分支上做一些修改，并提交，这样iss53分支就会在版本上超前于master分支，成为了master分支的子分支，不过我更喜欢叫“未来的分支”。 这时候，如果出现了一个bug，需要紧急修复代码，但是iss53分支还没有开发完成，更别提代码审议和测试，是不可能合并到master分支上的。这时候分支的并行能力就体现出来了。 提交iss53分支，有多少提交多少，否则工作区和暂存区的内容会冲突。 切换到master分支。 创建另一个叫hotfix的分支。 在hotfix分支上修改代码，解决bug，并提交。 此时master分支就会有两个“未来分支”，分叉了。 假设hotfix上的代码通过了审议，经过了测试，可以合并到master里面去了。切换到master分支，然后merge hotfix一下。 由于hotfix是master的直接上游，因此这样的合并只要让master分支前移几个commit对象就行了，不会出现什么冲突的。在git输出中会出现fast forward字样 hotfix存在的意义以及完成了，可以删掉他了。再切换到iss53分支，做一些修改，做一次commit。想要合并。 可以看出，这时候iss53分支和master分支的关系比较复杂，像是叔叔和侄子。只有父子关系的分支才能直接forward合并，这种叔侄关系的合并就比较厉害了（先切换到master，然后再merge iss53）： git会自动合并两个分支的内容，并自动commit这个合并之后的内容。现在master分支指向新的commit，并且这个新的commit成为iss 53分支的“未来分支” 这里master和iss53的公共祖先是c2，其实是git合并的是c2、c4和c5这三个commit。 现在iss 53分支的意义也以及完成了，可以说再见了。 git分支管理1234git branch //罗列当前所有的分支git branch -v //罗列当前所有的分支以及最新一次提交信息git branch --merged //查看当前分支以及已经合并到当前分支的分支，--no-merged同理git branch -d [branchName] //删除某个分支，如果没有合并过的话可能需要-D来强制删除 git分支开发工作流和","link":"/2017/09/07/git-2/"},{"title":"Git 面向项目开发的git教程","text":"Git是一种版本控制工具，Github是提供git服务器支持的一个社区。 首先在windows上安装githttps://git-scm.com/ 这个是git的官方网站。下载过程可能比较缓慢，需要耐心等待一下。在下载的过程中可以顺便把Github账户注册了。 注册Github账户https://github.com/ 这个是github网站，怎么注册就不多说了，大家都是注册过千万网站账号的人。 配置ssh密钥在git安装包下载完成并安装完成之后，启动git bash。输入命令 1ssh-keygen.exe -t rsa -C \"注册github的邮箱\" 后面会先后提示要求输入密码保存的文件，密码等。这里都不要输，直接回车就行了。 登陆github，点击用户头像，进入settings，选择ssh and gpg keys。新建一个key，title可以随便写，keys要填写的是当前用户目录下（形如c/Users/zdksc)的.ssh文件夹下的id_rsa.pub文件中的内容（全部复制过来）。提交，需要再输入一遍github密码。 第一次初始化仓库选择一个合适的目录，运行命令： 1git clone git@github.com:polydick/OrderDishSystem.git git会把该项目下载到当前路径下。 请先创建自己的分支，在自己的分支上提交代码。运行命令： 1git checkout -b yourOwnBranch 这行代码会建立一个叫yourOwnBranch的分支，并切换到该分支。然后就可以进行自己的开发了。 提交更改如果对该项目的进行了修改之后需要提交，在OrderDishSystem文件夹下运行命令（#后面的是注释，不要输入）： 12345678git checkout yourOwnBranch # 切换到自己的分支git add . # 将当前目录加入暂存区（包括所有新增的文件）git commit -m \"这次提交需要说些什么（比如修复了什么bug）\" # 将变更提交到本地仓库，并加上注释。git checkout master # 切换到主分支git pull origin master # 将origin远程仓库中的master分支下载到本地，并与本地的master分支合并，这样你本地的master分支就是最新的了。git checkout yourOwnBranchgit merge master # 把本地yourOwnBranch分支与本地master分支合并，这样本地的youOwnBranch分支就包含最新的master分支进度和自己写的代码的进度了。git push origin yourOwnBranch # 提交到origin远程仓库的yourOwnBranch分支 及时更新自己的或者别人的代码如果别人修改了代码，想要及时更新到自己的本地，运行命令： 12git checkout mastergit pull origin master # 这样本地的master分支就是最新的了。 如果想要在别人写的最新的代码版本上添加自己的代码，运行命令： 12git checkout yourOriginMastergit pull origin master 然后就可以愉快地在自己的分支上码代码了，码完了提交一下。 注意 不要让自己的分支和除了master分支之外的其他分支合并，那样会变得很乱，非常乱。 在commit之前请先确保在自己的分支上，不要直接commit到主分支。","link":"/2017/09/04/git/"},{"title":"Gradle——从各个“关键字”理解gradle","text":"buildscript { ext { springBootVersion = ‘2.0.1.RELEASE’ } repositories { mavenCentral() } dependencies { classpath(“org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}”) }} apply plugin: ‘java’apply plugin: ‘eclipse’apply plugin: ‘org.springframework.boot’apply plugin: ‘io.spring.dependency-management’ group = ‘com.example’version = ‘0.0.1-SNAPSHOT’sourceCompatibility = 1.8 repositories { mavenCentral()} dependencies { compile(‘org.springframework.boot:spring-boot-starter-web’) testCompile(‘org.springframework.boot:spring-boot-starter-test’)} 以一个Spring Boot项目的build.gradle为例。 buildscript指明了作用于该脚本文件的一些代码。这些代码和buildscript外的代码有所区别，外面的代码是作用于project的，buildscript内的代码是作用于该脚本文件的。 如 1234repositories { mavenCentral()} 这句话如果放在外面，就是声明project所需要依赖文件所在的仓库，放在buildscript里面，就是这个脚本文件所依赖的内容的仓库。 举个例子，比如我们要在运行这个build.gradle的过程中调用matplotlib画一张图（这是什么鬼需求），就需要在buildscript里注明所需要的仓库和所需要的依赖（虽然这个依赖实际上并不存在）。matplotlib仅仅是被导入到了build.gradle中，并没有被导入到项目中去，也就是说项目的依赖还是纯净无污染的。 apply plugin插件实际上就是一组task，只是将task做了一次包装，使得分发这些task的代码给他人使用时能够更加方便。 以插件Java为例 典型的Java项目的目录结构如下： group包名 version版本号 sourceCompatibility兼容到哪个Java版本的一个标记 repositories仓库，这是一个挺重要的关键字。如果开发者需要引入别人开发的库，就需要从仓库下载，得益于gradle高度自动化的依赖管理，开发者不需要自己在浏览器下载然后倒入jar包，而是只要在gradle脚本中声明一个依赖库即可。gradle自动从仓库搜索这个依赖库、下载并导入到该项目中。 常用的仓库有以下几个： 要使用某个仓库的话： repositories { mavenCentral()} 如果想要直接导入本地的jar包，就需要声明本地仓库。 repositories { flatDir { dirs ‘lib’ } flatDir { dirs ‘lib1’, ‘lib2’ }} dependencies依赖，这也是一个很重要的关键字。开发者引入的第三方库并不一定是在编译时需要导入的，也有可能是运行时，或者测试时，因此gradle提供了一些属性，供开发者确定这个依赖何时引入。 compile：编译时 runtime：运行时 testCompile：测试代码（src\\test目录下）编译时 testRuntime：测试代码运行时 archives：项目打包时 需要什么库，直接写在dependencies标签下就行 dependencies { compile “com.android.support:appcompat-v7:${SUPPORT_LIBRARY_VERSION}” compile “com.android.support:cardview-v7:${SUPPORT_LIBRARY_VERSION}” compile “com.android.support:recyclerview-v7:${SUPPORT_LIBRARY_VERSION}” compile “com.android.support:design:${SUPPORT_LIBRARY_VERSION}”}","link":"/2018/04/18/gradle-2/"},{"title":"Gradle——从groovy语法角度理解Project","text":"这是一个Spring Boot项目的build.gradle文件。 123456789101112131415161718192021222324252627282930buildscript { ext { springBootVersion = '2.0.1.RELEASE' } repositories { mavenCentral() } dependencies { classpath(\"org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}\") }}apply plugin: 'java'apply plugin: 'eclipse'apply plugin: 'org.springframework.boot'apply plugin: 'io.spring.dependency-management'group = 'com.example'version = '0.0.1-SNAPSHOT'sourceCompatibility = 1.8repositories { mavenCentral()}dependencies { compile('org.springframework.boot:spring-boot-starter-web') testCompile('org.springframework.boot:spring-boot-starter-test')} 首先，要明确一点：在这个文件中所见到的如apply、repositories、dependencies这样的看似是gradle关键字的东西，其实并不是关键字，而是函数，具体的说是Project对象自带的成员函数（Java中叫对象的方法）。这个Project对象，就是build.gradle所在的目录。每个gradle项目都有至少一个Project，那么gradle项目的根目录在什么位置呢？在settings.gradle所在的目录。 也就是说，这个build.gradle定义了一个Project对象，这个Project对象本身自带了一些函数，在build.gradle里调用了这些函数，并且是带着一些参数调用这些函数。 如 1apply plugin: 'java' 这句话如果用groovy语言补全了，就是 1apply([plugin:'java']) groovy用[]来定义map对象，也就是说这个apply的参数是个map。 再比如 1234dependencies { compile('org.springframework.boot:spring-boot-starter-web') testCompile('org.springframework.boot:spring-boot-starter-test')} 可以还原为 1234dependencies ({ compile('org.springframework.boot:spring-boot-starter-web') testCompile('org.springframework.boot:spring-boot-starter-test')}) 这个{}不是map，不要和python搞混了。groovy中用{}来定义闭包。闭包换种说法叫匿名代码块，即没有名字的函数。和C++中的代码块类似，区别在于闭包和lambda表达式比较接近，可以定义闭包的参数。 1{int a,int b->println 'a + b = ${a+b}'} 这就是一个简单的求a+b并打印出来的闭包。 回到dependencies，这个闭包作为一个参数传入dependencies，即gradle调用dependencies函数的时候，会把这个闭包对象传递到dependencies函数里面去，至于什么时候执行，有dependencies内部决定。 根据我的理解，把这个build.gradle翻译成python脚本文件，应该是这样。 （可能有语法错误，意思到了就行） 1234567891011121314151617181920212223242526272829303132333435363738project=Project()def buildscript_closure(): def ext_closure(): springBootVersion='2.0.1.RELEASE' def repositories_closure(): mavenCentral() def dependencies_closure(): classpath(\"org.springframework.boot:spring-boot-gradle-plugin:${springBootVersion}\") ...... ext_closure() ...... repositories_closure() ...... dependencies_closure()project.buildscript(buildscript_closure)project.apply([plugin:'java'])project.apply([plugin:'eclipse'])project.apply([plugin:'org.springframework.boot'])project.apply([plugin:'io.spring.dependency-management'])project.group='com.example'project.version='0.0.1-SNAPSHOT'project.sourceCompatibility = 1.8def repositories_closure(): mavenCentral()project.repositories(repositories_closure)def dependencies_closure(): compile('org.springframework.boot:spring-boot-starter-web') testCompile('org.springframework.boot:spring-boot-starter-test')project.dependices(dependencies_closure) 所以build.gradle中的groovy看似很像DSL（Domain Specified Language），实际上也就是语法糖多了一点而已。","link":"/2018/04/18/gradle/"},{"title":"HTTP协议详解","text":"HTTP协议应该是如今互联网中使用范围最广的协议了。现在对HTTP协议进行一个全面的分析，力求能够把常用的协议功能都覆盖到。 1 概述HTTP协议全名为超文本传输协议，英文名为Hypertext Transfer Protocol，不出意外也是一个外国人发明的网络传输协议。这个协议大概在1960年左右被构思出来，后来经过多方合作，不断地被修改和完善，最终进入RFC，成为互联网标准之一，应该是和TCP协议重要性相当的。 RFC 1945 - HTTP1.0RFC 2616 - HTTP1.1 HTTP协议被广泛运用于浏览器访问Web网页的场景中，当然其灵活性也使得它能够在其他领域发挥优势比如API、RPC。 1.1 协议栈如今互联网绝大多数主机使用的是TCP/IP协议，这个协议有5层，分别是：物理层、链路层、网络层、传输层和应用层，所谓的4G、5G、WIFI为物理层技术；链路层常用协议有以太网、X.25、令牌环等（实际上除了以太网之外其他的链路层协议都已经基本上不存在了，只是在计算机网络课上介绍过而已）；网络层协议为IP；传输层协议为TCP或UDP；到了应用层，协议就多种多样了，常用的有FTP、HTTP，任何一个基于Socket编写的应用程序都可以说是实现了一个应用层协议，或者说工作在应用层上。 HTTP协议属于典型的应用层协议，其工作在应用层之上，屏蔽了TCP层的接口，同时应用程序可以直接使用HTTP协议进行通讯。虽然HTTP协议看起来不复杂，用起来也挺方便，但是要时刻记住它还是基于TCP/IP协议运作的，只是将下层的细节屏蔽了而已，但是真正运行起来它还是要遵循下层协议的规则，如TCP层的拥塞控制、IP层的报文分段（MSS）、IP寻址等等。 如今将HTTP协议置于TLS层之上，变为HTTPS，也是一种较为流行的加密网络通讯协议。有关HTTPS协议的细节以后再谈，今天只看HTTP明文通讯。 1.2 运作流程以浏览器地址栏输入www.baidu.com并回车为例，一次典型的HTTP协议通讯过程如下： 1. 浏览器通过DNS协议解析得到baidu.com主机的IP地址（有关DNS的细节这里也不谈）。 2. 浏览器于该IP地址建立TCP连接，即Socket连接，HTTP服务一般是运行在80端口上，即本地的一个随机端口和远程服务器的80端口进行连接，需要进行3次握手（SYN、SYN+ACK、ACK）。 3. 在发送了三次握手的最后一次ACK报文之后，正式开始发送HTTP请求，这里可以先把请求理解为一个字符串，或者一个比特流，在该比特流外部包裹TCP头部、IP头部、以太网头部，然后经过网线发送给服务器。 4. 服务器逐层拆包，得到真正的HTTP请求内容后，根据请求内容中的具体信息，从服务器的硬盘或者内存中读取HTTP请求中所明确的具体资源（百度首页的html文件）作为响应内容，再次以比特流的形式，逐层打包，返回给客户端浏览器。 5. 浏览器逐层拆包，得到响应内容，经过浏览器渲染进而呈现出来。 以上是使用浏览器访问www.sina.com的过程中与服务器之间往返的数据。可以看到第四个报文才是HTTP请求报文，三次握手大概用了0.1s的时间，从HTTP请求到收到响应大概用了0.26s的时间，延迟还是很小的。当然到了HTTPS，由于交换证书和协商密钥的需要，又要花费更多的时间来建立连接（2-3个RTT）。 2 HTTP协议格式远程协议，最重要的就是协议格式，以太网、IP、TCP都有一套相当严格但又可扩展的格式，HTTP也有。 2.1 请求/响应格式请求和响应统称为HTTP消息，他们的格式其实这么形容： 首先是一个开始行，然后是消息头（包含0个或多个，每个头部以回车换行结尾），接着是一个CRLF（回车换行），最后是消息体（可选）。 消息头部中有一些头部字段是请求和响应都可以使用的，称为通用头部。 2.1.1 通用头 GeneralHeader Cache-Control：指定缓存机制，请求和响应头中都可以使用该字段。请求头：no-cache、no-store、max-age、max-stale、min-fresh、only-if-cached，响应头：public、private、no-cache、no-store、no-transform、must-revalidate、proxy-revalidate、max-age。 Connection：由于HTTP1.1默认使用长连接，因此如果要拒绝使用长连接，可以将该字段设为false。只要有一方的头部的false就不会使用长连接。 Via：报文经过的中间节点，网关或者代理。 Trailer：有一些字段的值可能是随着发送动态生成的，比如Expire，Date，Trailer允许指定某个字段的值被推迟到某个响应体的数据块的尾部。 Upgrade：希望升级的协议信息。 2.2 请求格式 2.2.1 请求行2.2.1.1 URI全名为统一资源定位符，明确指定了一个资源或者多个资源的所在的位置。URI可以是相对的也可以是绝对的，如之前访问sina的请求中URI就是相对的，即服务器根目录，而具体的主机地址则在请求头的Host字段中。 2.2.1.2 Method如今流行一种成为Restful风格的后端API设计风格，这其中就充分利用了Method字段，Method字段描述了这个HTTP请求的动作意图，是查询数据（GET）、提交数据（POST），还是删除数据（DELETE），甚至是只请求响应的头部而不要响应体（HEAD）。从语义上来说Method字段并没有明确的规范，如果你同时负责了客户端和服务器端的开发，那你完全可以随意约定，只需要保证一致性即可。当然尽量符合语义来总归是好的。 2.2.1.3 HTTP-Version采用的HTTP协议的版本号，常用的有1.0、1.1、2.0，不同的版本号代表客户端可以接受的HTTP协议版本，版本越低功能越low。 2.2.2 请求头 RequestHeader除了通用头部之外，请求头还可以使用如下字段： Accept：请求方希望接收的数据类型，常见的有text/html，application/json，以及通配符/。 Expect：客户端在发起请求前可以在该字段中注明将要进行的操作，然后将这个请求发给服务器作为一个握手信息，等到服务器返回接受后再发送真正的请求。在POST大对象的时候常常使用此字段来避免传输无效的流量。 Host：主机名。 Range：请求方希望接收的是完整响应文件的某一段数据，那么可以通过该字段来指定第一个字节和最后一个字节的偏移，这个字段常常用于HTTP断点续传。 Referer：从哪个链接过来的。服务器通过限制该字段可以防盗链。 User-Agent：标识了浏览器相关信息。服务器可以通过该字段来判断访问的是PC还是移动设备，或者根据更加详细的信息来返回充分适配的网页资源。 2.2.3 实体头 EntityHeader Allow：严格限定该URL能够接受的method。 Content-Encoding：编码方式，如gzip、x-gzip压缩。 Content-Length：数据实体的大小，用十进制整数表示。注意这个数字是数据实体的大小，和是否压缩无关。 Content-Type：数据实体的媒体类型，格式为 Type/SubType，如 text/html。 Expires：一个时间。既然HTTP是一个远程协议，那就会出现分布式系统中一定会出现的数据一致性的问题。Expire字段用于标识该资源的有效期，客户端能够在该有效期内缓存该资源。 Last-Modified：该资源上次被修改的时间，类似于文件属性的修改时间。当然如果资源实体的类型不同，也能够表示成别的意思，如某个数据表的某一行被修改的时间，某个实体的某个组件被修改的时间。 2.2.4 请求体请求也是可以 2.3 响应格式响应格式其实和请求格式大同小异。 2.3.1 状态行 响应报文的状态行的格式为HTTP版本+状态码+ReasonPhrase， 其中最重要的就是状态码。 RFC认为应用程序不必完全理解每一个状态码，但是必须能够区分状态码是什么类型的。 2.3.2 响应头2.3.3 响应体3 字段重新整理 X 常见问题X.1 GET和POST有什么区别","link":"/2018/06/30/http/"},{"title":"爱奇艺视频广告流量分析","text":"按照老师教的步骤，在wifi环境下抓安卓端的爱奇艺app的数据包，并从中分析出广告的流量。 三生三世十里桃花01 首先进行tcp会话的统计，我比较喜欢把rel start和duration两栏拉长： 先按照数据量排序： 好像看不出什么来，老师的经验之谈是高清的视频1s大概100kb。 获得的高清感观标准：720p、1080p，数据密度：0.25-0.35及以上，那么码率大致在5Mb/10Mb及以上，代表性作品，各高清小组的BD720p/BD1080p 。 获得的标清感观标准：a480p、480p，数据密度：0.12-0.18及以上，那么码率大致在1200-2500kbps，代表性作品，各小组的BDRip/DVDRip . 结合数据量和开始时间一起看前几个： 数据量第一的是在48s开始的，而且48s左右开始的连接特别多。整个过程中48s和84s是两个集中的爆发点，在1s内会有大概三四十个连接开启。 如果结合IO流量图，也可以看出些端倪： 可以看出数据较为集中的地方有三处，第一处是0-8s。我怀疑这其中就有广告。所以再按开始时间排序，找数据量较大的。 发现一个规律，一般视频流量，都是数据量较大（不一定是最大），但持续时间很短的。在把0-8s中的4个1Mb以上的流量一一跟踪之后，找到了： 这个请求get了一个f4v的视频片段，下载下来发现是15s，这个请求开始于7s，也就是22s左右结束。假设是在48s开始正式加载视频，那说明前面应该还有一两个15s的广告。继续找22s左右的。 这里出了一个意外，一个5.6s开始的900k的流量也是一个15s的视频。这下子好了，从6s开始，已经有了30s的广告储备了。在27s又发现一个15s的视频。然后就要到48s的正式视频传输了。 广告开始时间(s) 数据量(b) 5.7 942k 7.3 1621k 27.8 1625k 今天先这样，明天再看看别的数据包，找找规律。 震惊！实际播放中竟然看到了4段广告，我找漏了一个！可能是我第一次找广告没什么经验吧。","link":"/2017/09/13/iqiyi/"},{"title":"Java Collection——ArrayList","text":"123public class ArrayList extends AbstractList implements List, RandomAccess, Cloneable, java.io.Serializable{} ArrayList继承了AbstractList，实现了List、RandomAccess、Cloneable、Serializable接口。 RandomAccess接口定义里没有函数，他只是一个标记。如果一个实现了List接口的类支持快速的随机访问（一般是常数时间的），那就要用实现这个接口。或者说如果 12for (int i=0, n=list.size(); i < n; i++) list.get(i); 比 12for (Iterator i=list.iterator(); i.hasNext(); ) i.next(); 快，那么就要实现该接口。对于使用该类的开发人员来说，在需要遍历一个List的时候，可以通过instanceOf判断是不是RandomAccess，如果是的话，用get(i)遍历更快，如果不是，用迭代器next()遍历更快。 ArrayList类似于C++中的vector，表示在内存中顺序存放的一组元素组成的数组。 属性比较重要的属性有： Object[] elementDat：实际存放数组元素的数组。 DEFAULT_CAPACITY：默认的数组大小。 size：数组中实际存放的元素个数（不一定等于数组大小）。 方法有三个构造函数，如果在构造函数中注明存放的元素个数，就会new一个这么大的数组；如果不注明元素个数，那么就不new数组，而是直接用一个静态空数组来表示存放内容；如果在构造函数中传入另一个collection对象，则会把所有的那个collection对象中的所有元素拷贝一份，作为本对象的存放内容。 常用的几个方法有： set随机修改数组元素，只需要常数时间。 get随机访问数组元素，只需要常数时间。 add添加一个元素，可以以null为参数。 add如果在数组末尾添加元素，只需要常数时间，如果是在数组中部插入新元素，需要让插入位置以后的所有元素都向后挪动，所以就需要线性时间了。 在数组的末尾或者指定位置添加一个元素，在添加之前会先检查容量是否满足需求，即capacity是不是比size大1，如果放不下，就要考虑扩容到指定长度的问题了。 ensureCapacityInternal具体而言，判断的逻辑是这样的：先手动指定一个数组最小长度（比如，在add之前，指定这个数组最小长度为size+1），看这个数组是不是默认大小的空数组，如果是的话，取指定数组长度和10（默认空间大小）的较大值；如果不是默认大小的空数组，就直接取指定的数组长度。这个值作为新的capacity，如果新的capacity比当前的capacity大的话，就要扩大数组了。但并不是直接扩大到新的capacity，而是看当前的capacity的1.5倍和新的capacity谁大，取较大的那个值作为最终扩容的数组长度。 同时扩容的时候会检查扩容后的数组长度是不是比最大长度大（MAX_ARRAY_SIZE），如果是，检查指定的数组长度是不是溢出了，如果是，就抛出异常，如果没有溢出并且指定的数组长度比MAX_ARRAY_SIZE，就取Integer.MAX_VALUE作为新的数组长度，否则就取MAX_ARRAY_SIZE作为新的数组长度。也就是说在数组长度较大的时候，长度要么是Integer.MAX_VALUE，要么是MAX_ARRAY_SIZE。 1234567891011private void grow(int minCapacity) { // overflow-conscious code int oldCapacity = elementData.length; int newCapacity = oldCapacity + (oldCapacity >> 1); if (newCapacity - minCapacity < 0) newCapacity = minCapacity; if (newCapacity - MAX_ARRAY_SIZE > 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity);} 比如：如果我调用默认构造函数，那得到的数组的capacity=0，size=0，如果我再add一次，那么我需要的size应该是1，但是由于当前数组是默认空数组，情况比较特殊，新的capacity是10，10>capacity，要扩容，capacity=0，1.5倍还是0，显然是10比较大，所以最终的数组长度就为1，并且把新的元素加add到最后一个。 如果现在有个ArrayList，size=20，capacity=32，add一个新的元素，需要的最小数组长度应该是20+1=21，21比32小，那就不用扩容了。 如果有个ArrayList，size=20，capacity=20，add一个新的元素，需要的最小数组长度应该是21，21>capacity，要扩容，扩容到30。 addAll同样要检查数组长度，有必要的话需要扩容，既可以在数组末尾添加元素，也可以在某个位置添加，显然需要线性时间，因为需要深拷贝。 添加元素的顺序即为参数Collection的迭代器的顺序。 remove删除某个位置的元素，或者删除数组中的某个对象。二者都需要线性时间。可以以null为参数。 remove(Object)会通过equals从数组中找到第一个和参数相等的元素，然后移除他。 值得注意的是，remove中并没有数组缩容的逻辑，也就是说ArrayList只会变大不会变小。这点值得再继续研究一下。 写了一小段代码来测试一下ArrayList扩容的原理，因为elementData是protected的，所以要用反射才能获取到capacity。 123456789101112131415161718192021222324252627282930313233343536373839404142package collection;import java.lang.reflect.Field;import java.util.ArrayList;import java.util.List;/** * Created by intellij IDEA.But customed by hand of Dokyme. * * @author dokym * @date 2018/2/9-13:07 * Description: */public class LearnArrayList { private static Class arrayListClass; private static Field elementDataField; public static void main(String[] args) { try { arrayListClass = Class.forName(\"java.util.ArrayList\"); elementDataField = arrayListClass.getDeclaredField(\"elementData\"); elementDataField.setAccessible(true); List elements = new ArrayList(); for (int i = 0; i < 100 ; i++){ elements.add(new ListElement(\"Dokyme\")); printListCapacity(elements); } for (int i = 0; i < 80; i++) { elements.remove(elements.size() - 1); printListCapacity(elements); } } catch (Exception e) { e.printStackTrace(); System.exit(1); } } private static void printListCapacity(List listElements) throws Exception { System.out.println(\"Capacity is \" + ((Object[]) elementDataField.get(listElements)).length); }} 在测试的的过程中的确没有发现缩容的现象。 batchRemove(Collection)能够批量删除本数组中所有出现在参数中的元素。 indexOf查询一次需要线性的时间复杂度，可以以null为参数。 forEach、removeIf、replaceAll为函数式编程提供了支持。 sort调用Arrays.sort方法进行排序。排序算法今天暂时先不研究了。。。 迭代器ArrayList有两个迭代器的内部类，一个是Itr，一个是ListItr，后者比前者的方法多一些。 Itr实现了next()、hasNext()、remove()、forEachRemaining()这些方法。 ListItr在Itr的基础上，多了add()、set()、previous()、xxxIndex()等方法。 ArrayList的方法中没有出现对并发访问的控制，基本上没有见过哪里抛出了ConcurrentModificationException，但是在迭代器的方法中却经常见到抛出这个异常的代码。 会导致这个异常的最经典的场景就是在一个for循环中一边用迭代器遍历数组（或者用增强for循环）一边给数组添加或删除元素。 ConcurrentModificationExceptionArrayList在add和remove的时候会自增一个modCount，这个modCount属性记录的是本对象的结构被修改的次数。迭代器也有一个相同功能的expectedModCount，在构造迭代器的时候，expectedModCount和modCount的值是相等的，一旦在循环体中调用了ArrayList的remove(index)或remove(Object)方法，使数组结构发生了变化，两个modCount就不一样了。 next函数的第一行就是检查两个modCount是否一致，不一致就抛出异常了。 123456789101112131415161718192021222324252627282930313233343536@SuppressWarnings(\"unchecked\")public E next() { checkForComodification(); int i = cursor; if (i >= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i >= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i];}public boolean hasNext() { return cursor != size;}public void remove() { if (lastRet < 0) throw new IllegalStateException(); checkForComodification(); try { ArrayList.this.remove(lastRet); cursor = lastRet; lastRet = -1; expectedModCount = modCount; } catch (IndexOutOfBoundsException ex) { throw new ConcurrentModificationException(); }}final void checkForComodification() { if (modCount != expectedModCount) throw new ConcurrentModificationException();} 以上是java中ArrayList的迭代器Itr的几个函数的定义，其中hasNext就可能因为一边遍历一边修改数组导致返回值出现错误。 但也并不是就不能一边遍历一边添加删除元素了，Itr提供了remove方法，用于删除迭代器当前指向的元素，这个remove方法实际上是调用了ArrayList的remove方法，并修改expectedModCount到与modCount一致的状态。 当然，Itr的remove方法是要有条件的，就是在remove之前至少要有一次next方法，否则lastRet无法得到正确的值，使得迭代器删掉了这个元素之后不知道该指向哪个元素了。 当然，一边遍历一边添加删除数组还有另一种写法，就是依靠ArrayList的remove函数和传统的for循环，不使用增强的for循环或迭代器。 又写了一段代码测试remove的特性。 10个元素，分别是0-9，四次不同的循环，分别删除3，4，5，6。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798package collection;import java.util.ArrayList;import java.util.Iterator;import java.util.List;/** * Created by intellij IDEA.But customed by hand of Dokyme. * * @author dokym * @date 2018/2/9-13:07 * Description: */public class LearnArrayList { private static void testTraditionalForLoop(List listElements) { try { //iterate using traditional for loop for (int i = 0; i < listElements.size(); i++) { ListElement cursor = listElements.get(i); if (cursor.getName().equals(\"3\")) { listElements.remove(i); } } System.out.println(\"testTraditionalForLoop\"); System.out.println(listElements); } catch (Exception e) { e.printStackTrace(); } } private static void testForEachLoop(List listElements) { try { //iterator using foreach loop for (ListElement cursor : listElements) { if (cursor.getName().equals(\"4\")) { listElements.remove(cursor); } } System.out.println(\"testForEachLoop\"); System.out.println(listElements); } catch (Exception e) { e.printStackTrace(); } } private static void testIteratorRemove(List listElements) { try { //iterate using iterator,remove using iterator.remove Iterator iterator = listElements.iterator(); while (iterator.hasNext()) { ListElement current = iterator.next(); if (current.getName().equals(\"5\")) { iterator.remove(); } } System.out.println(\"testIteratorRemove\"); System.out.println(listElements); } catch (Exception e) { e.printStackTrace(); } } private static void testIteratorRemove2(List listElements) { try { //iterate using iterator,remove using ArrayList.remove Iterator iterator = listElements.iterator(); while (iterator.hasNext()) { ListElement current = iterator.next(); if (current.getName().equals(\"6\")) { listElements.remove(current); } } System.out.println(\"testIteratorRemove2\"); System.out.println(listElements); } catch (Exception e) { e.printStackTrace(); } } public static void main(String[] args) { try { List listElements = new ArrayList(); for (int i = 0; i < 10; i++) { listElements.add(new ListElement(\"\" + i)); } testTraditionalForLoop(listElements); testForEachLoop(listElements); testIteratorRemove(listElements); testIteratorRemove2(listElements); } catch (Exception e) { e.printStackTrace(); System.exit(1); } }} 实验的结果是，使用传统for循环+ArrayList.remove，删除和遍历都没有问题；使用增强for循环和ArrayList.remove，删除成功，但是抛出了ConcurrentModificationException；使用迭代器循环+迭代器remove，遍历成功并且正确删除了5；使用迭代器循环+ArrayList.remove，抛出了ConcurrentModificationException。 ListItr继承了Itr，并提过了几个额外的方法:previous,set,add。这个add是可以配合迭代器对象在遍历过程中添加元素的。但是同样有前提条件：lastRet必须有效，即在add之前必须调用一次next。 lastRet这个变量很有意思，cursor代表写一个元素的index，那么上一个不就是cursor-1吗，为什么还要专门找一个变量来表示上一个变量的index呢？？？ 注意：Itr还是ListItr，都可以通过ArrayList的方法获得。 其他内部类SubList代表ArrayList的一部分，奇怪的是，没有继承ArrayList，而是直接继承AbstractList并实现了RandomAccess。概念类似于数据库中的视图，即把ArrayList中的内容截取一段，ArrayList和SubList中的数组元素实际上是相同的。任何一处修改，另一处也会更随着变化。 提供的数个方法和ArrayList相差无几（set、get、size、add、remove、addAll），区别在于要比ArrayList的对应方法多一步：检查comidification。 SubList也提供了自己的迭代器，是一个继承了ListIterator的匿名内部类。 ArrayListSplitter Spliterator是一个可分割迭代器(splitable iterator)，可以和iterator顺序遍历迭代器一起看。jdk1.8发布后，对于并行处理的能力大大增强，Spliterator就是为了并行遍历元素而设计的一个迭代器，jdk1.8中的集合框架中的数据结构都默认实现了spliterator。 jdk8的新特性，暂不研究。。。","link":"/2018/02/09/java-collection-arraylist/"},{"title":"JUC——CopyOnWriteArrayList","text":"ArrayList和LinkedList都不是线程安全的。这时候就有必要研究一下CopyOnWriteArrayList了，一个略小众的容器类。 光看这个类的原型好像还挺简单的。 属性 – 就两个属性：一把ReentrantLock（可重入锁），一个Object数组（violatile修饰）。 在多线程的情景下，由于每个线程都有一个自己的工作内存（类似于高速缓存），在多个线程访问同一个变量的时候，可能会出现缓存不一致的问题，如线程1和线程2都把变量a读入到自己的工作内存中，线程1对a执行自增，线程2对a执行自增，这时候本来应该自增两次的a由于缓存不一致只自增了一次。 violatile关键字保证了不同线程对该变量的可见性，即一个线程修改了这个值，其他线程能够立即可见。 ReentrantLock：同一个线程可以多次获取一把锁，但是也需要释放多次，全部释放之后其他线程才能获得这把锁。因为还没有深入研究他的源码，所以这里就一笔带过了。 方法构造函数由于构造函数不太可能会受到多线程环境的影响，因此CopyOnWriteArrayList的构造函数和ArrayList的构造函数差不多，没有针对多线程的特殊处理。 所有只读操作由于只读操作不会修改数组中的元素，因此也没有必要针对多线程进行特殊处理。 CopyOnWriteArrayList提供的只读操作包括： size() isEmpty() contains(Object) indexOf(Object) toArray() get(index) set光看CopyOnWriteArrayList的实现，好像也就比ArrayList多了一个获取锁和释放锁的步骤。 下图是CopyOnWriteArrayList的set。 区别： 0. 先获得锁，在finally中释放锁。 1. CopyOnWriteArrayList没有对index做检查，可能会出现数组越界。 2. CopyOnWriteArrayList是先拷贝了一个副本，然后在副本上修改，最后setArray设置对象的属性。这么做的原因可能是因为violetile对于引用类型只能使得引用指向不同的对象时其他线程可见，如果引用的对象的某个属性变了，或者引用的数组的某个元素变了，是不会触发对其他线程中该引用的变化可见的（和final的概念有点类似）。所以为了修改数组元素之后对其他线程变化可见，只能修改引用的数组。 这就是CopyOnWrite的名字的由来吧。 但这样，开发者可能就会担心效率问题了，毕竟每次简单的修改都要对数组拷贝一遍。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package collection;import java.util.ArrayList;import java.util.List;import java.util.concurrent.CopyOnWriteArrayList;/** * Created by intellij IDEA.But customed by hand of Dokyme. * * @author dokym * @date 2018/2/11-12:00 * Description: */public class LearnCopyOnWriteArrayList { private List copyOnWriteList = new CopyOnWriteArrayList(); private List arrayList = new ArrayList(); private static final int testCount = 1000000; public static void main(String[] args) { new LearnCopyOnWriteArrayList().run(); } public void run() { testCopyOnWriteList(); testArrayList(); } public void testCopyOnWriteList() { System.out.println(\"testCopyOnWriteList\"); long start, end; start = System.currentTimeMillis(); for (int i = 0; i < testCount; i++) { copyOnWriteList.add(\"\" + i); } end = System.currentTimeMillis(); System.out.println(\"Timespan: \" + (end - start)); } public void testArrayList() { System.out.println(\"testArrayList\"); long start, end; start = System.currentTimeMillis(); for (int i = 0; i < testCount; i++) { arrayList.add(\"\" + i); } end = System.currentTimeMillis(); System.out.println(\"Timespan: \" + (end - start)); }} 运行结果如下： 类似的写操作还有： add(index,object)及其他addXXX，虽然也是使用的数组，但是和ArrayList相比，逻辑相当直接，没有采用1.5倍扩容的策略。 remove(index)及其他removeXXX 以remove(index)为例，详细分析一下他的代码： 首先得到一个snapshot（其实就是记录一下引用，称不上快照），接着寻找对应需要被删除的那个object的index，再调用下一个remove函数。 下一个remove函数首先加锁，接着判断快照和当前的引用是不是同一个对象，如果不是，说明有其他线程在indexOf的过程中修改了数组，进入findIndex语句块。 findIndex语句块中首先在current中找有没有和要删除的object相同的，并且这个object的位置已经发生了变化的。如果在current中找到了，就删掉他。如果没有找到，下面有三个if。 如果current的长度比index小了，由于current已经被全部遍历过了，没有object，说明object已经在其他线程中被删掉了，那就直接返回。 如果current的长度比index大，index之前的已经全都找过了，没有object，那再看index处是不是object，如果是，就直接删除。 最后再在current中调用indexOf函数找一遍object，删掉它。 这段代码考虑了object在两个版本的数组中位置有没有发生变化，是否因为其他线程的修改导致object消失等因素。 做类似处理的还有addIfAbsent，其实只要让加锁的范围大一些就不需要写这么复杂的代码了，可能还是处于性能的考虑吧。 forEachforEach没有实现CopyOnWrite，也就是说通过forEach修改某个元素的值在其他线程是不可见的。当然forEach只能得到每个元素的引用，但是不能修改引用。 实际上，通过get方法得到某个元素，然后修改那个元素的属性，这样的行为，对于其他线程也是不可见的。可以说，这个可见性不应该是COWArrayList所应该负责的。 迭代器不同于ArrayList和迭代器类，COWIterator是一个静态内部类，也就是不需要外部类的实例存在也可以实例化的内部类。 虽然该迭代器的形式和ArrayList的迭代器有所区别，但它不是线程安全的。 属性因为是静态内部类，无法接触外部的实例变量，因此有一个COWArrayList的snapshot（一个Object数组）。 此外还有一个cursor记录下一个元素的index。 构造函数负责初始化snapshot。 方法只支持hasNext,hasPrevious,next,previous,nextIndex,previousIndex,forEachRemaining。修改数组元素的方法在这个层面一律不支持。 由于不支持修改，也就没必要实现checkConmodification了。 实际上COWArrayList也并没有记录modCount。 其他内部类COWSubList，作为COWArrayList的视图，也是静态内部类，但是手都持有了以恶搞COWArrayList的实例变量，修改时调用COWArrayList的方法进行修改，如果说COWArrayList是线程安全的话，SubList也应该是安全的。","link":"/2018/02/11/java-collection-copyonwritearraylist/"},{"title":"Java Collection——HashMap","text":"本部分的内容可能会随着java版本的变化而产生较大的变化，因此事先注明：下面的代码和原理都是基于JDK1.8。 Map是一种不同于List、Set和Queue的数据结构，其中存放的每个单元实际上包含两个元素————Key和Value，这两个元素都是开发人员所关心的。Map描述的是Key和Value的对应关系，就如同List中一个index对应一个element，只是Map的Key的类型并不一定是整数，可以是其他任何类型。 HashMap继承了AbstractMap，实现了Map、Cloneable和Serializable接口。 其中Map接口如下： 123456789101112131415161718192021222324public interface Map{ int size(); boolean isEmpty(); boolean containsKey(key); boolean containsValue(value); get(key); put(key,value); remove(key); putAll(map); clea(); keySet(); values(); entrySet(); getOrDefault(key,defaultValue); forEach(action); replaceAll(function); putIfAbsent(key,value); remove(key,value); replace(key,oldValue,newValue); replace(key,value); computeIfAbsent(key,mappingFunction); compute(key,remappingFUnction); merge(key,value,remappingFunction);} 大部分函数的参数和返回值都是Key的类型或Value的类型，少数几个Java 8中的方法为函数式编程提供了支持，能够传lambda表达式为参数。 回到HashMap。 属性静态类属性静态属性中，除了两个用来约束capacity的属性之外： DEFAULT_LOAD_FACTOR默认装载因子，0.75。装载因子是一个很影响Hash容器效率的属性，反映的是Hash容器中数据的疏密程度。装载因子过大，Hash容器中绝大多数桶都放有元素，但每个桶中的链表很有可能会很长，这样在散列冲突的时候进行搜索就会花费一定的时间；装载因子过小，会导致空间利用率过低。 从直观上来说，装载因子决定了capacity的动态调节。 TREEIFY_THRESHOLDHashMap使用的结构是可以动态变化的，可以是List或者Tree，做出选择的标准就是bucket（桶）的数量，如果数量大于这个阈值，就由List变为Tree。 取值为8。 UNTREEIFY_THRESHOLD如果HashMap中桶的数量在remove的过程中逐渐变小，并越过了这个阈值，就untreeify这个map。换言之，使用Tree变为List。 取值为6。 MIN_TREEIFY_CAPACITY如果table的capacity过小，是不允许treeify的，只有capacity超过这个值，并且某个桶的链表长度超过了threshold，才会将那个桶treeify。 实例属性tableNode的列表。 entrySet将一对key-value看做一个entry，在以entry为单位遍历的时候，就需要一个容器来承载所有的entry，entrySet属性用于保存这个容器。它是一个Set threshold下一个需要resize的值。 loadFactor装载因子。size/capacity。 其他内部类Node桶节点，多个Node组成了一个链表。每个Node保存自身的hash、key、value和next。 方法其实各个方法所起的作用大都很简单，无非就是增删改查，重点在于散列的原理和解决冲突的办法。首先来看getNode方法如何根据hash值找到对应的节点。 该函数有两个参数，一个是hash值，一个是key值，hash值用来找到对应的bucket，key值用来验证或解决冲突。 首先通过table[(table.length - 1) & hash]找到hash对应的桶。进行按位与的目的就是为了把hash的空间映射到表空间中，相当于hash%table.length。 检查桶中的节点的hash、key是否和参数一致，确认是否是想要找的那个。 如果通过散列找到的桶中的节点不是所要的节点的话，说明有冲突了，就需要进一步搜索。 如果桶中的节点是树节点，调用getTreeNode方法进行搜索。其实是一颗红黑树。 如果不是树节点，说明是一个链表，按照开链法的底层构造进行遍历搜索，直到找不到，返回null。这里判断是不是树节点使用了instanceof关键字。 其中第一步从table中取对应的桶，使用了按位与运算，将hash映射到table.length-1的空间内。由于table的长度始终是2的整数次方，转为二进制后各位均为1，因此这里的按位与运算从结果上看相当于取余（mod）运算。至于这里使用按位与而不是取余运算，我认为还是处于效率的考虑。 操作Map时，大部分函数都需要key作为参数，在HashMap结构内部通过key计算出一个hash值，hash值的计算过程如下图所示： 通过将key的hashCode方法的返回值无符号右移16位，再与自身异或，得到正式的用于散列的hash值。显然，在这个过程中，hashCode方法起到了较为重要的作用。先右移16位再与自己异或的目的是为了让低16位和高16位能够充分参与到散列键的运算中，16位是个不小的数字。 异或是一个神奇的运算符，他之所以神奇，是因为他和与、或相比，能够使结果最为均匀的分布。就像洗牌洗的最均匀一样。真值表就能说明这个问题： and or xor 0 0 0 0 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 从表中可以看出，and和or得到的0和1的结果个数都不相同，只有xor得到的0和1的个数才是相同的。 使用hashCode的时候，通常要使得分布较为均匀，使用and和or运算都会造成0或者1的偏多或偏少，因此计算hashCode的代码常常用到异或操作。 hashCode()hashCode方法是Object类定义的方法之一，用来返回某个对象的散列值。在Object类的定义中，hashCode的默认实现返回的是该对象所在内存的地址。通常在涉及到HashSet、HashMap、HashTable之类的容器时，需要开发者重新考虑自定义的类的hashCode计算方式。 hashCode的计算需要遵循一些规则，这有助于容器类操作效率的提升以及正确性的保证。 Whenever it is invoked on the same object more than once during an execution of a Java application, the hashCode method must consistently return the same integer, provided no information used in equals comparisons on the object is modified. This integer need not remain consistent from one execution of an application to another execution of the same application.（一个Java程序运行的过程中，同一个对象返回的hashCode必须一致。） If two objects are equal according to the equals(Object) method, then calling the hashCode method on each of the two objects must produce the same integer result.（两个相等（equals）的对象的hashCode必须一致。） It is not required that if two objects are unequal according to the equals(java.lang.Object) method, then calling the hashCode method on each of the two objects must produce distinct integer results. However, the programmer should be aware that producing distinct integer results for unequal objects may improve the performance of hash tables.（不想等的对象的hashCode不必不同，但不同更好。） 针对最后一点，可以假设如下情境： 一系列数个对象返回的hashCode都是相等的，那么HashMap或其他Hash容器散列的时间复杂度接近于线性，因为需要逐个遍历一边。 在网上找到一个计算hashCode的较为合适的模式：通过加入两个质数成分，将一个对象的属性映射到不同的域上面，最后相加得到散列值。虽然这个计算方法中没有用到异或操作，但应该也是值得借鉴的一种写法。 12345678@Overridepublic int hashCode() { int hash = 7; hash = 31 * hash + (int) id; hash = 31 * hash + (name == null ? 0 : name.hashCode()); hash = 31 * hash + (email == null ? 0 : email.hashCode()); return hash;} putVal 如果有多个线程同时运行到上面代码截图的光标所在行，会出现put的值丢失的问题。 检查table是否时null或者capacity是否为0，如果是，resize，调整table，准备放东西进去了。 检查散列的桶table[(n-1)&hash]是否有元素，如果没有，就创建一个节点，把值放进去，就结束了。 散列冲突时，用变量e保存存放该key的位置。 3.1 如果桶中第一个节点的key和hash都一致，则只需要在后续步骤修改该节点的value。 3.2 如果桶中第一个节点是一个树节点（红黑树），调用putTreeVal函数存放键值。 3.3 否则，从第一个节点开始，沿着链表逐个遍历，找到hash和key都一致的链表中的某个节点，如果找不到，就存放在最后一个节点。如果走到头了发现这个链表过长，就会把这个链表变成一颗红黑树，即只有在链表尾部增加节点的时候才会检测是否需要treeify。 对找到的或者确定新增的节点，设定其value。如果是更新而不是新增，就返回原来的value。 putVal的过程用流程图描述如下：（图片来源：https://www.cnblogs.com/softidea/p/7261111.html） 也就是说，经过多次putVal之后的HashMap应该是链表和红黑树并存的情况，画出来就是下面这种结构：（图片来源：https://www.cnblogs.com/zhangyinhua/p/7698642.html） resizeresize用于动态调整table的大小，是HashMap成员函数中代码行数最长的函数，虽然逻辑并不复杂，但是有很多细节需要仔细考虑。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273final Node[] resize() { Node[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap > 0) { if (oldCap >= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } else if ((newCap = oldCap <","link":"/2018/02/14/java-collection-hashmap/"},{"title":"Java Collection——HashSet","text":"HashSet的实现是基于HashMap的，与Map不同的地方在于HashSet中Key对应的Value只需要一个标记即可。 由于HashSet最核心的部分————Hash已经由HashMap完成，因此只要加一个包装即可。 属性保存了一个hashMap。此外还有一个静态属性来发挥标记的作用。 方法1234567iterator();size();isEmpty();contains(Object);add(Object);remove(Object);clear(); 基本上所有方法都是直接调用HashMap中的方法，比如empty方法直接返回map.isEmpty()的结果，contains直接返回map.containsKey(Object)的结果。 迭代器没有直接定义迭代器的内部类，而是复用HashMap的迭代器。 其他内部类无。","link":"/2018/02/14/java-collection-hashset/"},{"title":"Java Collection——HashTable","text":"HashTable继承了Dictionary类，这个抽象类没有在HashMap中见到过。进入该Dictionary抽象类中，发现所有的方法都没有实现，而且和Map接口很相似。 初步浏览了一下HashTable，感觉比HashMap更轻量一些，同时额外地提供了同步机制（在方法之前加synchronized关键字，类似于Vector之于ArrayList）。 属性 Entry[]类型的table，用于存放散列bucket count，计数 threshold，重哈希的阈值，如果某次操作是的count超过这个值，就重新散列 loadFactor，装载因子，衡量buckets的稀疏程度，数值上count*loadfactor=threshold，装载因子是可以人为规定的 modcount，修改次数，避免一边便利一边修改结果 方法contains 对于每一个bucket的每一个节点，根据equal的结果判断是否相等。 get 与HashMap不同的是，HashTable的许多方法的key参数都不能为null。包括get和上面的contains。 首先将hashCode映射到buckets空间中，找到bucket，接着遍历链表，根据hash和equals判断是否相等。 put 首先将hashCode映射到buckets空间中，找到bucket，接着遍历链表，根据hash和equals判断是否相等。如果相等，说明原来就有这个key，只要修改该节点的value，如果找不到，调用addEntry来添加该key-value对。 addEntry 修改modCount，因为addEntry会改变HashTable的结构。 检查是否要rehash。如果要的话，会重新计算散列index。 直接将新的key-value封装到entry中，插入到链表的首位上。 rehash 计算新的容量，扩容规则为oldCapacity","link":"/2018/10/05/java-collection-hashtable/"},{"title":"Java Collection——LinkedHashMap","text":"LinkedHashMap和HashMap的区别在于前者在HashMap的基础上还维护了一个贯穿所有节点的双向链表，该链表决定了遍历的顺序，形如一个队列。 当然，这个链表并不会影响HashMap的结构，和HashMap的table或者链表或者红黑树是完全独立的一个结构。该链表的顺序可以是插入元素的顺序也可以是访问元素的顺序，具体选择何种顺序取决于accessOrder属性是true还是false。 在hashmap中预留了几个抽象方法，并在put和get等操作内调用了这些方法，为子类提供了可扩展的点。我认为这是“模板方法模式”的体现。 在HashMap的Node中，就已经提供了next属性，但没有用到。在LinkedHashMap中，每个节点的next属性根据访问或者插入顺序连接下一个节点。 属性为了记录双向链表的头和尾，使用了两个节点记录head和tail。 另外，使用了一个boolean标记访问顺序。如果为true，则遍历顺序为访问顺序，否则按照插入顺序遍历。默认为false。 这个值是final的，也就是说一旦构造完毕就不能再修改的，因为一旦决定了该值为true，在后续插入删除访问节点时就会做额外的操作来修改链表，这时再修改该值显然是无法还原访问顺序的。 方法afterNodeRemoval 在HashMap的removeNode方法中被调用。这个方法在一个节点被删除之后，删除该节点存在的前向和后向链接，并将该节点的前驱的后向链接和后继节点的前向链接重新设置。 afterNodeInsertion 在HashMap的putVal方法中被调用。在插入新的节点之后，根据需求判断是否需要把最老的节点删掉，如果需要，就调用HashMap的removeNode方法。默认的判断结果为false。 aftetNodeAccess 在HashMap的putVal方法中被使用到。在访问某个节点后（比如通过put修改该节点的值），把该节点移动到链表的最后。 transferLinks 拷贝src节点的链接情况到dst节点上。 put由于可以针对插入顺序和访问顺序进行遍历，因此在get和put方法中必须要添加额外的代码来修改链表。LinkedHashMap并没有重写put方法，而是在HashMap中就定义了几个回调函数（afterNodeXXX），然后重写了这些回调函数。 getLinkedHashMap重写了get方法，在调用HashCode的putVal之后再调用afterNodeAccess回调函数。将访问的节点移动到链表的最后一个。 迭代器LinkedHashIterator是一个抽象类，从双向链表的头节点开始遍历，提供了nextNode方法供其他迭代器使用。nextNode方法和remove方法都是fail-fast的。 LinkedKeyIterator、LinkedValueIterator、LinkedEntryIterator继承了LinkedHashIterator并完整实现了Iterator接口。","link":"/2018/02/17/java-collection-linkedhashmap/"},{"title":"Java Collection——LinkedHashSet","text":"区别于HashSet，LinkedHashSet使用了一个双向链表来串联所有节点。LinkedHashSet的实现相当的简单，它继承了HashSet，在HashSet的构造函数中插入了一个直接以LinkedHashMap作为实现的构造函数。其余所有方法均与HashSet一致。但是由于在构造函数中少传了一个参数，因此LinkedHashSet无法由用户决定是按照accessOrder还是按照insertOrder。可以得出这样的结论：LinkedHashSet中遍历元素始终是按照插入元素的顺序遍历的。","link":"/2018/02/19/java-collection-linkedhashset/"},{"title":"Java Collection——LinkedList","text":"LinkedList不同于ArrayList，虽然二者都是List，呈现给用户的都是顺序列表容器，但是底层的实现是不一样的。ArrayList直接以Java的数组作为底层实现，数组中相邻元素在元素中也是相邻的，保持一致的顺序排列。而LinkedList的底层实现使用的是链表，即LinkedList中的元素在内存中不必相邻，不必连续排列，可以分散在各处，元素花费额外的空间存放指针来记录相邻元素的位置。 链表与数组各有各的优劣，数组的随机访问效率最高，但是要插入删除元素时必须要搬移其他元素；链表插入删除元素只需要常数时间复杂度，但是随机访问需要逐个遍历元素。 比较一下LinkedList和ArrayList分别继承了哪些类，实现了哪些接口。 public class LinkedList extends AbstractSequentialList implements List, Deque, Cloneable, java.io.Serializable{} public class ArrayList extends AbstractList implements List, RandomAccess, Cloneable, java.io.Serializable{} AbstractSequentialList是AbstractList的子类。AbstractList提供了少数的几个方法的实现，如indexOf、clear等，其余方法均是抽象方法。AbstractSequentialList继承了AbstractList，并已经实现了大部分如今使用容器时较为常用的方法，但是完全是基于迭代器ListIterator实现的。 Java标准库并不是仅仅只能提供实现好了的东西供人使用，他也提供了很多AbstractXXX类，这些抽象类实现了部分的功能，使得开发者只要根据自己的需求，实现少数的几个方法即可。即花费最少的精力让满足自己需求的数据结构融入到JDK的容器类框架中。 注释也写道：如果开发者想要实现自己的随机访问容器，可以继承AbastractList，如需要顺序访问容器，最好继承AbstractSequentialList。 /** * This class provides a skeletal implementation of the {@link List} * interface to minimize the effort required to implement this interface * backed by a “random access” data store (such as an array). For sequential * access data (such as a linked list), {@link AbstractSequentialList} should * be used in preference to this class. * * To implement an unmodifiable list, the programmer needs only to extend * this class and provide implementations for the {@link #get(int)} and * {@link List#size() size()} methods. * * To implement a modifiable list, the programmer must additionally * override the {@link #set(int, Object) set(int, E)} method (which otherwise * throws an {@code UnsupportedOperationException}). If the list is * variable-size the programmer must additionally override the * {@link #add(int, Object) add(int, E)} and {@link #remove(int)} methods. * * The programmer should generally provide a void (no argument) and collection * constructor, as per the recommendation in the {@link Collection} interface * specification. * * Unlike the other abstract collection implementations, the programmer does * not have to provide an iterator implementation; the iterator and * list iterator are implemented by this class, on top of the “random access” * methods: * {@link #get(int)}, * {@link #set(int, Object) set(int, E)}, * {@link #add(int, Object) add(int, E)} and * {@link #remove(int)}. * * The documentation for each non-abstract method in this class describes its * implementation in detail. Each of these methods may be overridden if the * collection being implemented admits a more efficient implementation. * * This class is a member of the * * Java Collections Framework. * * @author Josh Bloch * @author Neal Gafter * @since 1.2 */ List接口规定了List必须提供的几个方法。 Deque是Double End Queue（双端队列）的缩写，常常发音为“deck”，队列的两端都可以添加、删除元素。 Node提到LinkedList，就不得不先提一下这个静态内部类。他表示LinkedList中的节点，封装了列表中每个元素的值、前驱和后继节点的引用。LinkedList是典型的双向链表。没有成员函数。 private static class Node { E item; Node next; Node prev; Node(Node prev, E element, Node next) { this.item = element; this.next = next; this.prev = prev; } }属性与ArrayList相比，LinkedList（以下简称链表）的属性更少，因为不需要额外表示空列表了。 只有三个属性：size，first，last。 first代表链表的头节点，需要遵循一个不变式：要么first和last都是null，要么first的前驱为null并且first的值不为null。 last代表链表的尾节点，也要遵循：要么first和last都是null，要么last的后继为null并且last的值不为null。 也就是说空链表的first和last都是null，如果有元素，那么first和last都有可能被存放元素。 方法linkFirst在链表头部插入一个新的元素。这需要修改老的头节点的前驱、新的头节点的后继、链表的头节点。如果头节点为null，说明是个空链表，那么这个链表的头节点和尾节点应该都是这个新加入的节点。 linkBefore在某个节点之前插入一个元素，顺序需要稍微注意一下。 unlinkXXX从链表中删除一个节点。 node根据索引获得链表中的元素。如果索引比size/2大，就从尾节点走，如果比size/2小，就从头节点开始遍历。 indexOf根据元素的equals方法，返回元素在链表中的索引。 addAll把参数（Collection）中的元素全部添加到某个index位置上。 public boolean addAll(int index, Collection c) { checkPositionIndex(index); Object\\[\\] a = c.toArray(); int numNew = a.length; if (numNew == 0) return false; Node pred, succ; if (index == size) { succ = null; pred = last; } else { succ = node(index); pred = succ.prev; } for (Object o : a) { @SuppressWarnings(\"unchecked\") E e = (E) o; Node newNode = new Node(pred, e, null); if (pred == null) first = newNode; else pred.next = newNode; pred = newNode; } if (succ == null) { last = pred; } else { pred.next = succ; succ.prev = pred; } size += numNew; modCount++; return true; }我觉得这段代码是LinkedList中比较有代表性的代码，要把一个集合中的所有元素插入到链表中的某个index上去，首先获得对于添加进来的所有元素的前驱和后继节点，如果在最后一个地方插入，后继为null，前驱为尾节点；如果在其他地方插入，后继为index处的节点，前驱为index处节点的前驱。然后就是一个一个插入，插入时如果前驱为null，说明是在链表头部插入，那么链表的首节点就是这个新的节点，后则的话前驱的后继节点就是新的节点，然后让新的节点成为下一个新的节点的前驱。 迭代器和ArrayList一样，LinkedList对于concurrentModification也有限制，不能一边用foreach循环或迭代器遍历，一边用链表的remove或unlink方法删除元素。 ArrayList使用一个lastRet记住上一个返回的元素的位置，用于调用迭代器的remove方法时能够返回上一个节点的位置。链表也有一个lastRet记录上一个返回的节点。 深入探究Array和LinkedList中迭代器的lastRet的作用ArrayList的迭代器中这么用lastRet lastRet初始化时是-1，也就是说，如果刚刚初始化了一个迭代器，还没有next，这时就remove，是不行滴。这符合一般的认识：迭代器在初始化的时候一般是指向第一个元素的前面，也就是说要next才能返回第一个元素。 为什么在add和remove之后，lastRet都要变成-1，从而禁止多次连续修改数组元素呢？从一般的认识上来说，连续的remove应该是符合需求的。 从使用的角度看 while(iterator.hasNext()){ //不管是从前往后还是从后往前。 E cur=iterator.next(); if(cur……){ //做一些检验，如果检验通过，就删除他。 iterator.remove(); }} 那么如果我在remove之后让lastRet变成lastRet-1是否可行呢？首先，iterator并不是只能单向移动的，ListIterator都可以previous，如果一个元素一定要先取得——检验之后再决定删除的话，就不能保证lastRet-1是否被检验过（可能这个iterator一直是从后往前走的呢。。。）既然方向不确定，就不能根据上一个推出上上一个，把遍历顺序记录下来肯定也不现实，所以就干脆把lastRet改为-1，这样连续remove就会直接报错了吧（应该也是fail-fast的一种体现）。 太无聊了，写个benchmark来比一比ArrayList和LinkedList的插入效率吧。 众所周知，人们常说，ArrayList中进行插入删除元素的操作，由于需要元素的搬移，效率往往低于LinkedList。结果倒还真的出乎我意料。 如果仅仅是在列表尾端插入元素，那么二者的运行时间没有什么大的差别，如果是在固定位置（往往是较靠前的index）插入，LinkedList的优势才能显现出来。 从代码上来看，LinkedList在固定位置插入，需要线性时间来找到那个位置所对应的节点（也就是node函数），然后再用常数时间插入新元素，因为在代码中我把index固定为了0，较为靠前，因此找到节点花费的时间近似为常数时间。ArrayList在0处插入新元素，每次都要搬移后面的元素，更不要提数组的扩容后的复制了。 但是在第一张图中，在列表尾端插入元素，ArrayList却比LinkedList更快呢？即便ArrayList不需要搬移元素，但是数组的扩容，扩容之后的集体复制也需要线性时间啊。 忽然灵光一现，想到了分摊复杂度的概念，顿时就豁然开朗了。由于数组扩容并不是每次都扩容，每次都批量复制，把少数几次扩容和批量符指分摊到每次操作上，每次操作所需要的时间也就接近与常数时间了。 所以，还是那句话，数据结构的选择是要根据具体的情境、具体的业务需求决定的，不是简单的一句链表比数组插入得快所能概括的了的。","link":"/2018/02/10/java-collection-linkedlist/"},{"title":"Java Collection——总览","text":"说明： 1. 本系列的内容主要为基于相关源码分析Java Collection的实现，包含List、Queue、Set 、Map这四大块。 2. 在学习和整理笔记的过程中，参考了网上其他前辈的资料，由于笔记最终会发布到网上，出于对原创的尊重和对原作者的感谢，需要在引用了前辈的文字、图片、代码处标记出处，但是为了方便（懒），我就不一一标注了，改为集中在本文开始部分罗列参考资料链接。再次对把自己的研究成果和学习资源上传到互联网上供他人参考和学习的前辈们表示感谢 参考链接： [CarpenterLee/JCFInternals]深入理解Java集合框架 总览JDK提供的集合类包含两大块，一个是collection，一个是map，二者都提供了java对一些常用数据结构是实现，为开发者的日常使用提供了工具。 俗话说，不要重复造轮子。既然JDK提供了相关实现，经过了JDK版本的迭代更新以及几十年的开发人员的使用和修改，经历了充分的检验，可以说是可靠的和可用的。 泛型虽然Java的泛型饱受诟病，collection还是基于泛型设计了相关的函数。 接口 目录（暂定内容，后期会增加） ArrayList LinkedList CopyOnWriteArrayList Vector HashMap HashSet LinkedHashMap LinkedHashSet","link":"/2018/02/09/java-collection-overview/"},{"title":"Java Collection——PriorityQueue","text":"Java中的优先队列采用的是最小堆的结构，即使用数组来保存一棵完全二叉树。无界，可以自动扩容。用于比较的元素值必须是不可变的否则最小堆的性质会被破坏。 属性 queue，一个Object类型的数组，这是保存完全二叉树的实体。 size，优先队列的大小。 comparator，比较器，优先队列有两种比较方式：直接传入comparator或者让元素继承comparable接口，大小关系由此确定。 modCount，修改次数，避免一边遍历一边修改的情况发生。 方法 构造函数 第一个参数是初始化容量，默认是11，优先队列是本身是无界的，因此初始化容量并不是最大容量。第二个参数是一个比较器。 offer 向优先队列中插入一个元素。由于offer的语义是向队列尾部插入一个元素，但是优先队列的首尾顺序是依靠元素大小顺序的，因此这里offer就是简单的插入一个元素的意思。 判断是否需要扩容，如果要的话就grow。判断的标准是数组有没有满。 如果数组为空的话，把元素放在第0位。 如果数组不为空，把元素放在最后一位，并shiftUp该元素使其上浮到路径上的正确位置。 和教科书上的最小堆不一样，这个最小堆的第0位也是放东西的，这就说明第i个元素的父节点是（i-1）/2，子节点是2i+1和2i+2。 grow 扩容的过程贼简单，如果原先数组大小较小（比64小），就扩容一倍，否则扩容一半。然后Arrays.copyOf把原数组的元素复制到新数组即可。 siftUp 元素上浮可以使用comparator也可以使用comparable的compareTo方法，但如果即没有comparator又没有实现comparable接口的话是不行的。而不论用哪种方式来表大大小关系，上浮算法的过程都是一样的。 siftDown 和上浮不同的是，下沉的过程中每个元素可能往两个位置下沉：他的左子节点和右子节点。如果是最小堆的话，就要和两个子节点中的最小的元素交换位置，以此保证最小堆的性质。但同时又要注意有的节点没有右子节点。如果父节点比两个子节点中的最小节点还要小，那就不需要继续迭代了。 heapify 从二叉树的倒数第二层开始，每个节点进行下沉操作。 poll poll的语义是从队列首部弹出一个元素，这里即为从最小堆中弹出一个最小的元素，在数组中，最小的元素就是第0位的元素。 把数组最后一个元素放到第0位，然后下沉。最后返回原来在第0位的元素。 removeAt 删除数组中的第i位元素，这个方法主要是为了remove(object)方法服务的。 修改modCount，size。 看删除的是不是最后一个元素，如果是的话，把最后一个设为null就行啦。 把最后一个元素放到第i位上，先下沉。 如果在上一步中最后一个元素不用下沉，那就说明该元素比它的两个子元素还要小，说不定比被删掉的元素还要小，出现这种情况的原因是最后一个元素不在原来第i个元素的子树中，这样他们的大小关系是不确定的。那就需要让现在的第i个元素上浮试一试了。 如果上浮成功了，就返回原先的最后一个元素，否则返回null。返回值主要是为了迭代器move方法服务的。 Itr迭代器属性 cursor，下一次要返回的元素在数组中的位置。 lastRet，上一次next操作返回的元素在数组中的位置。 forgetMeNot，一个贼神奇的东西。。。 lastRetElt，上一次next操作返回的元素。 expectedModCount，修改次数。 方法next 检查有没有发生并发修改错误。 如果cursor","link":"/2018/10/05/java-collection-priorityqueue/"},{"title":"Java Collection——TreeMap","text":"TreeMap继承了AbstractMap抽象类，实现了NavigableMap接口。这个接口是HashMap和LinkedHashMap的定义中都没有见过的，从字面意思上来看，应该是“可导航Map”，到底是怎么个导航法，需要先了解一下这个接口。 NavigableMap也不是一个顶级的接口，它实现了SortedMap接口。SortedMap接口描述了Map的排列方式的一种特殊情形：所有的Key-Value对都是按照Key的大小一致升序或者降序排列，这里的大小指的就是Comparable接口。降序或者升序集中体现在遍历该Map的时候。有了这个特性，就可以提供一些其他的功能。如给定一个key，返回所有的大于这个key的Key-value组成的Map（视图），或者所有小于这个key的key-value组成的Map（视图）。 NavigableMap在SortedMap提供有序key-value的基础上拓展了一些功能，能够提供离某个key最近的entry，可以以目标key为上界，也可以之为下界。有点类似于向上取整和向下取整的意思。 再回到TreeMap，根据它的注释，可知TreeMap是基于红黑树实现的Map（注释中还特别强调了是算法导论中的红黑树hhh），基本的增删改查需要log(n)的时间复杂度，这比HashMap慢一些，因为HashMap使用的是散列表，访问元素只需要线性时间。 TreeMap是一个相当复杂的类，不仅仅是在算法的实现上，更在于类结构的设计上。 属性一个Comparator，用于记录比较运算符。如果key实现了Comparable方法，就不需要这个Comparator。 一个Entry，用于记录根节点。 一个size，用于记录元素个数。 一个modCount，用于检查conModification。 方法getEntry 从代码中可以看到，如果没有comparator，那么key是不允许为null的，否则可以通过comparator来实现对于null键的判断规则，即便是根节点，也可以是null。而在HashMap中，null键是可以插入、查询的。 getEntryUsingComparator getEntry和getEntryUsingComparator的实现思路是相同的，都是从树根出发，向左或向右行走，直到找到相等的节点。两个方法的区别在于一个是通过comparator做比较，一个是通过key的compareTo方法。在注释中解释说把getEntryUsingComparator方法抽取出来的目的在于提高效率。 putput方法用于向TreeMap中插入新元素，或者修改原有的key的值。由于使用的是红黑树，因此插入元素可能涉及一些对于红黑树结构的修改。 插入新节点后调用fixAfterInsertion方法插入新元素并调整树结构。 这里先跳过红黑树相关操作的实现。 getCeilingEntry找到某个key对应的entry，如果找不到，就取比这个key大并且紧邻着这个key的entry，类似于向上取整。根据二叉搜索树的性质，需要向下遍历。 如果给定的key比此节点的key大，就向右子树走，如果没有右子树，沿父节点回溯，直到某个节点是其父节点的左儿子，返回其父节点；如果给定的key比此节点的key小，就向左子树走（显然右子树的节点都会比此节点的key大），如果没有左子树，就取这个节点。 getHigherEntry和getCeilingEntry类似，区别在于不找相等的key对应的entry，而是直接命中比给定key大并且紧邻着这个key的entry。 迭代器PrivateEntryIterator这是一个其他迭代器类的父类，实现了Iterator接口及其方法，同样控制了modCount，保证在遍历的时候map的结构不发生改变。 EntryIterator、ValueIterator、KeyIterator、DescendingKeyIterator分别是针对Entry、Value、Key的四个具体的迭代器。 其他内部类EntryTreeMap的静态内部类，定义了一个Key-Value对的节点。 此内部类只有寥寥数个方法，都是setter和getter方法，没有什么特别的。唯一有些特殊的是hashCode，这个函数根据key和value计算出hashCode，方法是将key的hashCode异或value的hashCode。 NavigableSubMapValues、EntrySet定义了两个视图。","link":"/2018/02/21/java-collection-treemap/"},{"title":"Java Collection——Vector","text":"Vector类的原型和ArrayList是一模一样的，实现了RandomAccess接口说明遍历的时候使用get方法比使用迭代器方法快（LinkedList就不必实现该接口），继承了AbstractList提供的一些较为基础和简单的方法实现（比如常常提到的modCount）。 属性Vector也是基于数组实现的列表，因此拥有一个Object数组来存放元素，其长度就是概念上的Capacity。 此外，还有elementCount和capacityIncrement，前者用来记录数组中有效元素的个数（size），后者表示当size>capacity时，自动增长的长度。 方法绝大多数方法都用synchronized修饰。 ensureCapacity在添加元素时确保数组够大，如果不够就需要扩容。 在grow时会使用到capacityIncrement，如果这个值大于0，就按这个值看扩大数组尺寸，否则就扩大一倍。当然还是要和minCapacity比一比，和Vector类的最大尺寸比一比。 elements在之前的所有List中都没有见过类似的方法，该方法返回一个实现了Enumeration接口的匿名内部类的对象，供外部程序遍历。 在内部类中实现的方法也是保证了同步。 之所以在其他的List实现类都没有遇到类似的方法，是因为Enumeration接口的功能以及被迭代器所取代，现在已经很少使用了。 Enumeration只支持两个操作，一是判断是否还有元素，二是返回下一个元素。 synchronized除了构造函数外，所有的public方法都用synchronized修饰，或者其间接的调用synchronized方法，如remove(Object)虽然没有synchronized修饰，但是其调用的removeElement(Object)是有synchronized的。 包括capacity(),size(),isEmpty()这些只需要返回一个属性的较为简单的方法，也是有同步控制的。 迭代器和其他List基本一致，除了多了额外的synchronized之外。 总结Vector和CopyOnWriteArrayList都是线程安全的List，二者的区别在于: 锁的方法不同：前者为使用synchronized，后者为使用ReentrantLock。但是二者都是可重入锁，也就是说一个synchronized的函数内部再调用一个synchronized函数是不会死锁的。 同步的范围不同，前者的读操作和写操作都同步了，后者读的操作没有加锁。","link":"/2018/02/14/java-collection-vector/"},{"title":"Java——IO体系概览","text":"java整个io包主要分为四块：字节流、字符流、File、RandomAccessFile。其中最复杂的是字节流和字符流两个簇，File和RandomAccessFile的功能较为单一。从名字可以看出，RandomAccessFile用于随机读写，字节流和字符流则用于顺序读写。 字节流字节流以字节为单位进行输入输出操作，也是JDK较早的版本中就存在的IO方式。字节流IO提供了两大接口InputStream和OutputStream，一个用于输入，一个用于输出。 输入是指从“外部”某个对象上读取数据到JVM内存中，这个对象可以是磁盘上的一个文件，可以是套接字，可以是另一个进程的标准输出，甚至也可以是一个本来就在这个进程的内存空间里的一个字节数组或者字符串。 InputStream字节输入流的总接口，接口中规定了输入字节流的各种实现所需要提供的方法。接口中声明的方法只有寥寥数个。 由于是字节流，所以输入的最小单位是字节（Byte），但是read方法返回的是int，范围是0-255，如果读到了文件末尾，返回-1。不带参数的read方法返回的是一个字节，带参数的read方法将读到的数据直接填充进参数的byte数组中，返回的是实际读取的字节数，类似于Unix中的read方法。 FileInputStream文件输入字节流，用于从一个外存中的文件读取数据。在FileInputStream的源码中可以看到很多出native关键字，说明读取数据的方法主要由C++调用本地接口实现的。 ByteArrayInputStream从一个字节数组中读取数据，这个字节数组实际上就是内存里的一个byte[]对象。常见的用法是这个样子的： ObjectInputStream是一个包装器流，能够将一个输入的字节流自动根据字节序解序列化，转换成开发人员所需要的对象或基本数据类型。当然转换成什么数据类型必须要在代码中自己指定，如果指定错了就会导致数据出现错误。 如上图所示，ObjectOutputStream读取基本数据类型或者变量需要提前知道输入流的结构。如果输入时不按照字节流的结构，很有可能会抛出异常。 直接读写对象时，要求对象实现Serializable接口。虽然这个接口只是一个标记接口，没有函数。而且序列化的方式完全由JVM决定，序列化之后的结果对于人来说是不可读的。 默认会把对象的所有字段都序列化，如果需要略过某些字段，可以添加transient修饰符。当然被序列化的对象中包含的其他对象的引用的话，也是会被序列化的。 反序列化不会调用对象的构造函数。 BufferedInputStream是一个包装器流，在输入的时候提供一个缓冲。这个缓冲就是一个byte[]对象，默认大小为8k，最大大小为Integer.MAX_VALUE-8。 输入的时候，默认从缓冲中读取，如果缓冲中没有足够数据，就先把缓冲区填满，再读。 DataInputStream类似于ObjectInputStream，能够提供一组与平台无关的函数来读取基本数据类型。至于ObjectInputStream和DataInputStream的区别： DataInput/OutputStream performs generally better because its much simpler. It can only read/write primtive types and Strings. ObjectInput/OutputStream can read/write any object type was well as primitives. It is less efficient but much easier to use if you want to send complex data. I would assume that the Object*Stream is the best choice until you know that its performance is an issue. 意思就是，二者在读取基本数据类型的时候几乎没有区别。 PrintStream这是一个输出流，System.out,System.err就是PrintStream实例。PrintStream所提供的方法与其他几个输出流不太一样，除了write其他的都是print。 PrintStream提供格式化输出字符串的方法printf，此外也可以输出各种基本数据类型和引用类型对象。相比于ObjectOutputStream和DataOutputStream，PrintStream的方法更加简练，一个print就可以应对各种数据类型。 PrintStream有两大特征：不会抛出IOException；自动flush（遇到换行、\\n、写byte[]）。 字符流为了解决国际化的问题，JDK在某个较新的版本中引入了字符流，由Reader和Writer两大簇组成。字符流不论输入和输出，基本单位都是字符，字符在Java中是Unicode字符集的，占2个字节。 字符流家族中很多类都是和字节流中某个类对应的。如BufferedInputStream和BufferedReader，FileOutputStream和FileWriter等。 因为输入输出的单位都是字符，因此（输入时）函数返回值和（输出时）函数传入参数都是char或者char[]或者String类型，其中也有int类型的，这个int的范围应该是0-2^16-1。","link":"/2018/05/07/java-io/"},{"title":"JavaEE——JSP概览","text":"JSP是基于Servlet和模板渲染技术发展出来的一种后端开发技术（虽然现在JavaEE说到底都是用的Servlet）。其原理非常简单，模板引擎解析JSP文件之后，将其中的Java对象取出，其余的字面常量统统放入print函数中，以最为淳朴的字符串连接的操作组成最终完整的响应，填充到servlet的对应函数中。 生命周期 编译：惰性编译，只有在第一次请求的时候，被请求的JSP会被编译为包含一个Servlet的java源码文件，然后再编译为class字节码文件。 初始化：和Servlet的init方法一样，只初始化一次，常见的初始化操作有初始化一些变量、连接数据库、打开文件等。 执行：调用一个以HttpServletRequest和HttpServletResponse为参数的方法。 清理：和Servlet的destory方法一样。 九大对象 因为JSP和SSM的思路脱节太严重了，所以我也只想了解到这个深度了。","link":"/2018/05/07/javaee-jsp/"},{"title":"JavaEE——Servlet总结","text":"谈到JavaEE，虽然如今绝大部分场合使用的都是SSH、SSM等框架，但这些都是基于Servlet技术所做的进一步发展，也就是说底层实现还是servlet，只是做了针对业务需求做了进一步的封装，使得开发人员能够更加有针对性的、更加高效的编写代码。这里稍微总结一下servlet技术的几个重要概念。 从没有在实际项目过程中使用过servlet，因此这里就从一个开发者的角度来考虑几个问题。 概念Servlet是Java中一系列对象的统称，也可以说是一个标准，也可以说是java中的一个借口（interface）。 标准和JDBC一样，Servlet规定了Java Web Server程序所需要的输入数据、输出数据的规范，遵守该规范的程序能够在常用的Servlet容器如Tomcat中运行。类比于JDBC规范，Servlet就是JDBC，在JDBC中，编写Java程序进行数据库的增删改查操作的函数接口都由JDBC规范约定好了，同一个Java程序可以应用于不同的数据库DBMC，只需要切换驱动即可；在Servlet中，Servlet容器也可以随意更换，编写的Servlet类是可以处处运行的。而Tomcat等容器对应于各种DBMS如MySQL、Oracle及其驱动。 对象开发人员编写Servlet程序是以自定义类并覆盖抽象类，或实现接口，这两种形式进行的。在运行时，类在容器中被实例化为对象，容器负责管理对象的生死存亡，负责向servlet对象中传入参数或接收返回值。 以HttpServlet为例，servlet对象被实例化后存活于容器中，每当有HTTP请求进入容器，容器会将该请求报文封装为一个HTTP请求对象（该请求对象也要符合Servlet标准），传入对应的Servlet中，Servlet根据请求内容采取操作，最后返回一个HTTP响应对象，该响应对象经容器序列化为响应报文，返回给客户端。 接口既然是标准，那肯定要有interface。 生命周期 Servlet的生命周期比较简单，过程中经历的函数也不是很多。 Servlet会被惰性初始化（或者在容器启动时初始化，取决于配置），即在容器在处理某个请求的时候检查其需要的Servlet是否存在，如果不存在，载入该Servlet类，实例化，运行init方法，读入一些初始化参数。 在某个请求到来的时候，会运行servlet的service方法，并传入相关参数。这些参数由容器打包。servlet处理结束之后，其第二个参数——response会被容器提取，并返回给客户端。每个请求都会运行service方法。 由容器决定何时销毁servlet（就像JVM垃圾回收一样），销毁时执行destroy方法，释放资源。一般是容器关闭的时候销毁。 多线程Servlet并不是单例，即允许实例化多个对象，尽管其在大多数情况下只有一个对象。一个servlet声明对应一个对象，声明多次就有多个。 For a servlet not hosted in a distributed environment (the default), the servlet container must use only one instance per servlet declaration. 如果 servlet 不是在分布式环境下（默认），servlet 容器必须使一个 servlet 实例对应一个 servlet 声明。 However, for a servlet implementing the SingleThreadModel(Deprecated) interface, the servlet container may instantiate multiple instances to handle a heavy request load and serialize requests to a particular instance. 然而，实现了 SingleThreadModel 接口的 Servlet，可以有多个实例。以处理繁重的请求，并且序列化 request 到特定的 servlet 实例。 在面对并发的情况时，servlet对象会运行在多线程环境中。即一个servlet对象会被多个线程共享，每个线程可能对应于一个HTTP连接（线程的复用机制取决于容器）。在service方法内部的局部变量不会受线程切换而影响，但是如果servlet类中定义了实例变量，那么就有可能造成线程的冲突。 因此要避免在servlet中定义实例变量，或者手动同步，不过手动同步可能会影响处理性能。 过滤器（Filter）和servlet对象一样，过滤器概念也是来自于servlet标准，即如果我按照标准编写并配置了一个过滤器，那么容器就必须将该过滤器实例化并装载在合适的位置，对于HTTP请求进行过滤。 生命周期和servlet类似，有init和destroy阶段，只初始化一次。初始化时机在servlet初始化之前，销毁于servlet之后。对应于servlet的service方法，filter有一个doFilter方法，表示在过滤HTTP请求时采取该行为。 配置直接上代码吧，理解起来没什么难度。这是一个处理字符编码的过滤器。 setCharacterEncoding com.company.strutstudy.web.servletstudy.filter.EncodingFilter encoding utf-8 setCharacterEncoding /* 如果声明了多个servlet对象，HTTP请求入站过程经过过滤器的顺序为从上到下。 模式一个HTTP可能会经过多个filter，filter的组装顺序和过滤控制是可以调整的。容器会自动把声明的过滤器组装在一个filterChain对象里面，在某个过滤器的doFilter方法中，如果想要让该请求进入下一个过滤器，就需要调用chain.doFilter方法。最后一个过滤器调用该方法把请求直接传给servlet对象。 监听器（Listener）监听器，顾名思义，就是监听某个事件发生并采取相应行动的组件。 监听器能够监视三类对象，ServletContext、HttpSession、ServletRequest，监听这三类对象的监听器的抽象类类名和接口名也不同： 监听应用上下文：ServletContextListener 监听用户会话：HttpSessionListener 监听请求：ServletRequestListener 每种监听器都有数个类似的方法，如监听对象的创建、销毁事件，增加、修改、删除某个属性的事件。有的监听器会有一些独特的方法。这里不详细解释每个监听器的原型。 配置 cn.itcast.web.listener.MyServletContextListener 如果在web.xml中声明了多个监听器，那么这些监听器会按照声明的顺序注册到相应对象中。 上下文（Context）容器启动后，会为每个web应用创建一个context，只要不关闭容器或者卸载该web应用，context就不会被销毁。servlet之间通过context来通信，servlet也可以通过context来获取全局的资源文件等。 配置 FristServlet com.hello.com.FristServlet","link":"/2018/05/01/javaee-servlet/"},{"title":"Spring Framework——概览及目录","text":"SpringFramework由十多个模块组成，其中最最核心的模块有： Beans：包括BeanFactory，负责生成Java Bean Core：负责DI和AOP，重中之重。 Context：管理web应用长下文。 目录： 1.IOC 2.Resources 3.Validation, Data Binding, and Type Conversion 4.AOP","link":"/2018/05/07/javaee-spring-framework/"},{"title":"JLink——借助JLink打包Java程序为可执行程序","text":"JDK9中的模块化技术使得原本一两百兆的JRE能够被拆分成多个较小的模块，因此如果想要把Java程序打包成可执行程序，并在没有JRE环境的机器上运行，不再需要带上一个庞大的JRE，而只需要额外增加几个JDK中的module。 项目目录结构如下，仅仅是一个很简单的Hello World程序。代码中仅仅使用到了System.out.println这一个JDK中提供的函数。 最重要的是module-info.jar文件，这个文件是module的定义文件，声明了我所编写的这个模块需要依赖哪些模块，以及对外暴露哪些东西。 由于只依赖java.base模块，而这个模块是默认包含的，因此module-info的大括号内可以为空。 12module dkm {} build.gradle文件也十分简单。 1234567891011plugins { id 'java'}apply plugin: 'application'mainClassName = 'com.dokyme.Main'group 'test-jlink-exe'version '1.0-SNAPSHOT'sourceCompatibility = 1.9 先运行./gradlew build，将该项目打成JAR包。接着再用JLink工具将多个模块链接成一个可执行文件，有没有一种C语言的链接器的既视感？ 1jlink --module-path /Library/Java/JavaVirtualMachines/jdk-10.0.2.jdk/Contents/Home/jmods:build/libs --add-modules dkm --launcher dkm=dkm/com.dokyme.Main --output dist –module-path 类似于CLASSPATH，JLink到哪里去找modules –add-modules 要额外添加的module –launcher dkm=dkm/com.dokyme.Main 启动器，或者说是可执行程序。后面跟着的是程序的名字以及入口主类的位置。 –output 输出路径 命令执行结束之后就会多出一个dist目录，里面的bin文件夹下存放着可执行文件，这个dist目录大约30MB左右。 这个入口文件实际上是一个shell脚本，调用同目录下的java程序载入模块中的主类，执行主类的代码。 1234#!/bin/shJLINK_VM_OPTIONS=DIR=`dirname $0`$DIR/java $JLINK_VM_OPTIONS -m dkm/com.dokyme.Main $@ 找到一个gradle的plugin，挺有意思： gradle-dplink-plugin","link":"/2019/01/06/jlink/"},{"title":"JUC——AQS","text":"AQS————AbstractQueuedSynchronizor，队列同步器，AQS是一个抽象类，它为开发者编写同步工具（比如锁、信号量）提供了开发框架，这个框架封装了对线程和竞态条件的细节，使得开发者能够只关注于锁的逻辑，而无需关注线程的维护以及竞态条件的控制。 JUC中很多锁都是基于AQS进行开发的。 Node类由于AQS是队列，需要有一个类来封装每个节点的状态，Node类代表队列中的每一个节点。除了构造函数之外没有方法。 属性 waitStatus，等待状态。状态值可能为SIGNAL(-1)、CANCELLED(1)、CONDITION(-2)、PROPAGATE(-3)、0。 prev，队列中的前驱节点。 next，队列中的后继节点。 thread，线程对象 nextWaiter， AQS实现——MutexAQS框架使用了模板方法模式来实现自身功能，某个锁的实现通常要继承AQS并重写数个方法，因此脱离具体的锁去谈AQS是有些不切实际的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192public class MutexLock implements Lock { private static final int LOCK = 1; private static final int UNLOCK = 0; private final Sync sync = new Sync(); @Override public void lock() { sync.acquire(LOCK); } @Override public void lockInterruptibly() throws InterruptedException { sync.acquireInterruptibly(LOCK); } @Override public boolean tryLock() { return sync.tryAcquire(LOCK); } @Override public boolean tryLock(long time, TimeUnit unit) throws InterruptedException { return false; } @Override public void unlock() { sync.release(UNLOCK); } @Override public Condition newCondition() { return sync.newCondition(); } public boolean isLock() { return sync.isHeldExclusively(); } /** * 一般同步器都使用静态内部类去控制外部类的状态 */ private static class Sync extends AbstractQueuedSynchronizer { /** * 独占模式 * * */ // 获取锁 @Override protected boolean tryAcquire(int arg) { if (compareAndSetState(UNLOCK, LOCK)) { //状态为0的时候获取锁 setExclusiveOwnerThread(Thread.currentThread()); return true; } return false; } //释放锁 @Override protected boolean tryRelease(int arg) { if (getState() == UNLOCK) throw new IllegalMonitorStateException(); setExclusiveOwnerThread(null); setState(UNLOCK); return true; } //锁是否被占用 @Override protected boolean isHeldExclusively() { return getState() == LOCK; } /* //共享模式 @Override protected int tryAcquireShared(int arg) { return super.tryAcquireShared(arg); } @Override protected boolean tryReleaseShared(int arg) { return super.tryReleaseShared(arg); }*/ final ConditionObject newCondition() { return new ConditionObject(); } } } 属性方法acquire tryAcquire，尝试直接获取锁。tryAcquire方法是子类实现的。以Mutex为例。 12345678@Override protected boolean tryAcquire(int arg) { if (compareAndSetState(UNLOCK, LOCK)) { //状态为0的时候获取锁 setExclusiveOwnerThread(Thread.currentThread()); return true; } return false; } 通过CAS操作尝试修改锁状态，注意这个state状态是volatile的，没有做其他同步。 使用unsafe对象类进行CAS操作，成功返回true，失败为false。如果CAS修改成功了，将当前线程设为持有该锁的线程。这里设置持有线程并没CAS，因为只有一个线程能够执行这行代码，不会产生竞争。 如果tryAcquire失败，说明这个时刻有其他线程持有锁了，如果锁是非共享的话，当前线程是无法获取锁的，先添加一个waiter，然后调用acquireQueued。 主体也是一个死循环（实际上这里就有点像自旋锁的旋转了）： 如果刚刚插入的新节点的前驱节点是head，说明刚刚插入的新节点是队列里的第二个。这时候再tryAcquire一下试试，如果试成功了，那刚刚插入的新节点就是锁的持有者了，把他当成head，setHead方法只会有一个线程访问因此不需要CAS。 如果刚刚插入的新的节点不是第二个节点，或者他是第二个但是tryAcquire失败了，需要判断一下当前线程是否需要阻塞（park）。判断规则挺复杂： 2.1. 前驱节点为SIGNAL，说明应该阻塞。 2.2. 前驱节点状态>0，说明前驱节点状态为CANCELLED，从队列中删除掉前驱节点，并继续判断前驱的前驱。不要阻塞。 2.3. 前驱节点状态为其他（CONDITION、PROPOGATE），通过CAS把前驱节点状态设为SIGNAL。同样不要阻塞。 如果要阻塞，LockSupport.park来阻塞当前线程。唤醒当前线程可以通过3种方式： 3.1. unpark当前线程 3.2. interrupt当前线程 3.3. unsafe.park意外返回了！ 线程不管通过哪种方式醒了，也不代表他可以拿到锁了，还是要进入循环中，判断是否是第二个节点并且tryAcquire的。 从acquireQueued可以看出队列中线程的竞争有以下特点： 只有第二个节点才有资格获得锁，第一个节点是正在持有锁的节点。 队列中线程的等待和争夺模式类似于自旋的概念，阻塞——>尝试获取——>阻塞——>尝试获取——>获取成功~。 release从概念上来说，release操作应该比acquire操作简单一些。。。 首先调用子类的tryRelease方法，直接设置状态为unlock即可，都不用CAS。如果tryRelease失败了的话，说明那个线程本来就没有拿到锁，是个非法调用。 拿到head节点，只要head存在并且head状态不为0，就通过unparkSuccessor唤醒他的后继节点。如果他刚刚tryRelease成功之后立刻有线程拿到锁，成为了头节点，那这时候头节点就不是这个释放锁的节点了。但还是要尝试唤醒一下这个节点（虽然他极有可能是拿不到锁的）。 如果这个节点的后继节点变为null了，或者状态时CANCLLED，就从tail节点开始沿队列往前找，直到找到一个状态不为CANCELLED的节点。唤醒这个节点上的正在沉睡的线程。 简单的Mutex的加锁解锁过程总结加锁 首先尝试能否通过CAS操作直接设置锁标志位，如果成功，说明当前线程很轻易地抢到了锁，加锁结束。 如果失败，把当前线程封装到一个node对象里，尝试通过CAS操作将该node插入队列尾部。如果一次CAS失败，进入一个死循环，不停地检查队列是否为空，不停地尝试通过CAS将node插入队列尾部，直到成功。 进入正式等待的死循环，如果当前节点为第二个节点，说明有资格获取锁，通过CAS尝试修改锁标志位，如果成功，当前节点升级为头节点，加锁结束。 如果失败，判断当前节点是否需要睡眠。只要其前驱节点状态为SIGNAL，就说明需要睡眠，通过unsafe的park方法让当前线程睡眠。 解锁 直接修改锁标志位，表示自己释放了锁。 调用LockSupport的unpark方法，唤醒他后继节点的线程。如果在唤醒之前发现他的后继节点为null了，就从tail往前找第一个状态不为CANCELLED的节点。","link":"/2018/10/06/juc-aqs/"},{"title":"JUC——ConcurrentHashMap","text":"1.7中的ConcurrentHashMap使用分段锁来解决并发线程之间的同步竞争问题，本文总结1.8中的实现。 属性 table：Node类型的数组，用于存放散列桶 nextTable：扩容阶段的新的table baseCount sizeCtl：一个状态变量 -1表示map正在初始化 -(1+n)表示正在有n个线程一起努力扩容map 在初始化之后，正数表示下一次扩容的阈值 在初始化之前，正数表示table的初始大小 transferIndex cellsBusy counterCells：CounterCell的数组，CounterCell似乎是用来计数的。 方法1. putVal一个很长很长的方法： 1234567891011121314151617181920212223final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node[] tab = table;;) { Node f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) & hash)) == null) { if (casTabAt(tab, i, null, new Node(hash, key, value, null))) break; // no lock when adding to empty bin } else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { V oldVal = null; //省略部分代码 } } addCount(1L, binCount); return null; } 通过spread算出hash。spread的算法与HashMap中的hash函数类似，把高16位与低16位进行异或得到结果（当然高16位还是被保留的），区别在于这里hash值的最高位用作标志位，因此只能用低31位作为hash之后的index，最高位强制为0。 进入一个死循环。 2.1. 检查刚刚拿到的table是否为null，或者长度是否为0，如果是空的，就初始化一个table。这里有可能出现多个线程同时调用initTable的情况。initTable函数也是一个死循环，在这个循环中多个线程争夺修改sizeCtrl变量的机会：抢着把sizeCtrl改为-1，如果CAS修改操作成功，说明抢到了初始化table的机会，再次检查一下table是不是空，如果是空的，就让table指向一个长度为n的新的Node数组。最后把sizeCtrl改为原来sizeCtrl的0.75倍。也就是说，在table为null的时候，sizeCtrl会发生缩减。 这里把sizeCtrl改为-1相当于一个latch，把其他线程锁在了外面（其他线程只能yield）。一旦某个线程完成了table的创建，所有线程就都会从循环中退出来，退出initTable函数。 在while条件处会判断table是否为null，或者长度是否为0，当CAS操作成功只有还会再做一次判断，这样判断两次的目的应该是为了防止在进入while循环后，其他线程完成了table的创建，sizeCtrl变为正数，那如果当前线程再new Table的话就会造成资源的浪费了。（光一个创建数组操作就如此复杂，滴水不漏） 2.2.1. 如果f为空，通过CAS在table的hash位置插入一个新的node，插入成功则退出，插入失败则继续循环（回到2）。这里CAS操作只做一次，如果失败了就说明有另外一个线程在table的同样的地方插入了一个新的元素，而插入失败的线程就只能在下一次循环中遍历这个链表，把node插在链表尾部了。 2.3. 如果table的hash位置的f的hash==MOVED（-1），说明这个node后面的链表正在扩容，调用helpTransfer帮助其扩容。 2.4. 通过synchronize把这个桶加锁，尝试往这个桶中插入元素。 2.4.1. 如果是链表，就遍历链表并检查有无Node的key与目标key相等，如果存在就修改value为新的值，如果没有就在链表尾部插入一个新的节点。 2.4.2. 如果是红黑树，调用TreeBin的putVal方法插入元素。 2.4.3. 在遍历链表的过程中还会统计这个链表中节点个数，如果超过TREEIFY_THRESHOLD就会把这个链表变为一棵二叉树。 2.5. 调用addCount，元素个数加一。 putVal的步骤总结一下： 1. 检验参数，计算hash，进入for循环2。 2. 一个循环。 2.1. 如果table为空，initTable。 2.2. 如果table[hash&(table.length-1)]为空，尝试CAS插入新节点。 2.3. 如果table[hash&(table.length-1)]的hash是MOVED，调用helpTransfer帮助其扩容。 2.4. 同步加锁，向链表或者红黑树中插入新节点，或者修改已有节点的value。 3. map元素个数加1。 2. addCountaddCount语义上的功能为让map的大小（size）加1。addCount的代码可以分为两段。 如果counterCells为null，就直接返回。 如果通过CAS直接修改baseCount成功，就直接返回。 如果刚刚拿到的counterCells为null（说明CAS是失败的），就调用fullAddCount，并返回。 如果刚刚拿到的counterCells（记为as）不为null，而as长度为0（本质上和null是一回事），就调用fullAddCount，并返回。 如果as的某个随机的位置为null，就调用fullAddCount，并返回。 如果通过CAS直接修改CELLVALUE失败了，就调用fullAddCount，并返回。 如果以上条件都不满足，调用sumCount计算得到一个s。 sumCount的实现逻辑是在baseCount的基础上，累加上所有的CounterCell的值。 以上是addCount的上半段，下半段应该是check>=0的时候需要进行的操作。 主体是一个while循环，循环判断的条件比较苛刻： 刚刚计算的sumCount比sizeCtrl大： 1.1. table正在扩容 1.2. 或者sumCount比下一次扩容的阈值大 table不为空 table大小不超过最大容量 while循环体： 根据table长度计算resizeStamp（记为rs）。 如果sizeCtrl","link":"/2018/10/14/juc-concurrenthashmap/"},{"title":"JUC——ReentrantLock","text":"单线程持有的、可重入的锁。 一个可重入的互斥锁定 Lock，它具有与使用 synchronized 方法和语句所访问的隐式监视器锁定相同的一些基本行为和语义，但功能更强大。ReentrantLock 将由最近成功获得锁定，并且还没有释放该锁定的线程所拥有。当锁定没有被另一个线程所拥有时，调用 lock 的线程将成功获取该锁定并返回。如果当前线程已经拥有该锁定，此方法将立即返回。可以使用 isHeldByCurrentThread() 和 getHoldCount() 方法来检查此情况是否发生。 ReentrantLock有3个内部类，一个Sync类及其衍生出的两个子类FairSync和NonFairSync，代表公平锁和非公平锁。 公平性的控制AQS的acquire方法的第一步就是通过子类的tryAcquire方法来尝试获得锁。 判断锁有没有被其他线程持有，如果在该线程之前没有其他等待线程并且CAS操作修改锁状态成功了，说明加锁成功。 如果锁被某线程持有，而且这个线程就是自己，给状态数值加上一个数（1），证明自己重入了。 不会判断是否有更早的等待线程。 release在AQS中，首先tryRelease，然后唤醒后继节点。 tryRelease会让锁状态state减少1，如果变为0了，说明释放锁的次数和锁重入的次数相等了，当前线程已经完全释放了锁。值得注意的是，由于一次只会有一个线程调用tryRelease，因此这里对state的修改不需要CAS。 如果tryRelease返回true了，说明这个线程完全释放了锁，可以唤醒后继节点了。 tryLock(timeout)带有延迟参数的加锁方法，这个方法是Lock类与synchronize关键字相比的一大优势，他提供了有限时间内尝试获取锁的机制，而不是无穷无尽的等待。 调用tryAcquire尝试获取锁。 调用doAcquireNanos方法。 doAcquireNanos方法和acquireQueued方法很相似。 计算ddl，即实际到期时间。 调用addWaiter，在队列尾部插入一个节点。 进入死循环，检查该节点是否有资格拿锁（即是否为第二个节点），如果有资格，tryAcquire。 计算距离ddl还剩多长时间记为delta。 如果delta","link":"/2018/10/06/juc-reentrantlock/"},{"title":"JVM——垃圾收集（GC）和内存分配","text":"GC日志JVM可以输出GC日志供开发者分析GC过程。 -XX:+PrintGC 输出GC日志 -XX:+PrintGCDetails 输出GC的详细日志 -XX:+PrintGCTimeStamps 输出GC的时间戳（以基准时间的形式） -XX:+PrintGCDateStamps 输出GC的时间戳（以日期的形式，如 2013-05-04T21:53:59.234+0800） -XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息 -Xloggc:../logs/gc.log 日志文件的输出路径 垃圾的定义在Java中，要定义一个堆中的对象是垃圾的方法是判断是否通过全局中所有的引用都无法接触到这个对象，如果是，说明这个对象再也不可能被建立新的引用，即没有存在的意义，因为对象存在就是要被引用到。因此它就是个垃圾。 引用计数（Reference Counting）引用计数是不可能引用计数的，这辈子是不可能引用计数的。引用计数无法解决循环引用的问题。 可达性分析（Reachability Analysis） 从一些被视为GC Root对象开始，从这些节点开始向下搜索，搜索经过的路径被称作引用链，如果一个对象到GC Roots没有任何引用链相连（不可达），就说明这个对象是个垃圾。 GC Roots包含以下几种对象： 虚拟机栈中：引用对象（本地变量表） 方法区中：类静态属性引用的对象 方法区中：常量引用的对象 本地方法栈中：JNI引用的对象 同时JVM更加细粒度地区分了引用的强弱： 强引用：类似于Object obj=new Object(); 软引用：有用但非必须的对象。被软引用的对象不会被首先考虑回收，只有在第一轮回收之后内存仍然可能溢出的时候才会将软引用的对象纳入回收范围。 弱应用：在下一次垃圾回收的时候都会被回收掉。 虚引用：虚引用对象对于对象的生命周期（何时被回收）不会产生任何影响，有时仅仅是为了使虚引用对象能够在被回收的时候收到系统的一个通知。 需要注意的是，GC Roots实际上是一组被特殊对待的指针。没有任何对象能够引用GC Roots。 GC Root在对象图之外，是特别定义的“起点”，不可能被对象图内的对象所引用。 一个常见的误解是以为GC Root是一组对象。 实际情况是GC Root通常是一组特别管理的指针，这些指针是tracing GC的trace的起点。它们不是对象图里的对象，对象也不可能引用到这些“外部”的指针，所以题主想像的情况无法成立。 另外，tracing GC能正确处理循环引用，保证每个活对象只会被访问一次就能确定其存活性。对象图里是否存在循环引用，tracing GC都能正确判断对象的存活与否。 作者：RednaxelaFX 链接：https://www.zhihu.com/question/29218534/answer/43580432 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 生存还是死亡一个对象被回收需要经历两次标记过程： 某次可达性分析发现该对象不可达。被标记后，JVM会将该对象加入到一个队列中，同时在一个优先级较低的线程运行该对象的finalize方法（但并不保证finalize方法会执行完成）。 对该队列中的对象做一次小规模的可达性分析，并标记不可达的对象。 回收方法区由于和堆区性质不同，有很多人认为方法区的对象是不会被回收的，然而事实并不是这样。JVM规范中没有规定方法区一定要GC，但实际上，面对大量使用反射、动态代理等字节码技术，JVM是需要在方法区做GC的。 方法区垃圾回收的效率一般较低，远不如堆区每次能够释放百分之七十以上的空间。 方法区主要回收两部分内容： 废弃的常量：标记的方法和堆区的变量类似。 无用的类：一个类是无用的有三个条件：该类所有实例都已经被回收；ClassLoader已被回收；Class对象没有在任何地方被引用，也无法再任何地方通过反射获取。当满足这三个条件，则说明该类可以被回收，但是否会采取措施进行回收还是要看JVM的参数。 垃圾收集算法标记-清除（Mark-Sweep）这是最原始也是最基础的方法，先扫描内存空间中的所有对象，对其中应该回收的进行标记。然后释放所有被标记对象所占用的空间。 两个缺点： 效率不高。 会在内存空间产生大量的不连续的碎片。 复制（Copying）将内存分为两块，每次只使用其中的一块。当被使用的块发生了GC之后，将其中剩余的对象复制到另一个块中，同时紧密排列存活的对象。 这样做的优点是对内存的空间管理（已用部分和剩余部分）非常简单易操作。缺点是牺牲了一半的内存空间的可用性。 实际应用中，新生代都是使用这种方法进行垃圾收集的。而且有聪明的人发现，新生代中，98%的对象的生命周期非常的短暂。因此可以采取如下做法： 如上图所示，新生代被分成三个部分，一个较大的区域名为Eden，另两个较小的且大小相等的区域名为Survivor。每次使用Eden和其中一块Survivor，当发生GC后，将Eden和Survivor中的存活对象复制到另一个Survivor中，最后清理掉Eden和Survivor中用过的空间。 标记-整理（Mark-Compact）在标记-清除的基础上，每次GC后重新排列存活的对象，使其位于内存的一端。 分代收集（Generational Collection）其实这只是一种内存分配规划思路，将内存分为新生代和老年代，对于不同的代采取不同的垃圾收集算法。 垃圾回收分类 MinorGC（新生代GC） MajorGC/FullGC（老年代GC） HotSpot算法实现枚举根节点要做GC扫描的话，就需要从每个GC Root出发，追踪所有引用的对象。可作为GC Roots的对象主要是方法区的常量或类静态属性，以及栈中的局部变量表，因此如何高效的表里局部变量表也是十分重要的。 为了避免在整个方法区或者栈中遍历寻找GC Roots，JVM维护了一个表（Oopmap），这个表记录了在方法区和栈中哪个位置存放了对象的引用。这样JVM扫枚举根节点时，只需要遍历Oopmap即可，大幅度降低了枚举的工作量。 在我看来，Oopmap有点像是一个对一个范围内所有引用对象的快照。 安全点由于在程序执行过程中，对象的引用关系随时可能发生变化，因此Oopmap也会随时发生变化。JVM不会实时更新Oopmap，因为这样对性能的消耗会非常的大，因此JVM只选择在特点的位置更新Oopmap到最新状态，同时可能进行GC。这些位置就是安全点。 安全点的选择基本上是以程序“是否具有让程序长时间执行的特征”为标准进行选定的，如方法调用、循环跳转、异常跳转等。 安全区域安全区域是指，在一段代码区域内，引用关系不会发生变化。相当于扩展的安全点。 根节点枚举是GC操作的必要步骤，同时进行根节点枚举必须在一个确保一致性的快照中，这就是GC需要停顿所有执行线程进行根节点枚举的原因。 垃圾收集器 ![](https://www.dokyme.cn/wp-content/uploads/2018/05/JavaGC.png) 按线程 单线程：Serial、SerialOld 多线程：ParNew、Parallel Scavenge、Parallel Old、CMS、G1 按适用代 新生代: Serial、ParNew、Parallel Scavenge 老年代: SerialOld、CMS 、Parallel Old G1可以在新生代和老年代使用 常见的组合 ParNew+CMS Parallel Scavenge+Parallel Old 作者：王常贵 链接：https://www.zhihu.com/question/21535747/answer/18548071 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。 CMS（Concurrent Mard Sweep）收集器 四个步骤： 初始标记（initial mark）：Stop The Word，并标记GC Root直接引用的对象，并做标记。这一步很快。 并发标记（concurrent mark）：恢复执行线程，同时继续追踪各个引用路径，完成可达性分析。 重新标记（remark）：Stop The Word，修正前一个阶段中标记产生变动的对象的标记记录。 并发清除（concurrent sweep）：恢复执行线程，同时进行垃圾清除操作。 缺点： 1. 对CPU资源敏感 2. 由于在并发清除阶段没有stop the world，因此无法应对“浮动垃圾”，即标记结束之后新产生的垃圾 3. 基于清除算法，会产生大量碎片 G1收集器四个步骤： 初始标记（initial mark）：同CMS。 并发标记（concurrent mark）：同CMS。 最终标记（final mark）：同CMS的重新标记 筛选回收（live data counting and evacuation）：对各个region的回收价值和所需时间（也就是性价比hhhhh）进行评估，排序，然后根据用户能够接受的GC停顿时间来指定回收计划。 G1和CMS最大的区别在于，G1将内存区域分为很多个region，对每个region的GC性价比进行评估，并维护一个GC优先列表。 找到了一个详细介绍G1垃圾回收过程的网站。 对于新生代对象很简单，把Eden和Survivor中的对象做标记复制，复制到另一个Survivor或者提升到老年代中。需要Stop the World。 对于老年代对象首先，初始标记，标记所有GC Roots直接关联的对象。实际上这一步是在新生代GC的过程中顺带着做的。需要Stop the World。 G1的优势 分代收集 空间整合，不会存在碎片 可预测的停顿时间 内存分配与回收其实内存分配的策略并不是一成不变的，不同的收集器组合可能会采用不同的内存分配规则。 对象优先在Eden分配对象会优先在Eden区分配，如果Eden区空间不足，会触发一次minor GC。 大对象直接进入老年代因为新生代采用复制算法收集内存，如果新生代中有很多大对象，那复制就会很费时间，因此大对象直接放入老年代，使用标记清除或者标记整理算法更合适一些。 长期存活的对象进入老年代JVM记录每个新生代对象的年龄（age），每经过一次minor GC，age加一，当超过一定值的时候，就会被提升到老年代。 动态对象年龄判定如果survivor空间中某个年龄的所有对象的所占的大小大于survivor空间大小的一半，年龄大于等于该年龄的对象就可以提升到老年代中。全体晋升。 空间分配担保在MinorGC时，会先检测老年代最大可用的连续空间是否大于新生代所有对象总空间。如果不成立，检查历次晋升到老年代对象的平均大小是否大于老年代最大可用连续空间，如果小于，则进行一次MinorGC，如果大于，进行FullGC。 担保失败后只能进行FullGC，回收老年代空间。","link":"/2018/05/06/jvm-gc/"},{"title":"JVM——内存区域和内存溢出异常","text":"内存区域作为VM，Java Virtual Machine模拟了操作系统的一些功能，其中之一就是对内存进行管理。JVM内存大致分为下面几个区域： 方法区，是线程共享的。用于存储已被JVM加载的类信息、常量、静态变量、编译后的代码等数据。过去开发者常常称这个区域为“永久代”，因为Hotspot虚拟机GC在管理方法区的时候是采用永久代的方式。当然Hotspot以及开始逐渐放弃永久代。本来垃圾回收（GC）在方法区就是较为少见，且较为令人满意的，这区域的内存的主要回收目标是常量池的回收和类的卸载，但这个区域的回收是十分有必要的。当内存无法满足需要时也会抛出OutOfMemoryError。 虚拟机栈，是线程私有的，因为每个线程的函数调用情况可能各不相同。每个方法在执行的同时会创建一个栈帧（frame），用于存储局部变量表、操作数、动态链接、方法出口等信息。在JVM规范中，VM栈可能会因为线程请求过多的栈（函数调用过多）导致抛出StackOverflowError，也有可能因为VM动态扩展VM栈的时候无法申请到足够的内存导致抛出OutOfMemoryError。 本地方法栈，和虚拟机栈类似，是线程私有的，区别在于虚拟机栈保存的是字节码指令的栈，而本地方法栈保存的是本地方法服务。Hotspot中本地方法栈和VM栈是合一的。 堆（heap），是线程共享的。和数据结构中的堆不同，这个堆就是用来描述内存中的一块区域的名字。一般来说堆是各个区域中最大的一块。JVM规范中指出：所有的对象实例以及数组都要在堆上分配。但是随着JIT编译器的发展与逃逸分析技术的成熟，栈上分配、标量替换优化技术将会导致这样的分配也不是绝对的了。堆在物理内存中可以是不连续的也可以是连续的。在堆无法扩展时（new对象过多）会抛出OutOfMemoryError。 程序计数器。计算机组成原理中一个很重要的概念就是存储程序，其中用来标记指令执行的进度、保持或改变指令执行顺序的就是程序计数器（Program Counter简称PC，或Instruction Pointer简称IP）。到了JVM中，也有程序计数器。由于每个线程都有自己的程序运行情况，因此每个线程都要由一个自己的PC，指向字节码指令的地址。JVM中的PC占用的内存空间很小，几乎可以忽略不计，也不会抛出OutOfMemoryError。 此外，还有两个概念。 运行时常量池。是方法区的一部分，用于存放编译期生成的各种字面量和符号引用。 直接内存。物理内存（当然还是在操作系统管理之下的）而不是JVM中的内存。 对象对象的创建 对象内存布局 对象锁状态对象处于不同的状态时，对象头的布局也会产生变化，对象头的格式并不是像TCP报文头这样子一直不变的。 锁有重有轻，有乐观有悲观，从这么多角度来将锁的概念分类并区分设计，目的就是为了应对不同的并发场景，最大程度提升系统性能。 对象的访问定位目前主流的访问方式有两种：句柄和直接指针。 句柄：在JVM堆中会有一个句柄池，引用变量存储的就是句柄在句柄池中的地址，句柄在存放实例池中的对象实例数据以及方法区的对象类型数据。当对象在JVM堆中被移动的时候，不需要改变引用变量的值，只需要改变句柄中的值即可。 直接指针：引用变量直接存储JVM堆中对象实例地址，对象实例数据中保留一个指针指向方法区中的对象类型数据。速度比使用句柄更快一些。 Hotspot使用的是第二种。","link":"/2018/02/16/jvm-mm/"},{"title":"K2-PandoraBox-IPV6（教育网）配置过程","text":"坐标：东南大学九龙湖校区梅园4C某宿舍 昨晚向某个同学要了一台斐讯K2路由器，本想替换掉宿舍的TP-Link来提供5GHz频段的Wifi，因为2.4GHz频段的Wifi对我MacbookPro2018的USB-HUB以及蓝牙干扰太过于严重。忽然想把宿舍隔壁床位的IPV6网口利用起来。 之前尝试过将PC网口与IPV6网口直连上网，体会到了飞一般的网速，举个例子：下载一个ubuntu-desktop的镜像，大概只需要两分多钟就可以下载完毕。 K2固件相关工作 刷breed不死鸟固件。 刷PandoraBox固件。 联网在浏览器中打开192.168.1.1，输入默认密码admin，进入PandoraBox管理页面。 由于我此时已经将IPV6网口与路由去WAN口相连，因此在网络-接口界面可以看到WAN6口已经通过DHCP从上游路由器（DHCP服务器）得到了IPV6网址。 如果插入了IPV6网线，但页面上没有IPV6地址，可以尝试点击“连接”按钮重新连接上游路由器，并等待1分钟左右，一般就可以获得IPV6地址了。 下载所需程序打开一个控制台，通过ssh连到路由器上，密码默认为admin： 1ssh root@192.168.1.1 路由器的操作系统也是Linux的一种，因此熟悉PC端Linux操作的人应该不会有太大困难。 123opkg update......opkg install ip6tables kmod-ipt-nat6 安装2个程序，装好了之后回到浏览器。 配置进入网络-接口-LAN-DHCP服务器页面，勾选总是通告默认路由。 进入网络-防火墙-自定义规则页面，添加一条规则，并重启防火墙。 1ip6tables -t nat -A POSTROUTING -o eth0.2 -j MASQUERADE 回到路由器终端，找出IPV6的默认路由网关地址，然后添加一条新的路由规则： 123ip -6 route | grep default假设输出的地址为xxxx::xxxxroute -A inet6 add default gw xxxx::xxxx dev eth0.2 最后，重启一下相关服务。 12/etc/init.d/firewall restart/etc/init.d/network restart 测试连通性重启结束之后，PC就可以ping通谷歌了，如果ping不通，可以重连一下路由器的Wifi。 打开网页离终点只有一步之遥了：由于chrome浏览器采用的策略是IPV6和IPV4共存，解析网址的IPV6和IPV4地址，然后优先与IPV6地址发起连接，当连接的时间超过300ms（具体数值不清楚）时，说明链路中的IPV6机制有问题，就会切换到IPV4地址发起TCP连接，最后发送HTTP请求报文。 这样的流程对于国内想要通过IPV6翻墙的用户而言是不合适的，因为连接远程主机的延迟一般会比较高，导致链路正常联通的IPV6链路被浏览器舍弃，转而尝试去访问无法连接的IPV4地址。 而我们希望的理想的过程是，浏览器先解析IPV6地址，只要IPV6地址存在，就直接去连接该地址，不要去管IPV4地址是多少。或者说串行地去请求做IPV6和IPV4地址的DNS解析操作。在DNS协议中，IPV6和IPV4对应报文头部的type值分别为AAAA和A。此外，需要在本机配置DNS服务器地址为本机地址。 我在Github上找到了这么一个好东西： adamyi/v6dns 这是一个DNS服务的中继程序，会在某个端口上监听所有DNS报文，并按照下面的逻辑运行： 为了解决IPV4/6优先级的问题，可以这么做：在本地运行v6dns程序，监听本机的53端口，并设置DNS查询服务器为某个远程的DNS服务器地址。这样，如果能够查询到某个网站的IPV6地址，那就直接发起连接，如果查不到，就再查询其IPV4地址。 最终总结本文介绍了在教育网环境下使用K2的PandoraBox固件进行配置最终访问IPV6网站的过程，其中最核心的步骤就是配置NAT6，NAT6是基于IPV6的网络地址转换，目的是将上游DHCP6分配的一个IPV6地址转换成内网的多个地址分配给多个主机。 最终测试的效果不佳，在Youtube上看视频时常常出现卡顿的现象，记忆中原来看视频能够达到4K画质，并且帧率为60fps的网速水准。推测原因如下： K2路由器不够稳定。 PandoraBox固件或NAT6机制不够成熟。","link":"/2018/11/25/k2-pandorabox-ipv6/"},{"title":"Linux——进程管理","text":"概念进程（Process）是处于执行期程序以及其相关资源的总和，不仅仅局限于可执行程序代码，通常还包括其他资源如打开的文件、挂起的信号、内核内部数据、处理器的状态、一个或多个内存映射的内存地址空间以及一个或多个执行线程、全局变量数据段等。 在现代操作系统中，提供两种虚拟机制：虚拟CPU和虚拟内存，都给进程一种独占设备的假象。 进程描述符内核把进程存放在一个叫任务队列的双向链表中。链表中的每一个项的类型为task_struct__。task_struct结构体相对比较大，在32位机器上大约有1.7KB。 123456789struct task_struct{ unsigned long state; int prio; unsigned long policy; struct task_struct *parent; struct list_head tasks; pid_t pid; ......} pid和tpidpid是进程的唯一标识符，Linux中使用task_struct来表示进程和线程，为了能够表达多个线程从属于一个进程的意思，使用tpid来记录线程组的标号。某个进程的第一个创建的线程的pid和tpid相同，其后创建的线程的tpid为第一个线程的pid。 内核栈一个进程在调用系统调用的时候，必然会陷入到内核态中，此时运行的代码所操作的栈不再是用户态的进程栈，而是一个内核栈。在内核栈的底部（从高地址生长的栈）存放了一个thread_info__，这个结构指向当前进程的task_struct，从而使系统能够高效地获取到当前进程的信息。 进程状态 Linux进程一般有5种状态 RUNNING：进程在运行或者在运行队列中等待运行 INTERRUPTIBLE：可中断 UNINTERRUPTIBLE：不可中断 TRACED\\STOPPED：被追踪\\暂停，比如在用gdb调试的时候 DEAD：退出 进程的创建fork系统调用 为新进程创建一个内核栈、thread_info和task_struct 检查当前用户有没有超出分配资源的限制 将统计信息清0或初始化 子进程变为UNINTERRUPTIBLE 分配一个pid 根据clone的参数，拷贝父进程的对应资源（文件、系统信息、信号处理函数、地址空间和命名空间等） 扫尾 传统的fork会拷贝所有父进程的所有资源，Linux使用了一种写时拷贝的技术：当fork时先只做浅拷贝（共享内存空间），当有数据需要写入内存空间时，再拷贝原内存空间副本给子进程，并做写入操作。这样，fork的职责就只剩下：复制页表、分配进程描述符。 Linux会倾向于让子进程先执行，因为他希望子进程能够尽快调用exec，这样就可以避免写时拷贝的开销。 vfork在使用了写时拷贝的技术后，vfork与fork的区别就仅仅在于vfork不需要复制父进程页表。 线程在Linux中，线程仅仅是进程进行共享资源的手段，他使用完全相同的task_struct结构来维护线程信息。因此创建线程就相当于分配新的task_struct。 创建线程可以通过clone的参数来指定创建线程时需要共享的资源：打开的文件、文件系统资源、信号响应程序等。 内核线程一些内核任务由内核线程执行，内核线程与普通线程的区别在于内核线程没有独立的地址空间，只在内核空间执行。他参与内核调度也参与内核抢占。 Linux内核程序是一个进程，其中的多个指令流分别运行在多个内核线程中。 常用的内核线程： kthreadd：管理调度其他内核线程 events：将软硬时间包装为event，并分发给感兴趣的线程 pdflush：脏页写回 kswapd0：回收内存 进程终结进程终结有两种情况： exit系统调用 接收到了不能处理但也不能忽略的信号或异常 进程退出时，一般会先释放内存空间资源，释放文件资源，设置退出代码，并向父进程发送退出的信号。此时进程还有内核栈、thread_info、task_struct这三样没有被释放，并且其状态变为ZOMBIE。 进程终结时，内核必须释放他所占用的所有资源并且告知父进程。 修改task_struct中的标志 删除任一内核定时器 释放mm_struct，如果这个地址空间计数变为0，则彻底释放这个地址空间。 分别递减文件描述符和文件系统数据的引用计数，如果计数变为0，则彻底释放。 设置task_struct的exit_code为exit提供的参数 向父进程发信号，给该进程的子进程重新找养父（找养父的规则是在终结进程的线程组中找存活的其他进程——即该进程是否还有存活的线程，如果没有，则选择init进程），线程状态变为ZOMBIE schedule切换到新的进程 在ZOMBIE状态中，进程所占有的资源只有内核栈、thread_info、task_struct，此时进程存在的唯一目的是向他父进程提供信息。","link":"/2018/08/26/linux-process/"},{"title":"MIT_6.828 简介，环境搭建","text":"简介MIT大名鼎鼎的操作系统课程，6.828，根据我初步的了解，这门课非常注重动手能力。大二下学期刚刚上了操作系统课，但是总共就做了两个实验，感觉动手的成分非常的少，作业都是以做题为主。所以想亲自动手尝试一下。 整个课程基于linux的xv6系统，使用qemu模拟器进行运行和调试，同时涉及c语言编译链接的工具链的使用。 相关链接官网： MIT 6.828 搜集的前辈的教程： MIT 6.828 JOS学习笔记5. Exercise 1.3 xv6试验环境bochs及qemu搭建 MIT 操作系统实验 MIT JOS lab1 这些都是中文的博客，感觉中文的资源比较的少，英文的可以用google搜到，就不贴在上面了。 环境搭建ubuntu基础环境先用virtualbox 装个ubuntu16.04的虚拟机，这里不再赘述。 装好以后，换ustc的源， apt update，upgrade，更新一下语言包，还有virtualbox的增强工具。 工具首先是gnu的c编译链接工具，和git，再搞个vim把。 1sudo apt-get install build-essential git vim 按照官网的说明检查一下工具链有没有到位。 1objdump -i 首先要说明一下，这个objdump是linux下的一个反汇编工具。 如果有elf32-i386说明就没错啦。 1gcc -m32 -print-libgcc-file-name 终端应该会输出/usr/lib/gcc/i486-linux-gnu/version/libgcc.a或者/usr/lib/gcc/x86_64-linux-gnu/version/32/libgcc.a这样的东西，按照步骤来的话应该是没什么异常情况的。 xv6和qemu官网上说linux发行版的qemu不行啊，跑xv6会出错，所以用我们mit打包过的qemu吧。但是我还是选择用apt大法安装qemu。要是真的出了什么问题，到时候再说吧。 1sudo apt-get install qemu 然后是xv6系统，我就把它放到home目录下了。 1234git clone git://github.com/mit-pdos/xv6-public.gitcd xv6-publicmakemake qemu 弹出这个窗口，万事大吉。不过要注意了，在qemu里面看不到鼠标哦。","link":"/2017/07/17/mit-6-828-1/"},{"title":"MIT_6.828 Homework: boot xv6","text":"HW:Boot xv6 nm 命令能够列出一个obj文件的符号表，也就是说这行命令在kernel中找_start这个符号，并且获得了它所在的地址。接着启动qemu-gdb和gdb进入调试模式。这里需要两个终端开着，一个运行make qemu-gdb，另一个运行gdb(记得都要在xv6-public目录下启动啊！)。 在地址0010000c处设置断点，然后运行到这个断点。 12br * 0x0010000cc 这里gdb报了个错误，好像是什么保护文件什么的导致不能调试了，按照他给出的提示修改.gdbinit文件就行了。其实在mit的HW提示中也有提到这个。 然后看一看各个寄存器和堆栈中都有一些什么（主要就是为了熟悉一下gdb调试命令吧）。如果不熟悉某个命令可以直接help [命令]查看帮助，而且还挺详细的。 比如x这个指令的格式。 查看一下各个寄存器的内容。 查看一下堆栈的内容（从esp开始，列出24个字，以16进制显示）。IO课结束了有一段时间了，有点忘记进制和字长的关系了。顺便复习一下。 一个word（字）应该是大多数指令能处理的最大长度，或者ALU能够运算的位宽度，字多长也就是常常说的32/64位机。那么问题来了，这个qemu是多少位的呢。。。。。。我猜是32位的，因为eax，ebx这些寄存器都是32位的。在make qemu-gdb的时候显示qemu-system-i386应该也能说明这一点吧。也就是说一个字应该是32位的，4个字节的。 16进制中，两个数字就能代表一个byte的值，即一个byte能存放0x00-0xff。那么一个字就应该是0xffffffff这样的规模。 也就是说这里从0x7bdc的位置开始打印出了24个word。 exercise要求每个位置写一点注释，说明哪里是真正的堆栈，并且弄清楚现在堆栈里面存放的都是啥。要想弄清楚堆栈里面的数据的意义，就需要知道这些数据是什么时候被压入堆栈的，所以要从头开始单步运行。根据exercise的提示，从bootmain.S的0x7c00开始走一遍吧。一遍单步执行一遍可以对照着bootmain.S看，避免走过来头。 乱七八糟的指令先不去管，一直走直到有往esp放东西的时候（esp寄存器存放堆栈段栈顶的位置，如果往堆栈中push了，那么esp中的值会变小，32位机中esp一次移动4byte）。 这个7c00在bootmain.S中的符号是start，也就是一开始的时候的地址嘛。把这个地址赋给了esp。其实逻辑上是讲得通的，因为原来0x7c00-0x7c43存放的是bootmain.S中的指令，现在指令执行完毕了，那这一段内存就可以挪作它用，比如用来做堆栈的内存空间了。 第一个问题解决了：在0x7c43处初始化堆栈。 第二个问题，call bootmain的时候，堆栈只有一个东西（刚刚才初始化，堆栈应该还是空的），应该是call bootmain下一条指令的地址。 执行完call之后立刻看一下堆栈的内容。得到验证了。 第三个问题，对堆栈操作的第一条指令是push ebp，这时候ebp是0看不出什么。 第四个问题，问什么时候eip变成0x1000c了，显然是一个函数调用，bootmain中函数调用只有readseg，stosb，entry三个，前两个地址一看就不对，那明显就是调用entry函数的时候了。这个函数调用会将下一条指令的地址压栈，但是似乎这个下一条指令永远不会被执行，因为entry()函数不会返回！ 花了2个半小时才做完一个作业，太菜了。","link":"/2017/07/19/mit-6-828-lec-1/"},{"title":"MIT 6.828 Lab1：C, Assembly, Tools, and Bootstrapping","text":"Boot a PC 熟悉汇编语言，由于去年学过IO，问题应该不大。这里就不复习了。 PC的物理地址空间 一台PC的物理内存大致是如上图所示分布的。第一台PC（16bit的8088）的可以直接访问的内存只有1MB，可用的就只有图上的Low Memory中的那一部分。然后是显卡的缓冲区，然后是一些设备，最后是BIOS。虽然后来的计算机内存早已超过了1MB，但是为了前向兼容内存较小的机器，1MB的内存的区域还是维持了原来的布局。 ROM BIOS开两个shell，一个运行make qemu-gdb，另一个运行gdb，这样就可以在gdb里面调试OS的指令的执行操作了。 其中： [f000:fff0]是这条指令的逻辑地址，f000是段地址，fff0是段内偏移量，由于第一条指令是代码，在CS段内，所以可以说成CS=f000，IP=fff0。 下一条即将执行的指令是ljmp ....,....，这个单步执行的回显和C++调试是差不多的，都显示了下一条即将执行的语句。这是一个远跳转指令，能够跳转到当前段的0-64KB范围内。 即便是PC开机，第一条执行的指令也并不是在内存的开始位置，而是在0xffff0，因为之前说过BOIS在0xf000-0x100000之间。当然这个第一条指令地址也是在BIOS区的特别靠后的地方了，如果顺序执行，也就剩16条语句了。显然是不够的，所以要跳转。 现在还处于_实模式_状态下，所谓实模式就是说物理地址=16*段地址+段内偏移。这样，一个16位的地址左移四位，变成了20bit，范围1MB。 当BIOS运行的时候，他会初始化中断描述符表，并初始化所有设备，包括VGA显示器，PCI总线，然后是硬盘，CD，最后找一个可以用的boot loader，一旦找到，就会把控制权转移给他。 熟悉使用gdb进行调试的操作，跟着bios走一走，也没什么可说的。 Boot LoaderBootloader的主要功能： 从实模式切换到32bit保护模式。 通过IO指令直接从硬盘读Kernel到内存中。 然后来阅读代码，boot/boot.S和boot/main.c。 先看boot.S，我觉得最重要的是下面这段： 首先从offset读8个sector，正好是一个page（4K），读到一个elf header中去。这个offset的值是0，也就是从硬盘的最开始的位置读1个页。 有个问题是：如果这个bootloader是存放在硬盘的最开始的位置，那为何还要从offset为0的地方再读1个page呢，这个offset为0的地方存放的是什么呢？ 根据代码的语义，这一个page中的数据按照elf格式排布，先读一个page，读到了header就可以获取到整个program的大小，然后就可以通过一个for循环把program的每个segment都读进来了。 一般的 ELF 文件包括三个索引表： ELF header：在文件的开始，保存了路线图，描述了该文件的组织情况。 Program header table：告诉系统如何创建进程映像。用来构造进程映像的目标文件必须具有程序头部表，可重定位文件不需要这个表。 Section header table ：包含了描述文件节区的信息，每个节区在表中都有一项，每一项给出诸如节区名称、节区大小这类信息。用于链接的目标文件必须包含节区头部表，其他目标文件可以有，也可以没有这个表。 根据课程网站的解释和代码前的注释，这一段bootloader代码是从硬盘读取kernel程序的，也就是elf是指的kernel。 通过linux的readelf工具可以读取一个elf程序的头部，当然也可以读elf中的program的header。 1readelf -h obj/kernel/kernal 针对Exercise3的四个问题： 具体是何时从实模式切换到保护模式的？ bootloader的最后一条指令是什么，kernel的第一条指令是什么？ kernel的第一条指令在哪里？ boot loader是怎么知道kernel有多大，有多少个sector要读取的？ 给出自己的回答： 如果说要精确到一条指令的话，应该是一个ljmp。 bootloader最后一条指令是((void (*)(void)) (ELFHDR->e_entry))();，kernel的第一条指令是movw $0x1234,0x472。 kernel的第一条指令在kern/entry.S中。 kernel是elf结构的，读一个头部就对整个kernel的结构一清二楚了。 但是又有一个问题了，只不过是跳转了一个地址，处理器怎么就知道该进入保护模式了呢，难道仅仅是因为这个地址是1MB之外的地址吗？因为第一次进入了1MB之外的区间，所以就进了保护模式？ Loading the kernel首先课程网站建议熟悉一下c语言中的指针操作，强烈监狱补全一下K&R，然后脑补一下pointer.c中程序的执行过程。 我觉得pointer.c中的关于指针和数组的操作都挺简单的，就我个人而言唯一不太熟悉的就是变量在内存中的布局，大端小端，补码反码，指针的强制类型转换等等，所以我自己写了个c程序练练手。 1234567891011121314151617181920212223242526272829303132#includetypedef struct { int i; char ch;}Em;int main(){ int a[6]={1,2,4,8,16,1","link":"/2018/02/16/mit-6-828/"},{"title":"MySQL——总体架构","text":"层次结构大致分为四层，从上往下看： 客户端和连接管理层：主要用于和用户开展交互，或者提供应用程序开发接口。这层还负责管理用户连接会话、权限控制。使用线程池来处理用户连接请求。 核心服务功能层：SQL的分析、优化、函数的执行，缓存结果的查询。这一层会负责生成查询操作。 存储引擎层：如InnoDB和MyISAM。真正负责查询和修改磁盘上的数据。 数据存储层：包括数据、日志、索引等文件。 内存分配 12345678910111213141516used_Mem =+ key_buffer_size+ query_cache_size+ innodb_buffer_pool_size+ innodb_additional_mem_pool_size+ innodb_log_buffer_size+ max_connections *( + read_buffer_size + read_rnd_buffer_size + sort_buffer_size + join_buffer_size + binlog_cache_size + thread_stack + tmp_table_size + bulk_insert_buffer_size) 线程共享内存在MySQL中，线程独享内存主要用于各客户端连接线程存储各种操作的独享数据，如线程栈信息，分组排序操作，数据读写缓冲，结果集暂存等等，而且大多数可以通过相关参数来控制内存的使用量。 key_buffer：MyISAM索引缓存。 query_cache：查询缓存，如果两次查询的结果相同，那么后一次查询的结果可以从缓存里得到。 innodb_buffer_pool：innodb数据和索引缓存。 innodb_additional_mem_pool：innodb字典信息缓存。 innodb_log_buffer：innodb日志缓存，主要用于记录事务操作，缓存里日志足够多了之后才会写到外存的日志文件中去。 线程独享内存 read_buffer：顺序读缓冲区，相当于一个窗口，满了就把查询的结果返回给上层。 read_rnd_buffer：随机读缓冲区，同上。 sort_buffer：排序缓冲区，数据在这里被排序，如果数据过多放不下，那就要放在外存进行排序，效率会大大降低。 join_buffer：join操作缓存，这里就不得不提一下join的原理——嵌套循环了，以驱动表的结果集作为基础，一条一条作为下一个表的过滤条件进行查询。如果说先把驱动表的关联字段读入join_buffer，然后让下一张表直接和join_buffer中的字段比较，这样就可以减少下一张表的扫描次数，从而降低时间开销。 binlog_cache：二进制日志缓冲。 thread_stack tmp_table：如group by、order by等操作会产生临时表，如果表够小的话直接存放在缓存里即可。 bulk_insert_buffer：批量插入操作缓存，如insert into table_name values(),(),()，会在缓冲区满的时候一次性写入磁盘。","link":"/2018/04/30/mysql/"},{"title":"Netty——Bootstrap","text":"以netty-example中的Echo（Server）为例，分析Netty源码的结构与运行过程。 跳过SSL相关的部分。从下面这一行代码开始： 1ChannelFuture f = b.bind(PORT).sync(); sync方法的功能是同步地等待该操作结束，因此对于server启动监听没有实质性的作用，这里可以忽略他。 首先进入到bootstrap.bind方法中，这个方法由AbstractBootstrap负责实现： 123456/** * Create a new {@link Channel} and bind it. */public ChannelFuture bind(int inetPort) { return bind(new InetSocketAddress(inetPort));} 再进入到bind方法中： 12345678910/** * Create a new {@link Channel} and bind it. */public ChannelFuture bind(SocketAddress localAddress) { validate(); if (localAddress == null) { throw new NullPointerException(\"localAddress\"); } return doBind(localAddress);} bind方法做了两件事：1.做一些校验工作；2.调用doBind方法进行真正的监听地址绑定。校验操作一般无足轻重，因此不妨直接进入doBind方法中去： 12345678910111213141516private ChannelFuture doBind(final SocketAddress localAddress) { final ChannelFuture regFuture = initAndRegister(); final Channel channel = regFuture.channel(); if (regFuture.isDone()) { doBind0(regFuture, channel, localAddress, promise); return promise; } else { // Registration future is almost always fulfilled already, but just in case it's not. final PendingRegistrationPromise promise = new PendingRegistrationPromise(channel); regFuture.addListener({ doBind0(regFuture, channel, localAddress, promise); }); return promise; }} 上面的代码并不完整，删除了一些细枝末节，因为目前阅读源码的目的是尽快尽可能完整地理清楚程序脉络，因此我删除了自认为对于程序总体认识没有太大帮助的代码。 doBind方法主要做了两件事： initAndRegiter doBind0 暂时不知道这两个方法各自起了怎样的作用，一个一个来看。 initAndRegister12345678final ChannelFuture initAndRegister() { ...... channel = channelFactory.newChannel(); init(channel); ...... ChannelFuture regFuture = config().group().register(channel); ......} initAndRegister方法所做的工作可以大致分为三块： 调用channelFactory实例化channel； 初始化channel； 将channel注册到group返回的对象上去。 实例化ChannelNetty和NIO都有一个叫做Channel的概念，为了方便区分，这里及后续的分析中都将Netty中的称为Channel，而NIO中的称为JavaChannel。 Channel是Netty中的核心概念之一，表示的是客户端或服务端与远程主机通讯的某个Socket。在大多数语言的网络通讯库中，客户端中只有一种Socket，但在服务端中有两种：ServerSocket和Socket，前者只负责监听端口，后者负责处理与远程主机的数据交换。 至于服务端为什么要使用两种Socket，我认为还是受限于OS系统调用的接口定义。 1channel = channelFactory.newChannel(); 和Spring等一众Java框架类似，对于核心概念的实例化采用了工厂模式。ChannelFactory接口结构很简单： 123public interface ChannelFactory { T newChannel();} 实现了ChannelFactory接口的类不多，工厂对象的实际类型多半就是ReflectiveChannelFactory了。 1234567public ReflectiveChannelFactory(Class clazz) { this.clazz = clazz;}public T newChannel() { return clazz.getConstructor().newInstance();} ReflectiveChannelFactory在构造函数中拿到了channel的类型，然后使用了反射构造函数来实例化对象，没毛病。至于它的构造函数是在何处调用的，在server启动代码链式调用bootstrap的数个方法中，有这么一个方法设置了channel的类型（准确地说是Class对象）。 123public B channel(Class channelClass) { return channelFactory(new ReflectiveChannelFactory(channelClass));} 1234public B channelFactory(ChannelFactory channelFactory) { this.channelFactory = channelFactory; return self();} 这里ReflectiveChannelFactory是直接new出来的，没有通过反射，传入构造函数的参数为channel的类型。AbstractBootstrap简单的实例化了一个channelFactory，并关联到该对象自身。 貌似Netty中很多的setter方法都是这种写法。 channel实例化的过程还是挺简单的：在bootstrap的链式调用中实例化channelFactory，然后在server启动监听时有工厂实例化真正的channel。 初始化channelAbstractBootstrap的init方法是抽象的，该方法的实现被下放到了子类中去： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647void init(Channel channel) throws Exception { final Map options = options0(); synchronized (options) { setChannelOptions(channel, options, logger); } final Map attrs = attrs0(); synchronized (attrs) { for (Entry e: attrs.entrySet()) { @SuppressWarnings(\"unchecked\") AttributeKey key = (AttributeKey) e.getKey(); channel.attr(key).set(e.getValue()); } } ChannelPipeline p = channel.pipeline(); final EventLoopGroup currentChildGroup = childGroup; final ChannelHandler currentChildHandler = childHandler; final Entry[] currentChildOptions; final Entry[] currentChildAttrs; synchronized (childOptions) { currentChildOptions = childOptions.entrySet().toArray(newOptionArray(0)); } synchronized (childAttrs) { currentChildAttrs = childAttrs.entrySet().toArray(newAttrArray(0)); } p.addLast(new ChannelInitializer() { @Override public void initChannel(final Channel ch) throws Exception { final ChannelPipeline pipeline = ch.pipeline(); ChannelHandler handler = config.handler(); if (handler != null) { pipeline.addLast(handler); } ch.eventLoop().execute(new Runnable() { @Override public void run() { pipeline.addLast(new ServerBootstrapAcceptor( ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); } }); } });} 将AbstractBootstrap负责维护的options和attrs无脑地全部设置到channel对象上去。为了避免另一个线程修改options和attrs，访问时都进行了同步，我认为这里的加锁在服务器启动阶段是不会对性能造成太大影响的。 在该channel的pipeline尾部添加一个ChannelInitializer，何时调用其initChannel方法目前还不得而知，但是该ChannelInitializer的目的还是挺明确的： 2.1. 在pipeline尾部添加一个ChannelHandler。 2.2. 在该channel的线程中，异步地在其pipeline尾部添加一个ServerBootstrapAcceptor。 注册channelChannelFuture regFuture = config().group().register(channel); group()返回的是EventLoopGroup，即线程组，Netty中一个EventLoop即为一个线程，将多个线程组合起来管理就产生了“线程组”的概念（和OS中的线程组概念不同）。这里调用了EventLoopGroup接口里的register方法来注册channel，但暂时还无法判断是哪个实现了EventLoopGroup接口的小可爱。 在这一行代码加断点并以调试模式运行Echo Example。 发现config().group()返回的是一个NioEventLoopGroup对象，这个对象是MultithreadEventLoopGroup的子类，其register方法所做之事很简单。 1234@Overridepublic ChannelFuture register(Channel channel) { return next().register(channel);} next()方法返回了一个EventLoop，根据直觉猜测EventLoopGroup是一个可迭代的对象，其中存放了EventLoop的列表，而next()方法类似于迭代器的next()方法，返回下一个EventLoop对象。 1234@Overridepublic EventExecutor next() { return chooser.next();} MultithreadEventLoopGroup的next()方法最终会借助于chooser返回下一个EventLoop对象。暂时还不知道这个chooser是何方神圣，只知道他是MultithreadEventLoopGroup维护的众多对象之一。 12345private final EventExecutor[] children;private final Set readonlyChildren;private final AtomicInteger terminatedChildren = new AtomicInteger();private final Promise terminationFuture = new DefaultPromise(GlobalEventExecutor.INSTANCE);private final EventExecutorChooserFactory.EventExecutorChooser chooser; 由于目前我们还只是专注于启动流程，而启动过程中线程的选择与切换的特点还不能很好地显现出来，因此先不管chooser，回到register方法中。 next()返回了一个SingleThreadEventLoop对象，通知他调用他自己实现的register方法。 1234@Overridepublic ChannelFuture register(Channel channel) { return register(new DefaultChannelPromise(channel, this));} 这里实例化了一个DefaultChannelPromise，并把channel和SingleThreadEventLoop作为参数传入，估计Promise会把他们两人关联起来。 如果忘记了channel是什么东西的话，可以回溯到ChannelFactory的部分：ChannelFactory通过反射实例化channel对象，而其类型就是在bootstrap对象上进行方法链式调用时传入的NioServerSocketChannel.class。 实例化promise后并没有结束，因为还没有注册呢： 123456@Overridepublic ChannelFuture register(final ChannelPromise promise) { ObjectUtil.checkNotNull(promise, \"promise\"); promise.channel().unsafe().register(this, promise); return promise;} 此时promise.channel()返回的就是刚才传入构造函数的NioServerSocketChannel。unsafe是个什么玩意儿还不是很清楚，但联想到JDK中有一个也叫Unsafe的组件，用于提供对内存的直接操作，就可以猜测这个unsafe应该是负责某些底层操作流程的。 12345678910111213141516@Override public final void register(EventLoop eventLoop, final ChannelPromise promise) { ...... AbstractChannel.this.eventLoop = eventLoop;//从此这个channel有了属于自己的eventloop if (eventLoop.inEventLoop()) { register0(promise); } else { eventLoop.execute(new Runnable() { @Override public void run() { register0(promise); } }); ...... } } unsafe的register方法的主要内容如上，最终目的是调用register0方法，注册promise。直觉告诉我，名字里带0的方法都差不多快要到终点了。 1234567891011121314151617private void register0(ChannelPromise promise) { ...... boolean firstRegistration = neverRegistered; doRegister(); ...... pipeline.invokeHandlerAddedIfNeeded(); ...... pipeline.fireChannelRegistered(); if (isActive()) { if (firstRegistration) { pipeline.fireChannelActive(); } else if (config().isAutoRead()) { beginRead(); } } ......} 除了触发了几个事件之外，最神秘的当属doRegister方法，直觉告诉我，当看到带有do字样的方法的时候，就离终点不远了。 123456789101112131415161718192021@Overrideprotected void doRegister() throws Exception { boolean selected = false; for (;;) { try { selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this); return; } catch (CancelledKeyException e) { if (!selected) { // Force the Selector to select now as the \"canceled\" SelectionKey may still be // cached and not removed because no Select.select(..) operation was called yet. eventLoop().selectNow(); selected = true; } else { // We forced a select operation on the selector before but the SelectionKey is still cached // for whatever reason. JDK bug ? throw e; } } }} doRegister的主体是一个死循环，虽然这个死循环只会重复一到两次，在javaChannel上注册selector以及感兴趣的事件，同时，由于AbstractNioChannel本身是一个AttributeMap，因此自己作为属性的持有者也被注册到了javaChannel上。 这个register方法以及属于java.nio中的代码，总算可以说到终点了。 现在回到AbstractBootstrap的doBind0方法。 doBind0123456789101112private static void doBind0( final ChannelFuture regFuture, final Channel channel, final SocketAddress localAddress, final ChannelPromise promise) { channel.eventLoop().execute(new Runnable() { @Override public void run() { ...... channel.bind(localAddress, promise).addListener(ChannelFutureListener.CLOSE_ON_FAILURE); ...... } });} doBind0在另一个线程中将监听的地址绑定到channel上去，猜测这个channel应该是一个NioServerSocketChannel。然而并不是： 1234@Overridepublic final ChannelFuture bind(SocketAddress localAddress, ChannelPromise promise) { return tail.bind(localAddress, promise);} netty试图找到pipline上的最后一个OutboundHandler，这个handler有一个与之关联的HeadContext，它的bind方法如下： 123456@Overridepublic void bind( ChannelHandlerContext ctx, SocketAddress localAddress, ChannelPromise promise) throws Exception { unsafe.bind(localAddress, promise);} 1234567891011121314@Overridepublic final void bind(final SocketAddress localAddress, final ChannelPromise promise) { ...... doBind(localAddress); ...... if (!wasActive && isActive()) { invokeLater(new Runnable() { @Override public void run() { pipeline.fireChannelActive(); } }); }} unsafe的bind方法在调用doBind同时，不忘发起一个channelActive事件。 最终让javaChannel监听该地址： 12345678@Overrideprotected void doBind(SocketAddress localAddress) throws Exception { if (PlatformDependent.javaVersion() >= 7) { javaChannel().bind(localAddress, config.getBacklog()); } else { javaChannel().socket().bind(localAddress, config.getBacklog()); }} 注意到，此时javaChannel上的interestOps只有0，也就是无论channel上发生什么事件，都不会被selector选中，作为一个服务器，是肯定要处理ACCEPT事件的，在pipeline.fireChannelActive()中，经过pipline，调用到了unsafe.beginRead()方法： 12345678@Overrideprotected void doBeginRead() throws Exception { ...... final int interestOps = selectionKey.interestOps(); if ((interestOps & readInterestOp) == 0) { selectionKey.interestOps(interestOps | readInterestOp); }} 这里的readInterestOp的值为16，对应的java nio中的OP_ACCEPT。 总结netty启动过程还是很复杂的，启动过程总结如下： channelFactory通过反射实例化ServerSocketChannel对象。 初始化channel的attrs和options。 要求新连接到来的时候，在pipeline尾部添加一个ServerBootstrapAcceptor。 选择一个eventLoop，注册到该channel上。 在javaChannel上注册这个eventLoop的selector。 触发handlerAdded和channelRegistered事件。 将javaChannel绑定到指定的地址和端口上去。 触发channelActivated事件。 注册OP_ACCEPT到selector上去。","link":"/2018/11/14/netty-bootstrap/"},{"title":"Netty——NioEventLoop(1)","text":"作为Netty中最核心的概念之一，NioEventLoop作为线程实体承载Netty中几乎所有代码的运行、所有事件的检测和触发。EventLoop这个单词并不陌生。在很多的图形用户界面（GUI）程序中，都会使用较少的数个线程来运行代码，以应对用户操作事件。 由于Netty的线程模型基于Java的NIO，而NIO又是通过IO多路复用实现的。IO多路复用是底层技术，在其之上则是负责管理多路IO的线程模型。Reactor模式就是一种经典的多线程IO设计模型。 上面的第一张图是多线程IO模型，第二张图是单线程NIO模型，第三张图是Reactor线程模型。第一和第二张图的区别主要在于线程是否复用，第二和第三张图的区别在于第二张图使用单线程（进程）处理读写事件而图三将对同一台远程主机的操作聚合到一个Handler里面，由Reactor负责派发事件给Handler执行。 当然，Handler中代码的执行也是需要线程的，这时就可以考虑线程的复用，即通过线程池管理线程。而Acceptor也可以进行多线程并发，也可以使用线程池。 EventLoop类体系 图中白框框处的是JDK中自带的类。显然Netty也是利用了JDK中的接口来表达类的语义。 Executor表示任务的执行者，只有一个方法execute()，和Runnable的概念有点类似，但实际上是caller和callee的关系。 ExecutorService表示提供执行功能的服务方，所有的线程池都是ExecutorService的子类，这个接口定义了和executor相关的一些方法，如execute,submit等。 EventExecutorGroup表示基于事件的执行者集合，它继承了Iterable，说明他可以迭代，且每个元素都是EventExecutor。同时它继承了ScheduledExecutorService接口，表明他具有调度定时任务的功能。 EventExecutor表示任务的执行者。 EventLoopGroup表示EventLoop的集合，即线程的集合，这个接口的主要功能一个是遍历EventLoop，另一个是允许将channel注册到某个EventLoop上。 EventLoop负责处理各个channel的任务，一般一个EventLoop会对应多个channel。此接口本身并没有提供什么有趣的方法。 第一个NioEventLoop不妨探索一下Netty启动时第一个NioEventLoop是何时何地启动的，以典型的启动代码为例： 1EventLoopGroup bossGroup = new NioEventLoopGroup(1); 忽略重载的构造函数。 1234public NioEventLoopGroup(int nThreads, Executor executor, final SelectorProvider selectorProvider, final SelectStrategyFactory selectStrategyFactory) { super(nThreads, executor, selectorProvider, selectStrategyFactory, RejectedExecutionHandlers.reject());} 这里调用了父类的构造函数。 12345678 static { DEFAULT_EVENT_LOOP_THREADS = Math.max(1, SystemPropertyUtil.getInt( \"io.netty.eventLoopThreads\", NettyRuntime.availableProcessors() * 2)); } protected MultithreadEventLoopGroup(int nThreads, Executor executor, Object... args) { super(nThreads == 0 ? DEFAULT_EVENT_LOOP_THREADS : nThreads, executor, args);} NioEventLoopGroup是一个MultiThreadEventLoopGroup，即多线程事件循环组，最关键的是多线程的，这里既可以指多个线程，也可以看做是多个EventLoop组成的group。线程个数取决于最开始传入的整数，如果这个数是0，则会取一个默认值——处理器个数x2。 继续跟踪这个类的重载的及其父类的构造函数，直到构造函数中有明显的代码。 12345678910111213141516171819202122232425262728protected MultithreadEventExecutorGroup(int nThreads, Executor executor, EventExecutorChooserFactory chooserFactory, Object... args) { if (executor == null) { executor = new ThreadPerTaskExecutor(newDefaultThreadFactory()); } children = new EventExecutor[nThreads]; for (int i = 0; i < nThreads; i ++) { try { children[i] = newChild(executor, args); success = true; } finally { if (!success) for (int j = 0; j < i; j ++) children[j].shutdownGracefully(); } } chooser = chooserFactory.newChooser(children); final FutureListener terminationListener = ......; for (EventExecutor e: children) { e.terminationFuture().addListener(terminationListener); } readonlyChildren = Collections.unmodifiableSet(childrenSet);} 在NioEventLoopGroup初始化时，参数executor为null，于是会首先创建一个ThreadPerTaskExecutor（顾名思义，为每个任务取得一个线程来执行它的Executor，至于如何取得，是直接创建新线程还是复用线程，要看具体情况）。 ThreadPerTaskExecutor的execute方法如下： 12345private final ThreadFactory threadFactory;public void execute(Runnable command) { threadFactory.newThread(command).start();} 在这里，threadFactory是一个DefaultThreadFactory对象，它提供线程的方式由newThread方法决定： 123456789101112@Overridepublic Thread newThread(Runnable r) { Thread t = newThread(FastThreadLocalRunnable.wrap(r), prefix + nextId.incrementAndGet()); if (t.isDaemon() != daemon) { t.setDaemon(daemon); } if (t.getPriority() != priority) { t.setPriority(priority); } return t;} 显然，这里的ThreadPerTaskExecutor会将收到的每个task放到一个新创建的线程里运行。 回到构造函数，接下来程序开辟了一个数组用于存放所有的executor。EchoServer中nThread取1，自然就开辟长度为1的数组。 newChild在MultithreadEventExecutorGroup是一个抽象方法，具体如何创建子executor取决于其子类的实现。NioEventLoopGroup是这么实现它的： 12345@Overrideprotected EventLoop newChild(Executor executor, Object... args) throws Exception { return new NioEventLoop(this, executor, (SelectorProvider) args[0], ((SelectStrategyFactory) args[1]).newSelectStrategy(), (RejectedExecutionHandler) args[2]);} 直接new了一个NioEventLoop，并且把executor作为构造函数的参数传进了NioEventLoop。不过executor并不是由NioEventLoop使用，而是由其父类的父类——SingleThreadEventExecutor作为属性之一进行维护。SingleThreadEventExecutor作为executor的维护者，在它的代码中也只有一处地方用到了executor，就在doStartThread函数中。 12345678910111213141516private void doStartThread() { executor.execute(new Runnable() { @Override public void run() { thread = Thread.currentThread(); ...... try { SingleThreadEventExecutor.this.run(); } catch (Throwable t) { ...... } finally { ...... } } });} 而这个doStartThread方法也不是SingleThreadEventExecutor一诞生就起作用的，在SingleThreadEventExecutor的execute方法中调用了该方法。 1234567891011@Overridepublic void execute(Runnable task) { ...... addTask(task); if (!inEventLoop) { startThread(); if (isShutdown()) { ...... } }} 重新审视SingleThreadEventExecutor，他提供了一个execute方法，当有人调用了该对象的execute方法时，会把要运行的任务加入到一个任务队列中（通过addTask方法，这个方法不看我们都能猜到是什么意思的）。如果是第一次execute，就在修改队列之后通过SingleThreadEventExecutor自带的executor启动一个线程，这个线程执行一个抽象的run方法，在NioEventLoop中对该方法给出了实现。 这里遇到了三种executor——ThreadPerTaskExecutor和SingleThreadEventExecutor，前者给每个任务（也就是Runnable）分配一个线程来执行，后者则只使用一个线程（只做一次startThread，那肯定只有一个线程了），NioEventLoop则也在其整个生命周期中只使用一个线程，此外它通过轮训来从队列中取任务。 那么问题来了，SingleThreadEventExecutor为什么要用ThreadPerTaskExecutor启动线程呢，二者看似是平级的概念，为什么却采用了包含关系？ 会到负责启动多个SingleThreadEventExecutor的MultithreadEventExecutorGroup，接下来通过chooserFactory得到了一个chooser，这个对象的作用是根据特定的策略选择多个executor中的一个，即一个选择器。 这里不禁想起了在负载均衡中应该也有一个这样的chooser，只是不知道这样的chooser应该处于怎样的层级，来收集做出选择所需要的信息。 回到最初的问题——第一个NioEventLoop是何时启动的？到目前为止，第一个NioEventLoop已经诞生了，但他的execute方法还没有被任何人调用，因此他还没有启动自己的轮训线程。也就是说，应该是在另外的某一个犄角旮旯里，某行代码调用了execute，启动了第一个NioEventLoop的线程。 启动EchoServer的代码就那么多，不是初始化EventLoopGroup就是配置Bootstrap，最后就是bind。我认为在bind中可能性最大。 在前一篇文章中曾经分析过服务端启动的流程。这里稍微略过一些细节。 1234private ChannelFuture doBind(final SocketAddress localAddress) { final ChannelFuture regFuture = initAndRegister(); ......} 在AbstractBootstrap的doBind方法中，发现了ChannelFuture的踪迹。Future是一种返回异步结果的常见形式，出现了ChannelFuture说明其中肯定有某个函数是通过另一个线程执行操作，然后异步返回运行结果的。继续紧跟这个Future。 12345final ChannelFuture initAndRegister() { ...... ChannelFuture regFuture = config().group().register(channel); ......} 这里的group()返回了一个NioEventLoopGroup。我有一个疑问： 这个NioEvnetLoopGroup是Boss还是Worker呢？ 这里我用了一个土办法，在调试模式下加断点，看对象的ID。 答案了然了，这是一个Boss。继续跟踪。 1234567891011@Overridepublic ChannelFuture register(Channel channel) { return register(new DefaultChannelPromise(channel, this));}@Overridepublic ChannelFuture register(final ChannelPromise promise) { ObjectUtil.checkNotNull(promise, \"promise\"); promise.channel().unsafe().register(this, promise); return promise;} 在SingleThreadEventLoop中，Future逐渐浮出水面——他是作为一个Promise被创建出来的。但这里只是创建，并没有修改Future的状态，也没有在Future中填充异步运行的结果。 123456789101112131415@Overridepublic final void register(EventLoop eventLoop, final ChannelPromise promise) { ...... AbstractChannel.this.eventLoop = eventLoop; if (eventLoop.inEventLoop()) { register0(promise); } else { eventLoop.execute(new Runnable() { @Override public void run() { register0(promise); } }); }} 在AbstractChannel中出现了一个eventLoop，这个eventLoop是不是就是通过newChild得到的NioEventLoop呢？再次祭出比对ID大法。 这里我就不截图了，结果是这里的eventLoop是作为boss的NioEventLoopGroup制造出来的。 这里代码会进入else分支，即eventLoop.execute，那么第一个NioEventLoop就会将这个Runnable加到自己的任务队列中，接着通过ThreadPerTaskExecutor.execute运行自己的run方法，在run方法中轮训任务队列，运行Runnable。 eventLoop.inEventLoop我认为这个方法是一个十分关键的方法，虽然实现逻辑很简单，但他却很清晰地诠释了netty设计思路中对象和线程之间的关系，即executor和thread这两大概念之间的关系。 netty把这个方法的定义放在了AbstractEventExecutor类中。 1234@Overridepublic boolean inEventLoop() { return inEventLoop(Thread.currentThread());} 在AbstractEventExecutor中，inEventLoop表示在当前这个线程的EventLoop，由于EventLoop是单线程的（它是SingleThreadEventExecutor的子类），一个EventLoop对象run方法的代码只会运行在一个线程中。 1234@Overridepublic boolean inEventLoop(Thread thread) { return thread == this.thread;} 在SingleThreadEventExecutor中，inEventLoop表示传入参数的thread对象和与对象自身绑定的thread对象是同一个对象。 也就是说，一个SingleThreadEventLoop对象会和一个线程绑定，当想要通过这个对象执行某些方法的时候，可以先通过inEventLoop()方法判断，当前线程是不是和这个eventLoop绑定的线程，继而针对不同情况进行区分处理。如：如果当前线程不是eventLoop绑定的那个线程，就通过execute方法把任务加入到任务队列中；如果就是当前线程，那就直接运行代码。这样可以避免产生多线程操作同一个对象所带来的同步问题。 重新梳理Bootstrap结合NioEventLoop的初始化过程，重新梳理netty程序的启动流程。 创建NioEventLoopGroup，包含多个NioEventLoop，个数视情况而定。每个NioEventLoop的run方法还没有开始运行。 Bootstrap在bind的时候，通过反射创建NioServerSocketChannel对象。 初始化NioServerSocketChannel上面的attribute和option。 NioEventLoopGroup选出下一个NioEventLoop，让channel注册到它上面。 在选中的NioEventLoop的线程上，注册interestOps和attachment到selector上。 在channel的eventloop线程上，通过headContext绑定到指定端口上。 NioEventLoop的核心——loop的梳理，见下一篇。","link":"/2018/12/24/netty-nioeventloop/"},{"title":"Netty——NioEventLoop(2)","text":"Loop过程NioEventLoop最核心的就是处理事件循环的run方法。这个方法看起来不长，但实际上它承担了最重要的逻辑，并且对很多细节问题做了处理。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556@Overrideprotected void run() { for (;;) { try { switch (selectStrategy.calculateStrategy(selectNowSupplier, hasTasks())) { case SelectStrategy.CONTINUE: continue; case SelectStrategy.BUSY_WAIT: case SelectStrategy.SELECT: select(wakenUp.getAndSet(false)); if (wakenUp.get()) { selector.wakeup(); } // fall through default: } cancelledKeys = 0; needsToSelectAgain = false; final int ioRatio = this.ioRatio; if (ioRatio == 100) { try { processSelectedKeys(); } finally { // Ensure we always run tasks. runAllTasks(); } } else { final long ioStartTime = System.nanoTime(); try { processSelectedKeys(); } finally { // Ensure we always run tasks. final long ioTime = System.nanoTime() - ioStartTime; runAllTasks(ioTime * (100 - ioRatio) / ioRatio); } } } catch (Throwable t) { handleLoopException(t); } // Always handle shutdown even if the loop processing threw an exception. try { if (isShuttingDown()) { closeAll(); if (confirmShutdown()) { return; } } } catch (Throwable t) { handleLoopException(t); } }} 既然叫eventLoop，那代码的主体部分自然就是一个循环，在每一次循环迭代都根据某些状态做一些针对性工作。 首先是selectStrategy.calculateStrategy(selectNowSupplier, hasTasks())，selectStrategy只有一个实现类：DefaultSelectStrategy。 1234567891011@Overridepublic int calculateStrategy(IntSupplier selectSupplier, boolean hasTasks) throws Exception { return hasTasks ? selectSupplier.get() : SelectStrategy.SELECT;}......selectSupplier = new IntSupplier() { @Override public int get() throws Exception { return selectNow(); }}; 判断队列中有无任务，如果有，则返回selectNow的结果。 如果没有，就返回SELECT。 selectNow方法返回的是JDK的select的selectNow方法执行的结果，这个方法执行非阻塞的select，返回SelectionKey的个数，当然如果无事发生的话也会返回0。 selectStrategy中定义了三个常量：SELECT、CONTINUE、BUSY_WAIT，然而后两种并没有在哪里被使用到。可能是netty是打算先设计好结构，然后再慢慢填坑把。 select当strategy为SELECT的时候，涉及到NioEventLoop的select方法： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657private void select(boolean oldWakenUp) throws IOException { Selector selector = this.selector; int selectCnt = 0; long currentTimeNanos = System.nanoTime(); long selectDeadLineNanos = currentTimeNanos + delayNanos(currentTimeNanos); for (;;) { long timeoutMillis = (selectDeadLineNanos - currentTimeNanos + 500000L) / 1000000L; if (timeoutMillis = currentTimeNanos) { selectCnt = 1; } else if (SELECTOR_AUTO_REBUILD_THRESHOLD > 0 && selectCnt >= SELECTOR_AUTO_REBUILD_THRESHOLD) { rebuildSelector(); selector = this.selector; selector.selectNow(); selectCnt = 1; break; } currentTimeNanos = time; } ......}protected long delayNanos(long currentTimeNanos) { ScheduledFutureTask scheduledTask = peekScheduledTask(); if (scheduledTask == null) { return SCHEDULE_PURGE_INTERVAL; } return scheduledTask.delayNanos(currentTimeNanos);} select方法在循环中尝试根据时间限制来进行阻塞或者非阻塞的select操作。这里的时间限制就是selectDeadLineNanos，他根据下一个任务的时间限制给出这次阻塞select最迟必须结束的时间。如果没有下一个任务，就给出长达SCHEDULE_PURGE_INTERVAL的时间，当然在这段时间内如果有任务到来还是会按照任务的deadline时间做判断。 如果还有不到500ms就要到ddl了，就结束循环，不过如果还没有执行过阻塞select，为了不破坏select方法的语义，可以先selectNow一次。 如果任务队列里有任务了，尝试将这个NioEventLoop设置为唤醒，如果唤醒成功，selectNow一次并结束。 通过selector执行阻塞的select操作，可以阻塞到ddl的前500ms，即在ddl之前预留500ms，说不定要做别的事情。记录阻塞select次数。 如果阻塞select操作真的返回了几个事件，结束循环。 如果用户（就是除了对象本身之外的其他人）唤醒了它，结束循环。 如果任务队列有任务了，或者有调度任务了，结束循环。 如果线程被中断了，结束循环。 如果阻塞select次数超过了一个阈值，说明selector出bug了，重建selector，selectNow一次，结束循环。 在循环体开头的switch可以这么解释： 如果任务队列里有任务，就让selectorselectNow一次，非阻塞地看看有没有什么事件发生，不管事件是0个还是几个，继续执行switch下面的代码。 如果没有任务，就考虑执行一段时间阻塞的select操作。 结束了switch，该处理事件和执行任务了。这里netty又来骚操作了——ioRatio，这个变量代表两个过程——IO和运行任务分别所占时间比例。 123456789if (ioRatio == 100) { processSelectedKeys(); runAllTasks();} else { final long ioStartTime = System.nanoTime(); processSelectedKeys(); final long ioTime = System.nanoTime() - ioStartTime; runAllTasks(ioTime * (100 - ioRatio) / ioRatio);} 判断方法很简单，如果ioRatio是100，就运行所有的任务；如果不是100，比如说是20，那么记录下IO所话的时间ioTime，然后限定运行任务的时间为ioTime*(100-20)/20，也就是4倍ioTime。 processSelectedKeys既然select操作检测到了IO事件，那么就要处理对应的key，这个key和JavaNIO中是SelectionKey是一回事。 12345if (selectedKeys != null) { processSelectedKeysOptimized();} else { processSelectedKeysPlain(selector.selectedKeys());} netty提供了两种处理selectionKey的方法——plain和optimized。NioEventLoop对象维护了一个SelectedSelectionKeySet对象，这个对象管理了一个selectionKey的集合。至于为什么不用Java原生的Set容器，应该还是出于效率上的考虑（TreeSet和HashSet在某些场景下都不如数组来的快）。 SelectedSelectionKeySet使用一个数组来作为set的底层实现，初始长度为1024，add操作向数组索引为size的位置插入元素，如果长度不够就扩容一倍，没有缩容的情况，没有remove操作，提供迭代器访问形式。 先看processSelectedKeysOptimized到底是怎么optimize的。 123456789101112131415private void processSelectedKeysOptimized() { for (int i = 0; i < selectedKeys.size; ++i) { final SelectionKey k = selectedKeys.keys[i]; selectedKeys.keys[i] = null; final Object a = k.attachment(); if (a instanceof AbstractNioChannel) { processSelectedKey(k, (AbstractNioChannel) a); } else { processSelectedKey(k, (NioTask)task); } if (needsToSelectAgain) { ...... } }} 遍历set中的每一个selectionKey，取出attach在它上面的对象，有可能是AbstractNioChannel，也有可能是NioTask。 这里可以思考一下：SelectionKey上的attachment是什么时候加上去的？ 在AbstractNioChannel中的doRegister中： 1selectionKey = javaChannel().register(eventLoop().unwrappedSelector(), 0, this); AbstractNioChannel被attach到了SelectionKey上面。 123456789101112131415private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) { final AbstractNioChannel.NioUnsafe unsafe = ch.unsafe(); ...... int readyOps = k.readyOps(); if ((readyOps & SelectionKey.OP_CONNECT) != 0) { k.interestOps(k.interestOps() & ~SelectionKey.OP_CONNECT); unsafe.finishConnect(); } if ((readyOps & SelectionKey.OP_WRITE) != 0) { ch.unsafe().forceFlush(); } if ((readyOps & (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) { unsafe.read(); }} 思路很清晰，就是根据readyOps的不同的值，采取不同的行为。 processSelectedKeysPlain实际上类似，区别主要在于它使用Java自带的Set存放SelectionKey。 runAllTasksrunAllTasks有两种，带时间限的和不带时间限的。不带时间限则执行所有任务： 123456do { fetchedAll = fetchFromScheduledTaskQueue(); if (runAllTasksFrom(taskQueue)) { ranAtLeastOne = true; }} while (!fetchedAll); // keep on processing until we fetched all scheduled tasks. 首先从ScheduledTaskQueue中取出任务，存放到taskQueue中，直到所有ScheduledTaskQueue被取空，或者taskQueue已满。 然后逐个执行taskQueue中的所有任务，直到taskQueue为空。 1234567for (;;) { safeExecute(task); task = pollTaskFrom(taskQueue); if (task == null) { return true; }} 引用闪电侠博客中的一张图来概括loop的过程。 wakenUp参考：Netty原子wakeup作用分析，虽然这篇文章已经严重过期了。 在NioEventLoop对象中，有一个AtomicBoolean——wakenUp，在run方法及其调用的其他方法中多次见到了对该变量的判断及CAS（Compare and Set）操作，之前一直忽略了这个变量的作用。 Boolean that controls determines if a blocked Selector.select should break out of its selection process. In our case we use a timeout for the select method and the select method will block for that time unless waken up. 字面意思为：决定Selector.select阻塞调用是否要中断select过程的一个Boolean，即是否应该通过select.wakeUp()唤醒正在阻塞的select操作。 主要目的是防止selector.wakeUp()被重复调用，因为selector.wakeUp()操作的开销还是不小的。 123456@Overrideprotected void wakeup(boolean inEventLoop) { if (!inEventLoop && wakenUp.compareAndSet(false, true)) { selector.wakeup(); }} 这里有个问题：NioEventLoop是单线程的代码逻辑，怎么会出现在selector阻塞的同时唤醒他呢？我认为问题出在selector可能不是单线程独享的，即不是每一个NioEventLoop都有一个独立的selector。此外，通过NioEventLoop.execute执行Runnable是串行的，但调用NioEventLoop的其他方法时还是会出现多线程并发的情况。 在switch语句块的SELECT分支中，会首先设置wakenUp为false，相当于每次循环时都给wakenUp变量设个初始值。 12case SelectStrategy.SELECT: select(wakenUp.getAndSet(false)); 在select()方法中，有两处对wakenUp的值做了判断： 12345678910for{ ...... if (hasTasks() && wakenUp.compareAndSet(false, true)) { selector.selectNow(); selectCnt = 1; break; } ...... selector.select(timeoutMillis);} 123if (wakenUp.get()) { selector.wakeup();}","link":"/2019/01/05/netty-nioeventloop2/"},{"title":"Netty——Pipeline(1)","text":")Netty中使用Handler对数据包进行处理，每个Handler成为整个处理过程的一个阶段，几个Handler前后相连构成了一个处理数据包的流水线（Pipeline）。同一个Handler实现可以在多种不同的处理流程中发挥自身的局部作用，可复用。 Pipeline的继承结构： 第一个Pipeline同样的，还是先寻找第一个Pipeline是在什么地方创建并初始化的。从我对Netty的了解来看，应该是Channel和Pipline之间关联较大，可能会具有一对一的关系。要么是NioServerSocketChannel，要么是它的某个父类。最终在AbstractChannel中找到了它： 1private final DefaultChannelPipeline pipeline; 倒也是直接，声明类型就是DefaultChannelPipeline，没有声明为其父类，说明Pipeline可能没有较多的多态特性需要表现出来。 123456protected AbstractChannel(Channel parent) { this.parent = parent; id = newId(); unsafe = newUnsafe(); pipeline = newChannelPipeline();} 123protected DefaultChannelPipeline newChannelPipeline() { return new DefaultChannelPipeline(this);} 每个AbstractChannel对象都有一个属于自己的Pipeline，并且在它的构造函数中实例化pipeline对象。 12345678910protected DefaultChannelPipeline(Channel channel) { this.channel = channel; ...... tail = new TailContext(this); head = new HeadContext(this); head.next = tail; tail.prev = head;} 至此第一个Pipeline诞生，这个Pipeline中只有两个节点：head和tail。Pipeline中的节点以双向链表的形式相连。 第一次插入节点下面来探索第一次向该Pipeline中插入节点的场景。在netty启动的过程中，AbstractBootstrap的initAndRegister方法会调用ServerBootstrap中的init方法，该方法第一次向Pipeline中插入元素。 12345678910111213141516171819202122@Overridevoid init(Channel channel) throws Exception { ...... p.addLast(new ChannelInitializer() { @Override public void initChannel(final Channel ch) throws Exception { final ChannelPipeline pipeline = ch.pipeline(); ChannelHandler handler = config.handler(); if (handler != null) { pipeline.addLast(handler); } ch.eventLoop().execute(new Runnable() { @Override public void run() { pipeline.addLast(new ServerBootstrapAcceptor( ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); } }); } });} init方法的参数channel是刚刚通过反射创建出来的NioServerSocketChannel对象，该对象具有一个Pipeline对象，现在在这个pipeline的尾部插入一个ChannelInitializer，并且在将来的某个时候，会调用这个initializer的initChannel方法，执行重写的代码。 暂时不去理会重写的代码什么时候执行，先搞清楚addLast方法做了什么事情。在深入了几个重载函数之后，看到了addLast的真身： 12345678910111213141516171819202122232425262728293031323334@Overridepublic final ChannelPipeline addLast(EventExecutorGroup group, String name, ChannelHandler handler) { final AbstractChannelHandlerContext newCtx; synchronized (this) { checkMultiplicity(handler); newCtx = newContext(group, filterName(name, handler), handler); addLast0(newCtx); // If the registered is false it means that the channel was not registered on an eventloop yet. // In this case we add the context to the pipeline and add a task that will call // ChannelHandler.handlerAdded(...) once the channel is registered. if (!registered) { newCtx.setAddPending(); callHandlerCallbackLater(newCtx, true); return this; } EventExecutor executor = newCtx.executor(); if (!executor.inEventLoop()) { newCtx.setAddPending(); executor.execute(new Runnable() { @Override public void run() { callHandlerAdded0(newCtx); } }); return this; } } callHandlerAdded0(newCtx); return this;} 1234567private static void checkMultiplicity(ChannelHandler handler) { ChannelHandlerAdapter h = (ChannelHandlerAdapter) handler; if (!h.isSharable() && h.added) { throw new ChannelPipelineException(); } h.added = true;} 123private AbstractChannelHandlerContext newContext(EventExecutorGroup group, String name, ChannelHandler handler) { return new DefaultChannelHandlerContext(this, childExecutor(group), name, handler);} addLast并不仅仅是将新的handler插入到链表中，还做了一些其他的事情： checkMultiplicity，检查该handler对象有没有被插入过，不管是该pipeline还是其他的pipeline。在netty中，只有被@Sharable注解的handler才可以被复用到pipeline中，否则一个handler对象只能出现在一处。这么做是为了保证状态变量的线程安全。 newContext，用一个ChannelHandlerContext对象来包裹这个handler。 addLast0，在链表中插入这个ChannelHandlerContext。 检查pipeline有没有完成到某个NioEventloop的注册，即channel有没有注册到NioEventloop。 3.1. 如果没有注册，那么handlerAdded的回调函数是不能立刻调用的，因为还没有eventloop线程实体与之绑定——在netty中各种事件的回调函数都是在NioEventloop中调用的，在以后还会遇到很多体现这个特征的场景。 3.2. 如果已经注册过了，在eventloop中调用callHandlerAdded0。 这里遇到了一个类DefaultChannelHandlerContext，这个类的地位十分重要，承担了维系pipeline和handler的工作。 12345678DefaultChannelHandlerContext( DefaultChannelPipeline pipeline, EventExecutor executor, String name, ChannelHandler handler) { super(pipeline, executor, name, isInbound(handler), isOutbound(handler)); if (handler == null) { throw new NullPointerException(\"handler\"); } this.handler = handler;} 12345678910AbstractChannelHandlerContext(DefaultChannelPipeline pipeline, EventExecutor executor, String name, boolean inbound, boolean outbound) { this.name = ObjectUtil.checkNotNull(name, \"name\"); this.pipeline = pipeline; this.executor = executor; this.inbound = inbound; this.outbound = outbound; // Its ordered if its driven by the EventLoop or the given Executor is an instanceof OrderedEventExecutor. ordered = executor == null || executor instanceof OrderedEventExecutor;} AbstractChannelHandlerContext维护了pipeline对象，表明他是属于这个pipeline的一个节点，以后若是有需要可以直接通过这个pipeline成员调用其方法。netty中的handler可以分为两类：inbound和outbound（当然有的handler可以同时具有这两种特性），为了区分包含的handler的方向，AbstractChannelHandlerContext使用了两个boolean，由于之前括号里的情况存在，显然一个boolean是不够的。此外，AbstractChannelHandlerContext还有一个executor，这个executor是pipeline通过childExecutor方法分配给他的，这里先不谈分配方式。 再来看addLast0: 1234567private void addLast0(AbstractChannelHandlerContext newCtx) { AbstractChannelHandlerContext prev = tail.prev; newCtx.prev = prev; newCtx.next = tail; prev.next = newCtx; tail.prev = newCtx;} 有数据结构基础的人都能够很快理解这段代码，对于没有学过数据结构的人来说双向链表的插入操作也不是很难。 停！现在来给pipeline拍一张快照： 现在pipeline中有三个节点，其中HeadContext和TailContext都是没有handler的，中间的HandlerContext包含一个ChannelInitializer。 在链表中插入了新的handlerContext之后，需要决定是否需要调用它的handlerAdded方法。如果暂时不调用回调方法的话： 123456789101112private void callHandlerCallbackLater(AbstractChannelHandlerContext ctx, boolean added) { PendingHandlerCallback task = added ? new PendingHandlerAddedTask(ctx) : new PendingHandlerRemovedTask(ctx); PendingHandlerCallback pending = pendingHandlerCallbackHead; if (pending == null) { pendingHandlerCallbackHead = task; } else { while (pending.next != null) { pending = pending.next; } pending.next = task; }} handlerContext会被包装成一个PendingHandlerCallback，插入到由pipeline维护的PendingHandlerCallback的链表的末尾。 如果我们在pipeline注册到eventloop之前，在pipeline中插入多个handler，那么这些handler的handlerAdded方法都会被包装成pendingTask，接续到链表后。同样的，如果是在注册之前删除某个handler，它的handlerRemoved也会被包装，并插入链表。 我认为这么做的目的是保证在注册前发生的add和remove操作都会导致其对应事件的回调函数能够被正确的调用，这里的正确指的是按照正确的顺序在eventloop中调用。不能因为某个handler先被add然后被remove就认为它从没有出现过。 当然了，也有可能注册得比较早，那么handlerAdded方法可以立刻执行，没有必要等什么。 123456789101112EventExecutor executor = newCtx.executor();if (!executor.inEventLoop()) { newCtx.setAddPending(); executor.execute(new Runnable() { @Override public void run() { callHandlerAdded0(newCtx); } }); return this;}callHandlerAdded0(newCtx); 12345private void callHandlerAdded0(final AbstractChannelHandlerContext ctx) { ...... ctx.handler().handlerAdded(ctx); ......} 现在，把时间轴向前推进，直接走到initChannel调用之前： 12345678910private void register0(ChannelPromise promise) { try { doRegister(); pipeline.invokeHandlerAddedIfNeeded(); safeSetSuccess(promise); ...... } catch (Throwable t) { ...... }} invokeHandlerAddedIfNeeded就是专门用来触发之前pending的handlerAdded事件的。 1234567final void invokeHandlerAddedIfNeeded() { assert channel.eventLoop().inEventLoop(); if (firstRegistration) { firstRegistration = false; callHandlerAddedForAllHandlers(); }} callHandlerAddedForAllHandlers也的确只会被调用一次，就是在这里。在其他地方，handler的added事件不会延迟执行，是即时的。 12345678private void callHandlerAddedForAllHandlers() { ...... PendingHandlerCallback task = pendingHandlerCallbackHead; while (task != null) { task.execute(); task = task.next; }} 沿着pendingTask的链表，逐个执行handlerAdded和handlerRemoved回调事件。当然这里的execute方法会保证在当前的eventloop中执行handlerAdded或handlerRemoved回调函数的代码。 也就是说，为了让handler的added和removed事件不遗漏、不乱序、串行地得到执行，netty采用将added和removed事件包装成pendingTask的方式，在pipeline注册到某个eventloop后，再在一个方法中去逐个回调pendingTask。 回到ChannelInitializer中，它的handlerAdded实现如下： 123456@Overridepublic void handlerAdded(ChannelHandlerContext ctx) throws Exception { if (ctx.channel().isRegistered()) { initChannel(ctx); }} ChannelInitializer作为一种特殊的handler，主要用于在channel启动时做一些初始化工作，为了达成这样的效果，它的做法是在它自身被插入到pipeline之后立即做initChannel操作。 12345678910private boolean initChannel(ChannelHandlerContext ctx) throws Exception { try { initChannel((C) ctx.channel()); // 调用重写的initChannel方法 } catch (Throwable cause) { ...... } finally { remove(ctx); // 把自己从pipeline中删掉 } }} 在ServerBootstrap中见到的ChannelInitializer匿名内部类的initChannel做了这些事情： 12345678910111213final ChannelPipeline pipeline = ch.pipeline();ChannelHandler handler = config.handler();if (handler != null) { pipeline.addLast(handler);}ch.eventLoop().execute(new Runnable() { @Override public void run() { pipeline.addLast(new ServerBootstrapAcceptor( ch, currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); }}); 从config()中取出一个handler，添加到pipeline的末尾。从调试时的结果来看，这是一个LoggingHandler（估计就是专门负责打日志的）。 异步地在pipeline末尾插入一个ServerBootstrapAcceptor。 再给pipeline拍一张快照：","link":"/2019/01/08/netty-pipeline/"},{"title":"Netty——下载、编译、调试源码","text":"Intellij，启动！参考资料：Netty分享之动态生成重复性的代码 完整过程我选择直接通过Intellij来clone Netty在Github上的源码，这样就不需要再另外起一个控制台了。Github地址如下： https://github.com/netty/netty.git 好了，现在就可以运行example中的程序了，直接点Intellij的运行就行。","link":"/2018/11/15/netty/"},{"title":"Orange'S——学习笔记","text":"由于时间关系，不能够跟着书的进度自己一点点动手写出一个操作系统，因此想着先把书的内容过一遍，只要了解到OS底层运作思路即可。Orang’S下文简称为OS。 启动和加载 ===== OS的启动和加载主要分为三个阶段。 BIOS检查外部存储设备并加载boot模块。 boot模块从外部存储设备搜索并加载loader模块。 loader模块从外部存储设备搜索并加载kernel模块。运行kernel.bin，正式进入系统。 由于BIOS对boot模块的大小有限制，只能有512字节，因此启动过程中的大部分操作都在loader中进行。 BootBoot是最先被读取并被加载进入内存的部分，根据约定，Boot不能超过512字节，且最后两个字节作为引导记录的标志，必须为0xaa55，这样BIOS才能识别的到。在OS中，boot、loader、kernel都是存放在floppy中的，为了方便写入文件，并且降低搜索文件和加载文件的复杂度，loader和kernel都是作为普通的文件写入floppy，而floppy被预格式化为了FAT12文件系统，这样boot只要按照读写FAT12文件系统的逻辑运行就可以读到loader和kernel。 Boot流程： 软驱复位 在根目录区，逐个读取条目，比对文件名，直到找到Loader.bin所在的扇区号（开始簇号）。 在FAT表1，根据开始扇区号，读取该扇区在FAT表中的条目（簇号），读取该簇号对应扇区的文件。 根据簇号读取该簇号（扇区号）在FAT表中的条目（另一个簇号），重复读取这样的链式结构，直到某个条目的簇号为结束标志符。 跳转到Loader的入口代码处，交出控制权。 FAT12文件系统FAT12文件系统将存储空间分为四块：引导区，FAT表1和FAT表2，根目录区，用户数据区。 在FAT文件表中，每个条目占12位（一个半字节），条目所在位置或者说条目在表中的索引为该扇区簇号，条目内容为该文件下一部分簇号。 LoaderLoader的功能如下： 通过BIOS的15号中断（SMAP）读取总的内存空间大小。 以和Boot相同的方式，在floppy中定位到kernel.bin所在的位置，并加载到内存中。 启动保护模式的一系列操作：关中断，打开A20地址线，修改cr0寄存器的值，跳转到32位代码段。从此正式进入保护模式。 根据之前读到的内存空间大小，初始化足够的页目录和页表。 由于kernel.bin是ELF格式的，因此要重整kernel.bin在内存中的数据。 将控制权交给内核。 Loader将内存组织为下面这样子： ELF文件格式ELF是一种文件格式约定。对象文件有三类： 可重定位文件。也就是.o文件。一个或多个.o文件可以被归档为一个静态库文件。 可执行文件。 可被共享文件。也就是.so动态库文件。 上图中左侧为链接视图，右侧为运行视图。链接过程中以section为单位，运行过程中以segment为单位。整个文件可以分为4部分： ELF头部 program header table（程序头部表） sections\\segments section header table（节头部表） ELF头部结构如下： typedef struct { unsigned char e_ident[EI_NIDENT]; //一个数组，数组内容包括：魔数、32位/64位、大端/小端、文件版本等。 ELF32_Half e_type; //文件类型，可执行文件或者可重定向文件或者可共享文件 ELF32_Half e_machine; //架构 ELF32_Word e_version; // ELF32__Addr e_entry; //可执行程序入口点地址 ELF32_Off e_phoff; //程序头部表地址 ELF32_Off e_shoff; //节头部表地址 ELF32_Word e_flags; ELF32_Half e_ehsize; //ELF头部大小 ELF32_Half e_phentsize; //程序头部表单个表项大小 ELF32_Half e_phnum; //程序头部表表项个数 ELF32_Half e_shentsize; //节头部表单个表项大小 ELF32_Half e_shnum; //节头部表表项个数 ELF32_Half e_shstrndx; //名称表的位置}Elf32_Ehdr; 程序头部表结构如下： typedef struct{ Elf32_Word p_type; Elf32_Off p_offset; //段相对于文件的偏移地址 Elf32_Addr p_vaddr; //段在内存中的虚拟地址 Elf32_Addr p_paddr; //段的物理地址 Elf32_Word p_filesz; //段在文件中的长度 Elf32_Word p_memsz; //段在内存中的长度 Elf32_Word p_flage; Elf32_Word p_align; //字节对齐} Elf32_phdr; 节头部表的结构如下： typedef struct{ Elf32_Word sh_name; Elf32_Word sh_type; //节区类型：程序定义、符号表、字符串表、重定位表等 Elf32_Word sh_flags; Elf32_Addr sh_addr; Elf32_Off sh_offset; Elf32_Word sh_size; Elf32_Word sh_link; Elf32_Word sh_info; Elf32_Word sh_addralign; Elf32_Word sh_entsize;}Elf32_Shdr; 常见系统节区： 字符串表。类似于JVM字节码文件的常量表。只负责定义常量，其他地方如果要用到某个字符串常量只要通过索引即可。一般会有一个或者多个字符串表。 符号表。符号表格式相对复杂，每个符号表项约定了该符号的名字、值（某个具体值或者函数地址）、占用大小、可见性、类型（函数、变量、外部文件、某个节区） 代码段（.text）。符号表中会有字段指向代码段中的某个位置，来表示函数的代码。 全局偏移表（.got）。为了使得对全局变量的有效引用不依赖于实际的地址空间，因此为全局符号（静态函数或者变量）提供一个偏移量表。 过程链接表（.plt）。 哈希表。 数据段（.data .bss .rodata）。 段页式存储逻辑地址经过分段转换变为线性地址，线性地址经过分页转换变为物理地址。段式存储是从逻辑上对内存的区域进行划分，以求对区域内内存的保护和管理，页式存储是从空间上对内存空间进行等分映射，目的在于控制内存分配粒度（外碎片）、虚拟地址空间。 分段x86段表也成描述符表，有全局（GDT）和局部（LDT）两种描述符表，全局描述符表由内核管理，局部描述符表由进程管理。描述符表能够介入实模式中“段基址+段内偏移”的直接寻址模式，是一个中间步骤。 x86的段表项的结构如下： 每个表项占8个字节，其中占字节位数最多的是base和limit，由于某些历史上的原因，base和limit各自都不是连续地存放在表项中的。 段基址共32位，4G，表示段的起点位置。 段界限共20位，表示段的长度，单位取决于G字段（1Byte或者4KByte）。 S为表示系统段、数据段。 DPL表示特权级，0、1、2、3。 P表示该段是否在内存中，和页表项的P位类似。 AVL位保留字段。 DB表示段的默认操作尺寸：16bit或者32bit。 要引用某个段表项时，只需要指定段表项相对于表头的索引，即索引加1对应内存地址加8。索引不是数字，而是段选择子（selector）。段选择子的格式如下： 一个selector占16位。 高13位用于索引段表项。Index加1的话整个选择子的值增加8，正好对应每个段表项8个字节的特性。 TI位用于区分是GDT还是LDT。 RPL表示特权级，0、1、2、3。 真正使用时，将选择子放入段寄存器中（CS、DS、SS等），硬件就会自动通过选择子找到对应的段表项，取得段表项的基址和界限，确认逻辑地址的段内偏移在界限内之后，通过基址+段内偏移得到线性地址。 系统通过GDTR寄存器来找到GDT的位置。 LDT相当于GDT的后级段表。 LDTR寄存器存放某个选择子，系统会根据该选择子从GDT中选择某个段，然后将该段的基址和界限内的内存空间视为一张LDT，通过逻辑地址SELECTOR:OFFSET的SELECTOR选择段表项，通过OFFSET得到具体的线性地址。 使用LDT的好处是，只需要切换LDTR，就可以让相同的逻辑地址指向不同的线性地址。 分页OS中使用页目录——页表组成的二级页表结构来将线性地址转换为物理地址。 页目录项和页表项的结构大致相同： Avail保留。 G位表示全局页，全局页表示当CR3加载时，该页目录项或者页表项也是有效的。 D位表示脏位。 A位表示是否被访问过。 PCD位表示单个页或者页表是否可以被缓冲。 U/S位表示特权级，与CPL（CR0中的WP位）一起共同控制读写权限。 R/W位表示读写权限，与CPL（CR0中的WP位）一起控制读写权限。 P位表示是否存在于内存中。如果不存在，会产生页错误。 注意：多个页表并不需要连续存放，页表可以存在于内存中的任何位置。 系统通过CR3寄存器获得页目录基址。 TSS（任务状态段）TSS可以看做一张表。 TSS的用户： 保存ring0、ring1、ring2的栈顶和堆栈段选择子。 通过JMP+TSS段选择子，可以一次性切换一堆寄存器（先保存当前寄存器值到TR指定的TSS，再用新的TSS中的寄存器值替换寄旧值）。GDT中可以存放多个TSS，引用TSS同样需要使用selector。 据说设计TSS的初衷就是为了方便任务切换，但由于效率太低，因此已经被启弃用了。 中断中断本质上是一种事件处理机制。 OS保护模式下的中断的处理机制如下： 由外部设备或者CPU产生触发中断，硬件会根据中断号在中断描述符表（IDT）中选择一个表项，IDT中存放的表项成为门描述符，有3种：中断门、陷阱门、任务门。 取出门描述符后，会进行一些检查：limit检查、权限检查。 从门描述符中读取段表选择子，判断是LDT还是GDT（依靠TI位）。 根据选择子从段表中读取代码段描述符，会进行一些检查：limit检查，权限检查。 段描述符的base+门描述符的offset得到中断处理程序的地址。 x86提供了一些预制的中断和中断向量号。这些预制的中断能够保证在该事件发生时，硬件会根据中断向量号去调取相应的门描述符，但门描述符的内容需要操作系统自己去设置。还可以使用8259来处理新的外部中断。 x86在进入中断处理程序之前，会把eflags、cs、eip、errorcode压栈，便于中断处理程序结束之后回到中断现场。 保护模式 进程管理","link":"/2018/08/18/oranges/"},{"title":"OS——The Abstraction: The Process","text":"抽象的概念进程（Process），指运行着的程序。程序指静静躺在在外存中的数据和代码，是静态概念，但是进程是动态的概念，他反应了程序在运行的时候的功能和性质。 现代典型的操作系统都是同时运行数十个至数百个进程的，使用分时（Time Sharing）的技术来实现看似每个进程都能独占一个CPU的效果，当然，分时是要付出性能上的代价的。 进程的组成部分： 内存：代码和数据存储的地方。 寄存器：存储运算操作或输入输出操作中间结果以及状态信息的地方，。比较重要的有PC（Program Counter），SP（Stack Pointer）等。 IO设备：如文件描述符（File Descriptor）。 进程的创建大致可以概况为以下几个步骤： 把代码和数据从外存载入内存（地址空间）中。如今OS读取程序数据，载入内存的操作可以以惰性的方式执行，即要多少读取多少，当然这需要借助置换（swaping）和分页（paging）机制。 分配栈空间，并初始化。 分配堆空间，并初始化。 初始化一些IO设备信息，如3个文件描述符（fd）：stdout，stdin，stderr。 最后，跳转到main子程序，正式开始运行。 进程的状态 运行（running）：此时进程拥有CPU资源，能够逐行执行指令、进行运算、发起IO请求。 就绪（ready）：此时进程随时可以开始或继续运行，但是OS选择忽视了他，而是去垂青其他进程了。 阻塞（blocked）：正在运行的进程发起了IO操作后，IO设备开始发送或接收数据，此时该进程处于等待IO操作完成的状态，也叫阻塞状态。此时CPU一般会分配给其他进程使用。 正在运行的进程可能会被OS调出而进入就绪状态，就绪的进程可能会被OS调入，进入运行状态。当进程在运行过程中发起IO操作，会变为阻塞状态，好像被冻住了一样。处于阻塞状态的进程无法继续运行（数据没准备好），直到IO操作完成后，变为就绪状态。 数据结构可以想象，OS通过一个列表来记录每个进程的状态信息。记录一个进程相关状态信息的数据结构叫做PCB（Process Control Block）。因为常见的OS一般是用C语言编写的，因此这样的数据结构也常常用C语言来描述。","link":"/2018/04/25/os-the-abstraction-the-process/"},{"title":"Spring——IOC流程概述","text":"tiny-spring这是一个简化版的spring框架，模仿spring的思路实现了IOC和AOP的功能，是github上的一位开发者编写的。 https://github.com/Dokyme/tiny-spring 其实spring IOC的步骤并不复杂，和把大象装进冰箱的过程（三步）是基本一致的，也可以说成是两步，根据我的理解： 读取bean的xml配置文件，并解析每个bean的定义，及其属性。 延迟bean的实例化，并根据依赖关系进行组装。 所用到的类文件如下： Resource和ResourceLoaderResouce代表一种资源，在SpringWeb项目中最常见的就是XML文件，spring内部通过Resource接口定义了资源实体所需要提供的数据。在tiny中，Resource接口只有一个函数，那就是得到一个InputStream。 Resouce不是凭空产生的，不是由用户new出来的，而是通过ResourceLoader制造出来的。ResouceLoader接口定义了制造Resouce的方法。在tiny中，ResourceLoader接口也只有一个函数，是根据一个字符串地址制造出一个Resouce实例。 Resouce和ResourceLoader接口都需要具体的实现类去实现各自的逻辑，tiny中只实现了Url资源的Resouce和ResourceLoader，即根据字符串地址得到Url，再建立Url连接，得到InputStream。 BeanDefinition和BeanDefinitionReader在spring中，万物皆为bean，bean指的是一个提供了默认构造函数、setter和getter方法的类，这个类可以是数据实体类，也可以是封装了一些逻辑操作的类。spring体系中使用xml文件来定义bean，一个bean节点通常包含名称（id），具体实现类，属性列表等。 BeanDefinition封装了xml中的bean节点，因此spring读取并解析xml文件后得到的就是一个BeanDefinition的列表。 读取xml并解析的过程则交给了BeanDefinitionReader接口，在tiny中BeanDefinitionReader接口提供了一个函数（loadBeanDefinition），其直接实现类AbstractBeanDefinition没有实现这个方法，但是给出了用于缓存bean的数据载体——一个Map，还有一个ResouceLoader，即该抽象类约定了其子类必须将多个bean保存在这个map中，但如何读取并解析，根据怎样的顺序，是实时还是延迟，该抽象类并没有做出规定。 tiny中XmlBeanDefinitionReader集成了AbstractBeanDefinition类，并实现了BeanDefinitionReader接口所留下的load方法。该类借用了AbstractBeanDefinition所维护的ResouceLoader获得Resource的InputStream，进而读取该InputStream，默认使用XML解析器来解析这个文件。解析完成之后遍历得到的所有XML节点，针对每个节点，Reader取出他们的id，类名，并遍历该节点内部的Property列表，针对每个Property，取出name和value，或者ref。最后将该节点的所有信息填入BeanDefinition对象中。 BeanFactory如果说BeanDefinitionReader是做静态处理的话，BeanFactory所完成了bean组装就是动态的。BeanFactory负责对bean进行组装，包括组装的时机、方式、顺序的控制。在tiny中，BeanFactory是一个接口，这个接口只提供了一个方法getBean，根据名字返回一个bean实例。该接口的直接实现类AbstractBeanFactory没有完全实现getBean方法，而是给出了返回bean的逻辑：首先从map中找到对应bean的BeanDefinition，由于BeanDefinition是在最开始就初始化好的，所以如果找不到BeanDefinition，那肯定是异常情况。BeanDefinition对象除了维护bean的信息之外，还会维护该bean的实例，如果这个实例没有被初始化，name就根据该BeanDefinition创建一个bean实例并初始化；如果已经被初始化过了，那就直接返回。听起来有点像单例模式，实际上tiny中的bean都是singleton的，因此全程只维护一个实例。 BeanFactory根据BeanDefinition创建实例的过程很简单，只要从BeanDefinition中拿到类名，直接实例化即可。组装bean的过程稍微复杂一些，BeanDefinition会维护该bean的Property列表，如果该Property是基本类型的字面量，那就直接进行属性的赋值，如果是一个引用对象，并且该引用对象是另外一个bean的话，就通过getBean方法先得到那个bean，然后再赋值，当然得到该bean的过程可能也包含着创建和初始化的步骤。 需要注意的是，对于bean属性的赋值都是通过反射获取对应的setter方法进行处理的，而不是直接赋值或者通过反射修改属性的可见性后赋值。 在tiny中没有对循环依赖的情况进行判断和处理。 ApplicationContextApplicationContext接口继承了BeanFactory接口，因为其本质上也是负责进行bean组装的容器。在tiny中AbstractApplicationContext类实现了该接口，并持有一个BeanFactory对象，将getBean方法委托给BeanFactory，而自己负责将bean配置文件的加载、解析、注册过程结合起来，变为一个refresh函数。 ClassPathXmlApplicationContext类继承了AbstractApplicationContext，直接在构造函数里调用refresh函数，即在构造的时候进行一系列初始化操作，同时其规定了加载bean配置文件是通过xmlBeanDefinitionReader进行处理，并且逐个注册解析到的BeanDefinition。 BeanPostProcessor这个接口提供了两个方法来拓展bean初始化前后的操作，AbstractApplicationContext会将所有BeanPostProcessor类型的bean注册到beanFactory中去，当beanFactory初始化某个bean时，会调用该Processor的两个回调函数。 Spring以ClassPathXmlAppliationContext的初始化为例。 public class ClassPathXml { public static void main(String[] args) { ApplicationContext applicationContext = new ClassPathXmlApplicationContext(“classpath:config.xml”); SimpleBean bean = applicationContext.getBean(SimpleBean.class); bean.hello(); }} public class SimpleBean { public void hello() { System.out.println(“SimpleBean Hello”); }}","link":"/2018/08/07/spring-ioc/"},{"title":"TCP/IP协议 TCP Connection Mnagement","text":"因为TCP是一个面向连接的协议，所以建立释放连接的过程是很重要的。 连接的建立和释放 三次握手 c发送一个syn被置位的报文给s，表示想要建立连接，当然这个报文包含了c的ISN。 s接收到了syn报文，需要确认收到了syn和c的ISN，所以回复一个syn，同时ack置位，确认号为c的ISN+1，当然这个报文包含了s自己的ISN。 c接收到了s回复的syn报文，需要确认收到了s发来的syn和s的ISN，于是c回复确认，确认号为s的ISN+1。 至于为什么要三次握手，一次两次不行呢。我是这样理解的：1次握手相当于承认传输信道的绝对可靠，就是说c发送的syn报文s绝对能收到；2次握手的话最多只能确保一个方向上的传输成功了，即c知道s能收到自己的消息，但s不知道c能不能收到自己的消息，因为s发送的syn报文还没有得到c的确认回复呢。逼乎上也有人说这是不可靠信道上要想建立双向通信必须要三次，不是说TCP/IP比较特殊要3次。 四次分手 c的应用程序要求close，c发送一个fin报文表示自己已经没有数据要发送了，等待s的确认。 s收到了c的fin报文，就好像c说自己想睡觉了，s也不能强求他吧。s只好确认c的fin报文。 此时c已经没有数据要发送了，但是s说不定还有，所以s可以继续想c发送数据，而且此时的c还可以接收数据并且确认s的报文，即发送ack报文。此时TCP连接处于半开状态。 s也没有数据要发送了，于是s也发送了fin报文，这个fin报文的顺序号可能比2中的确认报文的顺序号大一些，确认号应该和2中的确认号是一致的。 c确认s的fin报文，回复ack，TCP连接的另一半也关闭了。至此TCP连接彻底关闭。 注意：第2个报文和第3个报文有时可以合并为同一个报文。 但是为什么书上的四次分手中，第4个ack报文的顺序号还比第3个报文的确认号小1，我还没搞清楚。 半关闭TCP的四次分手其实可以看做两个半关闭，一个fin报文关闭一个方向，关闭一半的连接。socket是支持半关闭的（shutdown()），close（）是全关闭。 初始顺序号初始顺序号的选择也是挺重要的，因为这关系到很多方面的问题，比如在上次连接中发出的一个迷失在网络中的报文，在这次连接中突然出现了，然后他的顺序号正好合适，于是他就上位了，这样的情况。再比如有人能够伪造合适的顺序号来实现欺骗。实际上ISN的生成好想是取时间和随机数相结合，既能随时间增长，又能有一定的随机性，不容易让人猜对。 建立连接时超时了建立连接时，发出了syn报文之后c不会无限制地等待s的ack，通常会隔一个特定的时间发送一个syn，当次数超出限制之后，则说明timeout了。当然了这两个参数都是可以改的。 TCP Options TCP的options字段包含了一系列的可选项，除了EOL和NOP这两个选项是只占1个字节之外，其余的选项至少占2个字节：1个字节的kind和1个字节的Length，每种选项的长度都是不定的。 MSS最大报文长度在syn报文中使用的选项。规定了自己能够接收的最长的报文长度，不包括TCP头部和IP头部，仅仅是data的长度。 典型的长度1460，加上TCP和IP的报文头正好1500个字节。 SACK选择确认在syn报文中需要明确SACK permitted，然后在后续的报文中就可以使用选择确认。当没有按照顺序收到报文时（比如收到了1，2，然后立刻收到了5），会标注选择确认。一个SACK块包含两个4字节的数（1个上界顺序号1个下界顺序号）。SACK能够显著地提高效率，和累积确认相比显得更加有效。 窗口大小放大只能在syn报文中使用的选项。窗口大小缩放因子，能够把16bit的窗口大小放大到16bit-30bit的大小。假设放大因子是s的话，那么实际的窗口大小就是header的窗口大小左移s位。 使用放大因子选项的要求还是挺严格的：c发送的syn报文中包含window scale factor，s的syn+ack报文也必须要明确这一个选项，但是大小可以不同。这样就有两个scale。 不太好理解的是window advertisement的时候，假设有一个因子s用来发送窗口大小，有一个因子r来接收窗口大小，那么一旦收到了window advertisement，那么自己的window就要调整成header的window大小左移r位，每次发送window advertisement时，window header中的size就应该是自己实际的window size右移s位。 window scale shift count是由TCP自动选择的，缩放的程度取决于系统buffer的大小，当然这个是可以调的。 Timestamp时间戳和PAWS常用的options，而且不是只能在syn报文中使用。可以用来粗略地估计RTT，进而动态调整超时重传的时间（这个以后还会细讲）。一个时间戳4字节。发送者把自己这边的时间放在TSval，接收者收到后TSval填写自己的时间戳，TSecr抄一遍发送者的时间戳，这样每次都是抄一半写一半。 时间戳选项一般10个字节（4+4+1+1）。 接收者并不在乎发送者的时间戳的单位是什么，时间戳对于一方的意义仅仅在于能够比较大小，因此时间戳只要单调递增就可以了。 Protection Against Wrapped Sequence Number（PAWS），依靠时间戳的单调递增性，可以用来判断两个相同的顺序号的报文的先后。因为顺序号是循环递增的，因此当窗口极大时还是有可能出现前一个周期内有一个包丢了，后一个周期的相同顺序号的包上位了的现象。如下图所示： 问题：万一时间戳也循环递增，顺序号和时间戳同时重复了呢。 UTOUser Time Out，这个选项刚出不久，比较新，没有被广泛的使用。我也没看懂。。。 TCP-AOAuthentication Option，用来认证加密的，需要事先协商一些加密函数和key，然后在options中附加上traffic key，由这个key发送者和接受者都能够产生正确的解密key。没有被广泛的使用。 不过这是一种对于TCP sproofing attack强有力的反制措施。 实例分析1随便发了个HTTP请求，把数据包抓了一下，主要就是看一看三次握手的部分。 TCP状态转换一个TCP连接的每一端的状态都会在下面这张图中转换。有的转换是由于收到或者发送了包含状态位的报文而产生的，有的转换则是定时器超时导致。 其实CLOSED并不是一个官方的状态，但是这里把CLOSED作为起点能够更加清晰明了地介绍TCP连接的状态转换关系。 图中有2点没有写出来的需要额外注明： CLOSED->SYN_SENT在TCP协议中是合法的，但是Berkeley SOcket中没有实现，而且也很少见到。 从SYN_RCVD->LISTEN的转换，要求必须要该SYN_RCVD状态是由LISTEN转换而来才行，这时如果收到RST报文而不是ACK，那么又会回到LISTEN。这种转换在同时打开（simultaneous open）得情况下是不行的。 把三次握手和四次挥手的图与状态相结合，如下。 TIME_WAIT状态（2 MSL 等待）由于IP报文头部的TTL和跳数限制，一个数据包在网络中的生存时间是有一个最大值的，当然这个最大值是可以配置的。 一个主动关闭的TCP在TIME_WAIT状态下必须等待两个MSL（Maximum Segment Lifetime）的时间。原因是： 1. 他将在这2MSL的时间内等待被动关闭放有么有再次发送FIN，如果收到了FIN，说明他发的ACK没有被对方接收到，那么他将会重新发送最终的ACK。 2. 2MSL中，显然连接还没有完全关闭，因此TCP出于合理性和安全性，将在2MSL状态下的连接（4元组）都定义为不可用的。除非通过最高序列号和时间戳进行分辨。否则就会出现另外一个恰巧端口号相同的连接发送的报文“小三上位”的情况。更加严格的规范是，只要两端有一方是在2MSL状态中，那么端口就不可用。 实际上的表现是：客户端断开一个连接后，立刻要求以相同的端口号建立一个新的连接，这时候会报错：Address already in use。 FIN_WAIT_2当主动关闭方发送了FIN并收到了ACK后，会进入FIN_WAIT_2状态。这时连接是处于半关闭的，并且被动关闭放还是可以发送数据的，只要双方愿意，可以永远保持这种状态。 因此有人设计了这样的规则，如果应用程序表明了完全关闭，那么进入到FIN_WAIT_2状态后会启动一个定时器，时间到了就会CLOSE。 同时打开和同时关闭的转换状态图上有写，这里就不赘述了。 RST报文端口不存在Connection Refused. 终止一条链接比如用户忽然ctrl+c，这时所有排队的数据都会被丢弃，一个RST报文会被发送出去。同时另一方看到了RST也会采取终止操作而不是正常的关闭操作。 从某种程度上说RST报文应该是最后一个报文，因为他不会要求另一方回复任何消息，包括ACK。 半开链接如果存在一个半开链接。比如客户机连上服务器后忽然断电，客户机重启后一无所知，但是服务器以为客户机还在连接中。此时服务器向客户机发送数据，此时客户机理应回复RST。 时间等待错误 主动关闭方在2MSL中收到了本应该在之前收到的报文，回复了ACK，但是此时恰好被动关闭方已经CLOSE，已经失去了所有关于此连接的信息，一无所知的被动关闭方收到了这个不明所以的ACK，只能回复一个RST，而这个RST会导致主动关闭方提前CLOSE。 TCP服务器选项在绝大多数运行的服务器上，服务器程序会监听某一个端口，当有新的连接到来时，建立一个新的进程或者线程来处理他，通常这个线程（进程）会启动一个socket。 以ssh为例，主进程监听22端口，当一个新的连接到来时，创建一个新的线程和socket，这个socket的状态为established。主进程继续监听。 途中的本地ip地址和远程ip地址都是通配符，没有限制。 将要到来的连接队列为了解决当主进程正在创建一个新的socket的同时，又有多个SYN报文同时到达的情况，TCP中设计了一个队列来进行排队管理。 从服务端来看，一个新的连接具有两种状态（见状态图）： 1. SYN_RCVD 收到了SYN，发送了SYN+ACK，等待客户端回复ACK的阶段。 2. ESTABLISHED 收到了ACK，完成了三次握手的最后一次，但是还没有把连接交付给应用程序的阶段。 在现代linux内核协议栈中，应用程序能够控制第二个队列的最大数量。 在linux中，有如下的规则： 1. 当新的连接的SYN报文到来时，如果SYN_REVD状态的连接数超过某个规定值，拒绝该连接。 2. 应用程序能够规定一个backlog，来限制第二阶段的连接数量，当然这个backlog也是有（操作系统规定的）最大值的限制的。 3. 如果有剩余空间，那么TCP会ACK新来的SYN。如果TCP还没有将连接交给应用程序，那么此时收到的数据会被TCP缓存。 4. 如果没有剩余空间了，那么TCP会延迟这个连接，来个应用程序一个跟上的机会。除了Linux。 按照正常的TCP机制，当第二阶段的队列已满时，应该让客户端超时，而不是像Linux一样发送RST报文。 如果服务器应用程序想要拒绝一个连接，别无他法，只能在连接建立之后发送FIN，拒绝三次握手不是他所能涉足的操作。 与TCP连接管理相关的攻击SYN flood，sequence number attack等。","link":"/2017/07/27/tcpip-tcp-connection-mnagement/"},{"title":"TCP/IP协议 TCP Timeout and Retransmission","text":"","link":"/2017/10/18/tcpip-tcp-timeout-and-retransmission/"},{"title":"TCP/IP协议 TCP(Preliminaries)","text":"本章应是介绍TCP的引言，后面有5个章节是详细介绍TCP协议各个部分的。 一个设计思路，以及几个概念在容易出现差错，不可靠的传输频道上，怎样确保传输的内容正确呢————检错码，纠错码，重传，TCP选择了检错码和自动重传。自动重传需要确认的信号，就有了Acknowledgement。发送者发送一个报文，接受者确认一次，然后发送者再发送一个报文，这样叫“停等”。停等的确认机制很简单，但是吞吐量很低很低，怎样提高效率呢。于是就有了滑动窗口，可变长的。 也就是说TCP协议机制的一大部分都是为了保证传输的可靠性的，这是TCP的一大特征，另两大特征一个是面向连接的，对应着端口号或者socket，另一个是字节流的，由顺序号体现。 TCP header的结构 源端口和目的端口与IP报文头中的源IP地址和目的IP地址一起构成了一个四元组，标识了一个TCP连接（或者叫Socket）。 Sequence Number用来标识这个报文中的数据段在这个数据流中的位置。 Acknowledgement Number用来标识已经确认收到的数据。 Header Length用来表示头部长度，因为Options是变长的。 6个标志位。 Window Size明确了滑动窗口的大小，用于Window Update吧。 Checksum校验和。 Urgent Pointer还不清楚。 Options中的是可选项，比如最大报文长度，时间戳，选择确认（SACK），窗口尺寸的缩放因子等等。","link":"/2017/07/26/tcpip-tcppreliminaries/"},{"title":"TCP/IP协议 Introduction","text":"一边看书，一边做一些笔记。刚刚开始看第一章，书是全英文版的，一上来就感觉很难，一改这类书第一章都是导论所以很简单的印象。 架构原则因特网架构的首要原则主要考虑这几个关键词：高效，复用，连接已有的网络。 二级原则包含下面几个方面： 发生了loss也要继续。 支持多种服务。 适配多种网络。 支持分布式资源的管理。 高效。（高效性） 新主机的接入要足够方便。（拓展性） 资源必须可数。（高效性） 几个基本概念： packet（包） datagram（数据报） connection（连接） 还有两个原则，这两个原则我没怎么看懂，所以想先放一放。 end to end argumentfate sharing设计与实现 这是OSI7层模型，下三层是所有设备都有的，上四层是只有主机才有的。各层的分工各不相同。在OSI层结构的基础上，每一层能够复用数据，能够封装上层的数据，封装的方式是在上层的数据最前加上一个header，有的层会在最尾端加上trailer，封装了之后就不必关心上层协议下来的数据的内容。每层协议可以封装多个上层协议的PDU（协议数据单元）。 这里的复用是指，每一层所要封装的上层PDU可以是来自不同的协议的，比如IP层可以封装TCP和UDP的就好像每一层都有一个槽，槽里面放的是什么东西是可以变的。为了能够正确的解复用，需要在封装的时候在header部分加上一些标识符来表面PDU是什么类型的，否则接收放在拆包的时候就无法正确地处理了。 上面这张图描述了一个理想的网络结构，两台主机，中间一台交换机一台路由器，交换机只包含两层，路由器包含了三层，所以路由器可以用来联通不同类型的网络。当今时代也有很多路由器和交换机能充当主机的身份的，比如路由器能够供管理员登陆，那就说明肯定有应用层了。 网络层以上的层使用end to end协议，只有主机才使用这些协议（因为中继设备没有网络层以上的层），网络层使用hop to hop协议，所有主机和中继设备都会用到。 注：交换机或者说网桥不被普遍地认为是中继设备因为他们不被编址，换句话说，从网络层设备的角度看，交换机是透明的。 TCP/IP协议栈的结构和协议 有的层还包含一些附属的协议，协议很多而且后面的章节还会详细的介绍所以这里就不一一罗列了。 有关复用，解复用，封装的问题再一次得到了体现。如以太网帧包含16位的以太网类型字段（ipv4，ipv6，arp），IP数据报包含8位的协议字段（ICMP，IPV4，TCP，UDP），在传输层则使用端口号来进行解复用。","link":"/2017/07/18/tcpipillustration-introduction/"},{"title":"这是一个测试页","text":"Markdown语法渲染这部分主要是为了测试Hexo渲染Markdown的编译器对常用记号的支持程度。 各级标题 一级标题二级标题三级标题 四级标题五级标题六级标题引用 白求恩同志是加拿大共产党员，五十多岁了，为了帮助中国的抗日战争，受加拿大共产党和美国共产党的派遣，不远万里，来到中国。去年春上到延安，后来到五台山工作，不幸以身殉职。一个外国人，毫无利己的动机，把中国人民的解放事业当做他自己的事业，这是什么精神？这是国际主义的精神，这是共产主义的精神，每一个中国共产党员都要学习这种精神。列宁主义认为：资本主义国家的无产阶级要拥护殖民地人民的解放斗争，殖民地半殖民地的无产阶级要拥护资本主义国家的无产阶级的解放斗争，世界革命才能胜利。白求恩同志是实践了这一条列宁主义路线的。我们中国共产党员也要实践这一条路线。我们要和一切资本主义国家的无产阶级联合起来，要和日本的、英国的、美国的、德国的、意大利的以及一切资本主义国家的无产阶级联合起来，才能打倒帝国主义，解放我们的民族和人民，解放世界的民族和人民。这就是我们的国际主义，这就是我们用以反对狭隘民族主义和狭隘爱国主义的国际主义。 列表 jekyll hexo hugo wordpress c++ VisualStudio Java IDEA Python Pycharm Pycharm的第二段落 Pycharm的第三段落 PHP 代码区块（缩进） public class Server{ public static void main(String[] args){ System.out.println(\"Hello World\"); } }代码区块（`) 12345public class Server{ public static void main(String[] args){ System.out.println(\"Hello World\"); }} 分割线 链接 This is an example inline link. This is an example reference-style link. 强调 宫中府中，俱为一体，陟罚臧否，不宜异同。若有作奸犯科及为忠善者，宜付有司论其刑赏，以昭陛下平明之理，不宜偏私，使内外异法也。 前端加载 Travis CI测试一下持续集成。","link":"/2019/01/24/test/"},{"title":"TCP/IP实验 到指定目标的路由追踪（实现tracert功能）","text":"路由追踪指当访问一个远程的设备时，能够得到沿途的路由器的IP。这个功能已经由tracert（traceroute）程序实现了，我一来闲的没事，而来早就想做一些关于主机发现和路由追踪的小程序，因此就研究了一下。 使用python的scapy库进行数据包的收发，由于scapy已经实现了traceroute功能，更何况windows和linux都有tracert（traceroute）命令，但是我还是想从底层搞清楚原理，所有先用traceroute做追踪，同时使用wireshark抓包，研究一下包的结构和原理，再自己用scapy实现一下traceroute。 注意：Win10 & Python2.7 环境下，使用pip安装scapy可能会遇到问题（global name xxx is not defined），建议直接clone github上的仓库，然后python setup.py install。 使用我们学校jwc网站作为实验对象，所有实验都在seu内网进行（感觉内网变化小一些）。 先抓个包看看jwc网站ip地址是多少。其实应该是有更加简便的方法的，只是我不知道。而且tracert直接填主机名好像不行。 从wireshark中也可以看出tracert探测的模式：每个TTL发送三个ICMP报文，收到三个回复后再把TTL+1，再继续发送。 基于上述原理，可以开始尝试自己发送ICMP报文并实现tracert功能了。 注：由于国庆回家，网络环境发生了一些变化，我现在只能在家进行实验，因此每一条的结果肯定会和在宿舍进行追踪产生的结果不同。 下面先贴出我写的py程序： 12345678910111213141516171819202122232425262728from scapy.all import *def send_ICMP(ttl, dst): ans, unans = sr(IP(dst=dst, ttl=ttl) / ICMP(), timeout=8) if ans is not None and len(ans) is not 0: reply = ans[0] print \"Hop:{} reach:{}\".format(reply[0].ttl, reply[1].src) if reply[1].type is 11: return True else: print \"Reach to the destination\" return False else: return Truedef tracert(dst): ttl = 0 while send_ICMP(ttl, dst): ttl += 1 if ttl > 30: breakif __name__ == '__main__': tracert(\"jwc.seu.edu.cn\") # traceroute(\"jwc.seu.edu.cn\") 这是windows自带的tracert的输出结果。 从图中可以看出，scapy的动作也是很有规律的，基本上符合发送两个ICMP报文，收到一个reply的模式，而且对同一个地址的两个ICMP之间相隔不到20ms。","link":"/2017/09/29/tracert/"},{"title":"Typescript 简述，安装，开发环境配置","text":"简述Typescript是一个由微软开发的编程语言，听说有C#的首席架构师助阵Typescript的开发。是Javascript的超集，提供了可选的静态类型和基于类的面向对象编程，感觉就像C#和JavaScript之间的一种语言。 废话少说，先把环境搞起来，环境搭好了就可以码代码了。 以下所有的代码都来自于 Typescriptlang 这个网站，感觉比较良心，大家可以参考一下。 安装 用vs插件 用nodejs的npm 我选择用npm 1npm install -g typescript 编辑器的话，这次就不靠jetbrain家族了，尝试一下vscode吧，好像vscode对typescript的支持也是相当充足的。说来也有趣，vscode里typescript的插件几乎有一般都是用来自动import的。。。。。。 Hello World我觉得还是先码一点代码，看看效果，再决定是否要找插件比较好。 12345function greeter(person) { return \"Hello, \" + person;}var user = \"Jane User\";document.body.innerHTML = greeter(user); 嗯这就是一段HelloWorld，不是从标准输出而是在网页上输出的。另存为greeter.ts。 1tsc greeter.ts 这一步将ts代码编译为js代码了，因为helloworld实在太简单，所以ts和js应该是没有区别的，除了他会帮你自动格式化一下。这还不算ts，毕竟还没有引入静态类型呢。 加上一点点类型信息12345function greeter(person: string) { return \"Hello, \" + person;}var user = [0, 1, 2];document.body.innerHTML = greeter(user); 还没运行呢，编辑器就报错了，类型不对。Number[]无法适用于string。不写参数也算错。虽然tsc依然会生成js代码，但似乎不保证js代码运行的正确。 其他插件12npm install -g typingstypings install dt~node –global typings是一个自动补全的工具，除了对语法块（snippet）补全之外，还可以对其他模块，其他包的内容进行分析，并提供自动补全的候选。 创建一个完整的项目12mkdir ts_democd ts_demo 然后创建一个tsconfig.json文件，一般这个文件都是用来写ts项目的配置文件的。 1234567{ \"compilerOptions\": { \"target\": \"es5\", \"module\": \"commonjs\", \"sourceMap\": true }} 然后写一个java风格的HelloWorld。 12345678class Startup { public static main(): number { console.log('Hello World'); return 0; }}Startup.main(); 再然后，调处vscode的命令Configure Task Runner，选择TypeScript - tsconfig.json，他会在.vscode目录下新建一个task.json文件，文件内容如下： 12345678910{ // See https://go.microsoft.com/fwlink/?LinkId=733558 // for the documentation about the tasks.json format \"version\": \"0.1.0\", \"command\": \"tsc\", \"isShellCommand\": true, \"args\": [\"-p\", \".\"], \"showOutput\": \"silent\", \"problemMatcher\": \"$tsc\"} 这个文件描述的应该是把ts文件转换为js文件的操作。只要新建就可以了，不需要修改什么。保存一下以后，Ctrl+Shift+B会转换当前文件夹下的所有ts文件为js文件，然后用node运行相应的js文件即可。","link":"/2017/07/11/typescript-1/"},{"title":"Typescript 类型和变量","text":"因为我js没有认真学，所以从typescript学起的话要从基础的一点一点来了。 基本类型1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556let Done: boolean = false;let decimal: number = 6;let hex: number = 0xf00d;let binary: number = 0b1010;let octal: number = 0x744;let color: string = \"blue\";color = \"red\";let fullName: string = `Bob Bobbington`;let age: number = 37;let sentence: string = `Hello,my name is $(fullname).I'll be $(age+1) years old next month.`;let list: number[] = [1, 2, 3];let list_2: Array = [1, 2, 3];let x: [string, number];x = [\"helloworld\", 10];console.log(x[0].substr(1));// console.log(x[1].substr(1));x[3] = \"world\";console.log(x[3].toString());enum Color { Red, Green, Yello}let c: Color = Color.Green;console.log(Color[1]);let notSure:any=4;notSure=\"maybe a string instead.\"notSure=false;function weather():void{ alert(\"This is an alert.\");}let unusable:void=undefined;let u:undefined=undefined;let n:null=null;function error(message:string):never{ throw new Error(message);}let someValue:any=\"this is a string.\";let strLength:number=(someValue).length;let strLength_2:number=(someValue as string).length; 和其他解释型语言类似，基本类型主要有：boolean，number，string。数组的话有：Array和string[]。还有枚举类enum。 从我个人角度看，比较陌生的有：any，void，undefined，null，never。 any意思是不规定类型，常常用于使用第三方库的情况。一个any类型的对象既可以引用number，也可以引用string。在python和js中任意一个变量都可以像这样操作，但是在强类型的ts中就不行了，所以有js这种东西。 另外，any类型和Object类型（万物的父类）有所区别，any类型自带的一些方法，object类型并不能使用。 void类型的遍历只能被赋予undefined或者null，没什么大用处，但是void函数倒是很常见。 never类型的函数永远都不会返回，更不要说返回值类型了，要么函数中途抛出异常，要么函数里有死循环。 有关 Type assertions（assertions好像是断言的意思，assert常常用来测试，如果不为真就抛出异常，姑且就翻译成类型转换吧） 类似其他语言的类型转换，只会在编译时产生影响，不会有额外的检查。有两种形式，一种是尖括号，一种是as（C#的影子）。注意类型转换的优先级较低，要用括号括起来。 变量一提到变量，需要关注的点大概有这些：生命周期，static，const，作用域，可见性，值类型还是指针类型，浅拷贝深拷贝，还有一些常见的运算符，常见的方法。 ``` var a=10; function f(){ var message=”Hello World”; return message; } function f_2(){ var a=10; return function g(){ var b=a+1; return b; } } var g=f_2(); g(); function f_3(){ var a=1; a=2; var b=g(); a=3; return b; function g(){ return a; } } f_3(); //return 2!!! function f_4(shouldInitialize:boolean){ if(shouldInitialize){ var x=10; } return x; } f_4(true); //return 10 f_4(false); //return undefined!!! function sumMatrix(matrix:number[][]){ var sum=0; for(var i=0;i","link":"/2017/07/11/typescript-2/"},{"title":"Typescript 接口","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141function printLabel(labelledObj: { abel: string }) { console.log(labelledObj.abel);}let myObj={size:10,abel:\"size 10 object\"};printLabel(myObj);interface LabelledValue{ label:string;}function printLabel_2(labelledObj:LabelledValue){ console.log(labelledObj.label);}let myObj_2={size:10,label:\"Size 10 Object\"};printLabel_2(myObj_2);interface SquareConfig{ color?:string, width?:number}function createSquare(config:SquareConfig):{color:string,area:number}{ let newSquare={color:\"White\",area:100}; if(config.color){ newSquare.color=config.color; } if(config.width){ newSquare.area=config.width*config.width; } return newSquare;}let mySquare=createSquare({color:\"black\"});interface Point{ readonly x:number; readonly y:number;}let p1:Point={x:10,y:20}// p1.x=5;let a:number[]=[1,2,3];let ro:ReadonlyArray=a;a=ro as number[];let mySquare_2=createSquare({colorr:\"red\",width:100} as SquareConfig);interface SearchFunc{ (source:string,subString:string):boolean;}let mySearch:SearchFunc;mySearch=function(source:string,subString:string){ let result=source.search(subString); return result>-1;}mySearch=function(src,sub){ let result=src.search(sub); return result>-1;}interface StringArray{ [index:number]:string,}let myArray:StringArray;myArray=[\"Bob\",\"Fred\"];let myStr:string=myArray[0];interface NumberDictionary{ [index:string]:number; length:number; // name:string;}interface ClockInterface{ currentTime:Date; setTime(d:Date);}class Clock implements ClockInterface{ currentTime:Date; constructor(h:number,m:number){} setTime(d:Date){ this.currentTime=d; }}interface ReadonlyStringArray{ readonly [index:number]:string;}let myArray_2:ReadonlyStringArray=[\"Alice\",\"Bob\"];// myArray_2[0]=\"Mallory\"interface Shape{ color:string;}interface Square extends Shape{ sideLength:number;}let square={};square.color=\"blue\";square.sideLength=10;interface Counter{ (start:number):string; interval:number; reset();}function getCounter():Counter{ let counter=function(start:number){}; //function 被转型为实现Counter接口的一个对象。 counter.interval=123; counter.reset=function(){}; return counter;}let c=getCounter();c(10);c.reset();c.interval=0.5;class Control{ private state:any;}interface SelectabelControl extends Control{ select():void;}class Button extends Control{ select(){}} ts中，一个接口可以包含如下内容： 属性包括基本类型的和类类型的，可以通过private等关键字修饰访问权限，可以通过readonly修饰可变性。从这个方面来开，ts中的接口和java中的接口还是类似的。 方法包括匿名的和非匿名的。包含匿名函数的接口可以当成一个函数使用，也就是说这里的接口不再是只能由class实现的东西了，一个函数也可以实现一个接口。从动态的角度看，函数本身也是一个对象。函数可以转型成一个实现了接口的对象。 索引包括数字索引和字符串索引，可以构件数组和字典的形式。 实例化可以由对象进行实例化，也可以由函数实例化。 可见性，可访问性这里对可见性讲的非常的粗糙，应该在后面的章节还会提到。 继承接口可以继承接口，也可以继承类。","link":"/2017/07/13/typescript-3/"},{"title":"Typescript 类","text":"123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081class Greeter{ greeting:string; constructor(message:string){ this.greeting=message; } greet(){ return \"Hello,\"+this.greeting; }}let greeter=new Greeter(\"World\");abstract class Animal{ name:string; constructor(name:string){this.name=name;} move(distanceInMeters:number=0){ console.log(`${this.name} moved ${distanceInMeters}m`); }}class Snake extends Animal{ // constructor(name:string){super(name);} move(distenceInMeters=5){ console.log(\"Slithering...\"); super.move(distenceInMeters); }}class Horse extends Animal{ // constructor(name:string){super(name);} move(distanceInMeters=45){ console.log(\"Galloping...\"); super.move(distanceInMeters); }}let sam=new Snake(\"Sammy the Python\");let tom:Animal=new Horse(\"Tommy the Palomino\"); //有一点多态的感觉咯sam.move();tom.move();class Person{ protected readonly name:string; protected constructor(name:string){this.name=name;}}class Employee extends Person{ private _department:string; constructor(name:string,department:string){ super(name); this._department=department; } public getElevatorPitch(){ return `Hello,My Name is ${this.name} and I work in ${this._department}.`; } public get department():string{ return this._department; } public set department(newDepartment:string){ if(newDepartment==\"\"){ console.log(\"Error:Unauthorized update of employee\"); }else{ this._department=newDepartment; } }}let howard=new Employee(\"Howard\",\"Sales\");howard.department=\"\";howard.department=\"Tech\";class Grid{ static origin={x:0,y:0}; calculateDistanceFromOrigin(point:{x:number,y:number}){ let xDist=(point.x-Grid.origin.x); let yDist=(point.y-Grid.origin.y); return Math.sqrt(xDist*xDist+yDist*yDist)/this.scale; } constructor(public scale:number){}} 嗯这一节的代码就看起来舒服多了，和java几乎没什么区别。get和set函数也是形如C#中的存取器的写法。 忽然就不想写什么了。","link":"/2017/07/13/typescript-4/"},{"title":"Typescript 函数","text":"1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071function add(x: number, y: number): number { return x + y;}let myAdd: (x: number, y: number) => number = function( x: number, y: number): number { return x + y;};function buildName(firstName: string, lastName?: string) { // function buildName(firstName:string,lastName=\"Smith\"){ if (lastName) return firstName + \" \" + lastName; else return firstName;}let result1 = buildName(\"Zou\", \"Dikai\");let result2 = buildName(\"ZouZOU\");function buildName_2(firstName: string, ...restOfName: string[]) { return firstName + \" \" + restOfName.join(\" \");}let buildNameFun: (fname: string, ...rest: string[]) => string = buildName_2;interface Card { suit: string; card: number;}interface Deck { suits: string[]; card: number[]; createCardPicker(this: Deck): () => Card;}let deck: Deck = { suits: [\"heart\", \"spades\", \"clubs\", \"diamonds\"], cards: Array(52), createCardPicker: function(this: Deck) { return () => { let pickedCard = Math.floor(Math.random() * 52); let pickedSuit = Math.floor(pickedCard / 13); return { suit: this.suits[pickedSuit], card: pickedCard % 13 }; }; }};let suits = [\"hearts\", \"spades\", \"clubs\", \"diamonds\"];function pickCard(x: { suit: string; card: number }[]): number;function pickCard(x: number): { suit: string; card: number };function pickCard(x): any { if (typeof x == \"object\") { let pickedCard = Math.floor(Math.random() * x.length); return pickedCard; } else if (typeof x == \"number\") { let pickedSuit = Math.floor(x / 13); return { suit: suits[pickedSuit], card: x % 13 }; }}let myDeck = [ { suit: \"diamonds\", card: 2 }, { suit: \"spades\", card: 10 }, { suit: \"hearts\", card: 4 }];let pickedCard1 = myDeck[pickCard(myDeck)];let pickedCard2 = pickCard(15); 总结一下，总共四部分： 1. 函数的类型，原型。 2. 可选参数，默认参数，变长参数。 3. this的坑。 4. 函数重载。","link":"/2017/07/22/typescript-5/"},{"title":"Ubuntu Server配置BBR拥塞控制算法","text":"自从大佬告诉了我一个新的黑科技：BBR，可以显著提升服务器发送数据包的速度，尤其是在翻墙的时候，我给我的Digital Ocean上的服务器配置了BBR之后，果然下载速度得到了极大的提升，本地使用SEU Wlan下载大文件基本可以稳定在8Mbps左右。之前开了Shadowsocksr，下载Chromium的源码，任务管理器里的网络IO图真的是很漂亮。 但是那次配置BBR没有记录，但是我国内的云服务器（就是你现在访问的这个）还没有升级过内核，因此想要给这台机子也升级一下内核，然后开启一下BBR。 BBR简介BBR是TCP层的一种拥塞控制算法，是Google设计并开源的。传统的拥塞控制算法通过丢包的情况判断网络情况，从而及时调整拥塞窗口的大小，这样使得整个网络处于稳定状态的同时，单个终端又能够享受尽可能大的传输速度。 目前大规模使用的应该是Qubic算法，我没有深入了解过这种算法，好像也是指数增长——丢包回落的形式，具体细节不清楚，有时间可以去了解一下。 知乎上各种吐槽TCP现在的拥塞算法太保守，不能够充分提升传输速度。BBR则是另外一套算法，能够通过更加合理科学的判断和调节，更加充分的提升传输速度。 在Linux 4.9之后的内核中自带BBR，因此如果是老版本内核想要享受BBR，就需要升级内核了。 配置过程首先查看内核版本uname -r。 是4.4，64位的。接下来到 http://kernel.ubuntu.com/~kernel-ppa/mainline/ 这个网站找对应架构和版本的内核。我选了一个4.12的。 注意看清楚架构，我一开始把arm64看成了amd64，架构选错了是肯定没办法安装的。把他下载下来，用wget（刚刚那个网站是国外的，网速有点慢，自己想办法吧）。 一般几十MB，下载完成后，sudo dpkg -i xxxxxx.deb 安装完成，执行sudo update-grub，更新引导程序，并重启。 重启完成，剩下的就只是修改几个配置项了。 echo \"net.core.default_qdisc=fq\" >> /etc/sysctl.conf echo \"net.ipv4.tcp_congestion_control=bbr\" >> /etc/sysctl.conf 如果提示permission deny，并且sudo也没有用的时候，使用下面两条命令： sudo bash -c 'echo \"net.core.default_qdisc=fq\" >> /etc/sysctl.conf' sudo bash -c 'echo \"net.ipv4.tcp_congestion_control=bbr\" >> /etc/sysctl.conf' 保存生效。 sudo sysctl -p 最后检查一下BBR是否开启。 sudo sysctl net.ipv4.tcp_available_congestion_control 如果返回如下，说明成功啦。 的确，开启了BBR之后，下载大图的速度得到了显著的提升~","link":"/2017/12/19/ubuntu-server-bbr/"},{"title":"休息一年，然后回来","text":"本来在大三时打定主意，本科毕业了之后就直接参加工作，迅速进入资本积累的人生阶段。结果在大四秋招之后对职业生涯却产生了新的理解和认识。人生应该不至于此，我决定参加2019年考研。 故事 秋招本身的过程是十分坎坷的。我从九月初开始陆续投简历，参加笔试、面试。因为对自己专业水平的错误评估，以及自己有一定好高骛远、心高气傲的态度，没有”海投简历”，而只是把赌注下在了那些业内人耳熟能详的大厂上面。 结果自然是，缺乏面试经验，专业基础知识不够扎实，还对面试有天生抗性的我，被大厂拒了个遍。 自我认识出现了偏差，最后的责任自然只有我能够承担。 2018年10月找到工作之后，笔试面试的压力逐渐变小，再加上还要应对KXD的实习，我也开始松懈了下来。虽然offer不是来自什么大厂，但好歹也算是对应领域的头部公司，且公司主要围绕的两大核心——区块链技术和数字加密货币交易业务都非常地合我胃口，工资待遇不低，地处北京未来可期。面试时与面试官的亲切交谈也让我对这家公司产生了相当不错的初步感受。 我有个亲戚在字节跳动工作，他在了解到了我的情况之后，给我的意见是“毕业之后的第一份工作一定要去大公司”。他从内部情况、职业发展的角度解释了“大公司”的好处。我当时觉得他的建议和理由有一定的道理，但也仅仅是有一定的道理。毕竟上升和成长主要还是靠自己的努力。我当时还没有其他的想法。\b （这里盗用一下富士康的图） 大概在十一月份的时候，我在b站上看到了一个视频。加拿大白嫖王Linus去参观一加手机的生产车间。一加手机的装配流程采用人工流水线的形式，每个工人从流水线中取下一部手机，安装上自己负责的组件，再将其放回流水线。工序被划分的很细，安装一个组件和整理该组件连接线都被分配给了两个人做。联系到软件开发行业的工作，又何尝不是这样的呢。 在一个稍显复杂的系统中，每一个研发工程师都只负责开发其中的一个模块，这个模块如一个螺丝一般细小，但又的确不可或缺。模块是不可或缺的，但人不是。高薪的互联网研发岗位吸引了大量的劳动力，供过于求，单位就只能提高招聘门槛来筛选人才。 “面试造火箭，工作拧螺丝”。说的就是这个意思。 拧螺丝不是什么可怕的事情，可怕是拧一辈子的螺丝而浑然不知，甚至是拧了几年的螺丝，忽然就失业了。试想那些流水线上的装配工人，有几个能在日复一日的手机装配工序中，总结出手机内部组件编排设计的规律或者原理，进而成为一个扫地僧式的人物？不可能的，因为核心技术不在这里，另外技术门槛也高。那换一个角度看，能否就沿着普通的管理人员的发展道路走呢？看似行的通，但实际上难度也不小，毕竟僧多粥少。 凭什么你能上别人不能上？对个人综合能力的评估是一件复杂的事情，虽说现在有科学客观的评估方法，但有时就是说不清道不明。不会有多少人能够搞清楚各个指标在各个方向上占多少权重，大多数人能做到的是尽可能地让每个指标都变得更漂亮一些。 学历就是一个非常重要的指标。有多重要呢？根据我个人的估计和在面“网易”时与面试官的交流，至少进入面试阶段与我“同台竞技”的同学中，十个有九个是研究生，剩下一个是本科生的我。 社会关系 人的本质是一切社会关系的总和，不是鸽子，亦不是复读机。——马克思 我在大学本科四年中没有正确地处理好所有的社会关系，或者说没有有意培养处理各种关系的能力。直到大四才意识到处理好关系的重要性。 当然，能力的培养可以在学校，也可以在社会，社会有社会的关系，最典型的如工作和生活之间的二元关系。学校里也有学校所特有的关系，社会环境和校园环境没有高低之分。我知道人总是要走进社会的，只是不读研究生的话，学校里的一些关系很有可能这辈子都接触不到了。 取之其上 “取之其上，得之其中；取之其中，得之其下”。 在我爸那个年代，走出校园之后安安稳稳地在一个岗位上干一辈子是非常常见的。而在市场经济高度发达的今天，这样的人几乎只存在于文学作品以及宣传劳模的媒体报道中，不想着职业的成长和发展，不想“往上升”，就会“被别人踩在脚下”。 若是只想着在一个岗位上安安稳稳地干下去，那么按照取其上得其中的规律，结合如今的社会生活实践，很有可能做不到“安稳”。 有个高速公路收费站要取消了，需要安排那里的工作人员再就业，“我在收费站工作了二十年，现在你说取消就取消了？我今年40了，你说还有哪个单位要我这么大的人？我这二十年只会收费，你说这个职业技能哪个单位要？”。 所以说，人还是要有远大的理想，不仅仅是为了实现人生价值，更是让自己的努力和奋斗所带来的“成果支票”能够有一定的冗余和保障。 我在高中之前的理想都是瞎想，高中时期只想着考上一个好大学，从未思考过职业规划。大学时我对自己的职业规划就是简单的 “进入BAT，升职加薪，迎娶白富美，走上人生巅峰”。产生这个想法的原因有以下几点： 程序员工资水平很高，自给自足不愁。 软件和互联网在当前还是代表着最为先进的生产力水平，短时间不会被淘汰。 我是软件工程专业的，有能力写好代码，干好活儿。 这三点认识本身没有什么大问题，问题在于仅带着这三点认识就匆忙进入职场，可能会使视野受到局限。 我出生在一个传统家庭，因为和爷爷奶奶生活在一起，因此很多生活习惯和思想观念都比较接近老一代。老一代的职业观念就是找一个稳当的工作，定定心心地赚钱，买车买房结婚生子，安安稳稳过日子。在和亲戚朋友的交谈中感受到了 “钱”和地位 的重要性。房子大，存款多，开公司当老板，人脉广有抓手，自然就有话语权。这也使得作为普通工薪阶层的我的父母常常是饭桌上默默无闻的那几个人。 家长所处社会地位的制约导致我在进入了大学之后很难从他们那里得到足够的指导，同时我的自身的思想也还是深深地受传统观念影响。再加上我天生内向，在学校也很少与学长和老师做深入交流，只凭知乎等网络信息渠道获得的信息还不足以驱动我去做出改变。 宽阔的视野是树人、成材之路上的必要因素，甚至比挣钱的能力更重要。程序员因为与网络较为接近，接触信息的渠道比其他行业的人更加丰富多样，但我认为仅仅这样还不够。拓宽视野的重要步骤是实践，而以996闻名的程序员是否具有实践的时间和精力，这点是值得怀疑的。由于我目前较为薄弱的处理工作和生活这二者之间关系的能力，在参加工作之后很有可能导致二者失衡。 是的，我还需要锻炼，还需要成长，但我怕在有的地方一忙起来就得不到成长。 野心膨胀说着说着，我对权利和财富的欲望就开始膨胀起来了。其实我觉得欲望的膨胀不是坏事，取之有道即可。人这一生只有一次，不抓紧机会走上人生巅峰，可没有第二次膨胀的机会。 路在何方当然，大四没有保研，结果现在选择了毕业之后考研的道路，说到底就是走了弯路。给这个东西定性的话应该说：因为自己对自我水平和社会环境的认知出现了偏差，导致做出了错误的选择，不过人的认识本身就是随着社会大环境和个人小情况的发展而不断变化的，现在认识到了这些，也不算晚。眼前有路，就上。 但我认为过程也是很重要的，四年课不是白上的，五万行代码也不是白写的。 如果我考上了研究生的话，我认为还是存在两种可能性。 研究生毕业，仍然去互联网公司做研发岗。目前我不是很希望自己走到这条路上，我一直想 “站着把钱挣了”。 若是家里条件允许，并且自身条件合适，则继续深造。 但无论走哪一条路，我都希望我能够以一个不同于现在的面貌去应对。","link":"/2019/04/06/%E4%BC%91%E6%81%AF%E4%B8%80%E5%B9%B4%EF%BC%8C%E7%84%B6%E5%90%8E%E5%9B%9E%E6%9D%A5/"},{"title":"如何看待江苏要南京提高首位度？","text":"这个主题来自知乎，今天在回家的高铁上偶然刷到。作为一个在南京生活了3年半的土生土长的常州人，我想围绕这个主体就知乎上的一些回答给出我自己的些许看法。 关于首位度（Primary Ratio） 首位度在一定程度上代表了城镇体系中的城市发展要素在最大城市的集中程度。——百度百科 早在1939年，马克·杰斐逊（M.Jefferson）就在《Why Geography? The Law of the Primate City》一文中提出了城市首位律（Law of the Primate City）的概念。他发现，一个国家的“首位城市”往往会比第二位以及排位更靠后的城市大很多，会拉开一段差距。为了衡量发展要素在最大城市的集中程度，他定义了“两城市指数”：首位城市和第二位城市的人口规模之比。很显然，两城市指数越大，说明首位城市和其他城市的差距越大，资源越集中，首位度越强。 城市首位律更多的只是提出了首位度这个概念，至于这个度应该在怎样的水平最为合适，学术界一直存在争论。大致的分析角度有：规模经济与聚集经济、城市位序规模分布。 南京首位度江苏是一个GDP大省，从省总量上看，常年居全国第二，仅次于广东省。从省内来看，南京作为省会在科教文卫上处于省内的领先地位，其中教育更是能够排到全国TOP3。与高度发达城市功能相对应的，南京的GDP却多年不及苏州，且与后继城市如无锡、南通、常州之间的差距不是特别大。 最近几年南京的首位度的相关问题也正在被有关领导重视起来。 十九届中央第一轮巡视整改 江苏：整改省会城市功能发挥不够 20181027 巡视组提出的要求是要推动南京交通建设，支持创新发展，加快产业升级，结合GDP不及苏州这一现象来看，南京的短板应该是在产业结构上。 南京产业结构南京本身是一个制造业相对发达的城市，重工业、化工业占较大比重，自然资源比较匮乏。改革开发以来，南京的经济也随着市场发展逐步向集约型（提高生产要素质量和生产效率，而不是扩大生产要素规模）转变。 优化产业结构2008年南京的一二三产业GDP比重为2.5：47.5：50，呈现“三二一“的结构形态。其中行业增加值占第三产业比重最大的五个行业为：批发零售、金融、交通仓储邮政、房地产、教育。 “十二五”以来，南京新兴产业发展迅猛，以信息技术为代表的七大类战略性新兴产业占比首次超过石化、钢铁、建材等传统制造业，2016年占比达到40%左右。 2017年南京服务业增长10.3%，比第二产业高出5.2个点，占GDP比重达到59.7%。传统服务业如贸易、交通、房地产、教育等稳定增长，对总体经济运行起到稳定作用。同时现代服务业如信息、商务、金融等对增长贡献较大。当年一二三产业占GDP比重为2.3：38.0：59.7。 扩大有效需求近年来，南京的旅游消费不断升温，传统的有名的旅游项目（如夫子庙、中山陵、总统府）继续保持热度，每年吸引大批游客。新增的项目如牛首山、大报恩寺等则进一步增强了南京的旅游吸引力。每年的元宵节夫子庙都会有灯会，华丽的灯光下游人如织，场面堪比上海世博会，这也从侧面说明只要继续挖掘潜力，积极探索现有条件下新形式的旅游项目，南京的旅游市场还能够得到巨大的提升。在旅游项目中，培育服务消费、信息消费、绿色消费，推动消费升级。 激励创新驱动南京拥有位于全国第一梯队的科教力量，且在很长一段时间内其地位不会产生太大变化。如果能够将人才“留住”，同时吸引海内外优秀人才，那必然能够为长期稳定高质量的经济发展提供可靠保障。在雨花台区软件大道两侧，有华为南研所、中兴云计算中心；在河西有小米、阿里巴巴江苏总部（在建）；在江宁，有无线谷（我东南边）。越来越多的公司开始重视在南京的投入，“近水楼台先得月”，这也为每年南京的高校毕业生提供了丰富的择业选择。 推动创新创业靠人才，激励创新创业靠政策。近年来多个创新创业园区在南京成立，配合优惠政策，能够集中资源，吸引高潜力创新企业。优质人才和企业的流向，就是未来的流向。 如何看待江苏要求南京提高首位度的这个意见，的确会对南京的方方面面产生影响。“揣摩上意“的话：在江苏省这样一个沿海大省，需要一个像样的带头城市，使其在充分发挥自身区位优势的同时，带动周边地区的发展，诸如苏北的几个城市以及与安徽省邻接的马鞍山、芜湖、宣城等。南通近几年经济的迅猛发展，我认为很大程度上就是得益于上海的辐射效应，不过南通的产业结构与南京存在差距（南通六大产业：船舶海工、高端纺织、电子信息、智能装备、新材料），政治经济条件上差距也较大。 从地图上看，苏锡常与上海较为接近，因此很自然地与上海结成“环沪都市圈”，南京若是想要提高首位度，必然需要与周边城市展开合作，扩大自身影响力的同时在一定程度上吸收优势条件。即知乎上所说的“吸血”。事实上，南京凭借目前的水平，已经有足够的吸引力，能够在现有的形式下保持对资源的强大竞争力。因此人才和资本的富集是在所难免的。 富集是在所难免的，北上广深作为排头兵，一直有源源不断的人才和资本涌入，除非政策调控，否则会一直保持这样的趋势。自上海发达以来，昆山、苏州近水楼台，发展速度相对较快。如果南京也能够立足苏中，成为地区心脏，那对于江苏的发展来说是大有裨益的。南京应该成为一个国家层面的城市，增强话语权，成为苏皖交接的一颗明珠。城市发达了，话语权响亮了，地位上升了，才不会错过历史发展的机遇。 至于说什么和上海展开竞争，我觉得还是团结一些比较好。 随意想想，现在阻挡我入户上海的唯一客观因素就是房价，可能这也是绝大多数人望而却步的原因。南京房价相比于上海“天价”还是有些遥远，但也已经进入跑道，准备起飞了。当然房价受大环境和政策影响较大，但也会在很大程度上受城市综合水平影响。如果让我选择未来定居的地方，南京是我第一选择，常州第二。","link":"/2019/01/27/%E5%A6%82%E4%BD%95%E7%9C%8B%E5%BE%85%E6%B1%9F%E8%8B%8F%E8%A6%81%E5%8D%97%E4%BA%AC%E6%8F%90%E9%AB%98%E9%A6%96%E4%BD%8D%E5%BA%A6%EF%BC%9F/"},{"title":"我理想的文档管理模式","text":"我理想的文档管理模式印象笔记 在我把电脑换成了 Macbook Pro 之后，我开始尝试用印象笔记来进行文档管理。现在，我的印象笔记不仅用来存放我平时的所有正式和非正式的书写，包括零碎的草稿和阅读笔记，还承载了稍后阅读、文件归档（证书、发票、说明书）这两大功能。 通过 ios 的分享、MacOS Safari 的剪藏插件，可以将网页（准确地说是 URL）的内容存放在印象笔记中，虽说有时剪藏得到的网页内容并不完整，但也算可用。 得益于印象笔记优秀的中文全文搜索功能，那些需要长期保存的文件也被我放在了印象笔记中，方便随手检索。而 Alfred 的印象笔记 Workflow，更是让文档的检索更加快捷方便。 但是我也发现了一些问题，例如 印象笔记的文档和 MacOS 的本地文件（先不论是不是 iCloud）的管辖范围在某些程度上会产生重合。 印象笔记虽然在笔记领域还是比较成熟的，但是 MacOS 端的客户端体验并不完美，有一些很气的小问题： 在某个笔记包含较多的图片时，上下滑动不平滑，一卡一卡。 对 Markdown 格式支持不完整。 要钱（虽说穷是我的问题）。 Devonthink 在少数派的文章中了解到了 Devonthink 这款软件。Devonthink 是个非常专业的文档管理工具，他本身的存储从文件系统的角度看是和 MacOS 的图库一样的，有一个专门的数据库文件，需要把文件加入到 Devonthink 数据库里进行整理，但 Devonthink 支持引用而不拷贝文件，这样就不会出现两个副本的问题。Devonthink 不仅是从文件的归类整理、检索来说都是非常专业的，唯一的问题在于这个软件太贵了！！！ Onedrive && VSCode我最近挖掘了一种新的文档管理模式。 使用 Onedrive 进行多设备同步。Onedrive 的云同步和版本管理还是很可靠的。 使用 VScode 进行 Markdown 的写作。 在 iPhone 和 iPad 上都有 Onedrive 的客户端，可以直接搜索、阅读 pdf、word 等格式文件，唯一美中不足的是 iPad 上的 Onedrive 在打开 Markdown 文件时会出现乱码，导致该文件上的全文搜索索引失效，因此搜素 Markdown 文件中的内容算是废了。 Onedrive 对于 Office 套件的文件格式的搜索倒是支持地很不错。 理想的文件管理我理想的文件管理模式有以下的特征： 有快捷的全文检索，尤其是要对中文有良好的支持。对于扫描版的 PDF 要能够进行 OCR 扫描后进行检索，对于可编辑的 PDF 也要能够进行检索。 多终端同步，最好能够跨平台，因为未来我还是有可能用到 Windows 生态的。如果不能跨平台的话最好有 WEB 端。 优秀的 Markdown 撰写体验，对于其他的文档格式（word、excel、pdf）要能够支持阅读。 目前还未找到完全符合我要求的文档管理软件，目测需要我自己开发。等我有空了就自己去写一个~ 如果需要我来开发这个软件的话APP 的形式和大多数的笔记 APP 类似。 多格式阅读。 Markdown 格式写作。 借助于 Onedrive（或者 Dropbox、iCloud）的的跨平台同步。 全文检索。 其他高级的文档组织功能，例如标签、关联等。","link":"/2020/04/22/%E6%88%91%E7%90%86%E6%83%B3%E7%9A%84%E6%96%87%E6%A1%A3%E7%AE%A1%E7%90%86%E6%A8%A1%E5%BC%8F/"},{"title":"新的主题","text":"起因之前一直使用 Hexo 的 next 主题来呈现博客，最近觉得 next 太素了，想换一个花哨一些的~ Nexmoe这个主题还是我在 Github 首页被推荐到的，看来 Github 的推荐算法越来越智能了，连博客主题的个人喜好都能作为推荐的标准了（开个玩笑）。 额外的工作博文封面Nexmoe 支持为每篇文章配置一个漂漂亮亮的封面图，操作方法是在文章 md 文件的头部添加下面几行代码，分别是图片的 http 地址、长度和宽度。据作者说，之所要要显式地声明文件长宽，是为了消除图片的抖动。 12345---cover: https://i.loli.net/2019/07/21/5d33d5dc1531213134.pngcoverWidth: 1200coverHeight: 750--- 社交链接作者的博客首页摆上了一堆社交网站的 Logo，特别可爱。 在主题的 _config.yml 文件中，可以添加社交网站的 Logo 和个人链接。我暂时搞了四个。 123456789101112131415161718192021social: 哔哩哔哩: - https://space.bilibili.com/13288240 - icon-bilibili - rgb(231, 106, 141) - rgba(231, 106, 141, .15) GitHub: - https://github.com/kherrisan/ - icon-github - rgb(25, 23, 23) - rgba(25, 23, 23, .15) Telegram: - https://github.com/kherrisan/ - icon-telegram - rgb(39,167,229) - rgba(39,167,229,.1) 知乎: - https://www.zhihu.com/people/smalldk - icon-zhihu - rgb(00,132, 256) - rgba(00, 132, 256, .15) 代码高亮Mexmoe 自带了代码高亮，需要把 Hexo 默认的代码高亮功能关掉，来避免功能冲突。 评论系统我使用的是 Gittalk。先简单描述一下 Gitalk 的原理： 博客的网页中包含 Gitalk 的 js 脚本，在打开文章页的时候，浏览器会加载 js 脚本的内容。 js 脚本会去请求 Github 的一个接口，通过这个接口去查询指定仓库的 issues，这个仓库就是在 _config.yml 中的 repo，repo 不是存放博客内容的仓库名，而是指存放评论的仓库名，当然很多人把评论直接放在博客内容的仓库里，所以 repo 就是 xxx.github.io 这个格式。使用 key 和 secret 来对接口进行鉴权。我另外新建了一个叫 gitalk 的仓库，故 repo 就只要写 gitalk 就行了。 Github 的服务器接口收到了查询 issues 的相关请求，同时会检查请求来自的 Host 是不是你博客真正部署所在的网址，对于大多数人是 xxx.github.io，如果使用自定义域名，也可以是 www.xxx.com 这样。至于到底是什么样，是由用户在 Github - OAuth App 中的 Authorization callback URL 指定的。如果你在本地调试，那就应该写为 localhost:4000。当然它也会验证 key 和 secret 是否匹配。至于 Homepage URL 我还不知道有什么用，貌似可以随便写。 Github 的接口返回 issues 给浏览器，浏览器呈现评论列表。 我的 gitalk 是这么配置的： 123456gitalk: admin: kherrisan owner: kherrisan repo: gitalk clientID: cef6ec0ef4709xxxxxx clientSecret: c184430c4d516f1f71911b1ba06947815fxxxxxx 既然评论列表是浏览器直接从 Github 请求得到的，这就会产生一个问题：Github 服务器在国外，国内的用户在查看评论列表时可能会加载较长的时间。 其他优化网页代码压缩通过 gulp 来压缩 html、css、js 的代码，压缩完了之后的代码，在 chrome 的调试工具中，会变成特别长的一行~ css/js内联使用 hexo-filter-asset-inline。 先 npm 下载这个库，然后在每个需要内联的 js、css 文件地址后面加上 ?__inline=true。由于 hexo 没有一个统一的地方存放 css、js 的地址，而各个主题引用外部 css、js 的地方又各有不同，因此只能在编辑器里面搜索，然后一处处手动添加了。 全部地址修改完成后，打开网页应只需要加载 html 文件以及少数一个图片文件。 全站加速（氪金上云）所有图片放到阿里云 OSS 上去，并且使用全站加速（实际上就是 CDN）来加速对 https://www.kherrisan.cn 这个域名下所有资源的访问。 持续集成我想实现的效果是，我在本地完成撰写后，只需要提交 source 代码，然后 Github 的 Action 会自动帮我完成压缩、编译、部署的工作。 参考资料Nexmoe 文档博客，我优化了什么？hexo优化之——使用gulp压缩资源","link":"/2020/04/10/%E6%96%B0%E7%9A%84%E4%B8%BB%E9%A2%98/"}],"tags":[{"name":"考研","slug":"考研","link":"/tags/%E8%80%83%E7%A0%94/"},{"name":"复习","slug":"复习","link":"/tags/%E5%A4%8D%E4%B9%A0/"},{"name":"记录","slug":"记录","link":"/tags/%E8%AE%B0%E5%BD%95/"},{"name":"日常","slug":"日常","link":"/tags/%E6%97%A5%E5%B8%B8/"},{"name":"消费","slug":"消费","link":"/tags/%E6%B6%88%E8%B4%B9/"},{"name":"月记","slug":"月记","link":"/tags/%E6%9C%88%E8%AE%B0/"},{"name":"预算","slug":"预算","link":"/tags/%E9%A2%84%E7%AE%97/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"编译原理","slug":"编译原理","link":"/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"},{"name":"Git","slug":"Git","link":"/tags/Git/"},{"name":"Gradle","slug":"Gradle","link":"/tags/Gradle/"},{"name":"HTTP","slug":"HTTP","link":"/tags/HTTP/"},{"name":"网络","slug":"网络","link":"/tags/%E7%BD%91%E7%BB%9C/"},{"name":"网络流量分析","slug":"网络流量分析","link":"/tags/%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/"},{"name":"数据结构","slug":"数据结构","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"并发","slug":"并发","link":"/tags/%E5%B9%B6%E5%8F%91/"},{"name":"jsp","slug":"jsp","link":"/tags/jsp/"},{"name":"servlet","slug":"servlet","link":"/tags/servlet/"},{"name":"SSM","slug":"SSM","link":"/tags/SSM/"},{"name":"jvm","slug":"jvm","link":"/tags/jvm/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"OS","slug":"OS","link":"/tags/OS/"},{"name":"MIT_6.828","slug":"MIT-6-828","link":"/tags/MIT-6-828/"},{"name":"MySQL","slug":"MySQL","link":"/tags/MySQL/"},{"name":"Netty","slug":"Netty","link":"/tags/Netty/"},{"name":"TCP/IP","slug":"TCP-IP","link":"/tags/TCP-IP/"},{"name":"测试","slug":"测试","link":"/tags/%E6%B5%8B%E8%AF%95/"},{"name":"Markdown","slug":"Markdown","link":"/tags/Markdown/"},{"name":"前端","slug":"前端","link":"/tags/%E5%89%8D%E7%AB%AF/"},{"name":"优化","slug":"优化","link":"/tags/%E4%BC%98%E5%8C%96/"},{"name":"Python","slug":"Python","link":"/tags/Python/"},{"name":"scapy","slug":"scapy","link":"/tags/scapy/"},{"name":"Typescript","slug":"Typescript","link":"/tags/Typescript/"},{"name":"成长","slug":"成长","link":"/tags/%E6%88%90%E9%95%BF/"},{"name":"博客","slug":"博客","link":"/tags/%E5%8D%9A%E5%AE%A2/"},{"name":"主题","slug":"主题","link":"/tags/%E4%B8%BB%E9%A2%98/"}],"categories":[{"name":"未分类","slug":"未分类","link":"/categories/%E6%9C%AA%E5%88%86%E7%B1%BB/"},{"name":"周记","slug":"周记","link":"/categories/%E5%91%A8%E8%AE%B0/"},{"name":"消费分析","slug":"消费分析","link":"/categories/%E6%B6%88%E8%B4%B9%E5%88%86%E6%9E%90/"},{"name":"日常","slug":"日常","link":"/categories/%E6%97%A5%E5%B8%B8/"},{"name":"编译原理","slug":"编译原理","link":"/categories/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"},{"name":"Git","slug":"Git","link":"/categories/Git/"},{"name":"Gradle","slug":"Gradle","link":"/categories/Gradle/"},{"name":"HTTP","slug":"HTTP","link":"/categories/HTTP/"},{"name":"网络流量分析","slug":"网络流量分析","link":"/categories/%E7%BD%91%E7%BB%9C%E6%B5%81%E9%87%8F%E5%88%86%E6%9E%90/"},{"name":"Java Collection","slug":"Java-Collection","link":"/categories/Java-Collection/"},{"name":"JUC","slug":"JUC","link":"/categories/JUC/"},{"name":"JavaSE","slug":"JavaSE","link":"/categories/JavaSE/"},{"name":"JavaEE","slug":"JavaEE","link":"/categories/JavaEE/"},{"name":"JVM","slug":"JVM","link":"/categories/JVM/"},{"name":"OS","slug":"OS","link":"/categories/OS/"},{"name":"MIT_6.828","slug":"MIT-6-828","link":"/categories/MIT-6-828/"},{"name":"MySQL","slug":"MySQL","link":"/categories/MySQL/"},{"name":"Netty","slug":"Netty","link":"/categories/Netty/"},{"name":"OS:Three Easy Pieces","slug":"OS-Three-Easy-Pieces","link":"/categories/OS-Three-Easy-Pieces/"},{"name":"TCP/IP","slug":"TCP-IP","link":"/categories/TCP-IP/"},{"name":"测试","slug":"测试","link":"/categories/%E6%B5%8B%E8%AF%95/"},{"name":"Typescript","slug":"Typescript","link":"/categories/Typescript/"},{"name":"Spring Framework","slug":"JavaEE/Spring-Framework","link":"/categories/JavaEE/Spring-Framework/"}]}